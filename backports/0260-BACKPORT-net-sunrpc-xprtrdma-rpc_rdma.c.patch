From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/rpc_rdma.c

Change-Id: Ie041c79fd73b40f605d2afff7787484c5b4c64a0
---
 net/sunrpc/xprtrdma/rpc_rdma.c | 182 ++++++++++++++++++++++++++++++++-
 1 file changed, 180 insertions(+), 2 deletions(-)

--- a/net/sunrpc/xprtrdma/rpc_rdma.c
+++ b/net/sunrpc/xprtrdma/rpc_rdma.c
@@ -52,8 +52,15 @@
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
+#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
+#ifndef RPCDBG_FACILITY
+#define RPCDBG_FACILITY    RPCDBG_TRANS
+#endif
+#endif
 /* Returns size of largest RPC-over-RDMA header in a Call message
  *
  * The largest Call header contains a full-size Read list and a
@@ -307,11 +314,19 @@ static struct rpcrdma_mr_seg *rpcrdma_mr
 	}
 
 	rpcrdma_mr_push(*mr, &req->rl_registered);
+#ifdef HAVE_RPC_XPRT_OPS_FREE_SLOT
 	return frwr_map(r_xprt, seg, nsegs, writing, req->rl_slot.rq_xid, *mr);
+#else
+	return frwr_map(r_xprt, seg, nsegs, writing, req->rl_xid, *mr);
+#endif
 
 out_getmr_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_nomrs_err(r_xprt, req);
+#endif
+#ifdef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
 	xprt_wait_for_buffer_space(&r_xprt->rx_xprt);
+#endif
 	rpcrdma_mrs_refresh(r_xprt);
 	return ERR_PTR(-EAGAIN);
 }
@@ -361,7 +376,9 @@ static int rpcrdma_encode_read_list(stru
 		if (encode_read_segment(xdr, mr, pos) < 0)
 			return -EMSGSIZE;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_chunk_read(rqst->rq_task, pos, mr, nsegs);
+#endif
 		r_xprt->rx_stats.read_chunk_count++;
 		nsegs -= mr->mr_nents;
 	} while (nsegs);
@@ -425,7 +442,9 @@ static int rpcrdma_encode_write_list(str
 		if (encode_rdma_segment(xdr, mr) < 0)
 			return -EMSGSIZE;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_chunk_write(rqst->rq_task, mr, nsegs);
+#endif
 		r_xprt->rx_stats.write_chunk_count++;
 		r_xprt->rx_stats.total_rdma_request += mr->mr_length;
 		nchunks++;
@@ -436,8 +455,10 @@ static int rpcrdma_encode_write_list(str
 		if (encode_rdma_segment(xdr, ep->re_write_pad_mr) < 0)
 			return -EMSGSIZE;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_chunk_wp(rqst->rq_task, ep->re_write_pad_mr,
 					nsegs);
+#endif
 		r_xprt->rx_stats.write_chunk_count++;
 		r_xprt->rx_stats.total_rdma_request += mr->mr_length;
 		nchunks++;
@@ -503,7 +524,9 @@ static int rpcrdma_encode_reply_chunk(st
 		if (encode_rdma_segment(xdr, mr) < 0)
 			return -EMSGSIZE;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_chunk_reply(rqst->rq_task, mr, nsegs);
+#endif
 		r_xprt->rx_stats.reply_chunk_count++;
 		r_xprt->rx_stats.total_rdma_request += mr->mr_length;
 		nchunks++;
@@ -625,7 +648,9 @@ static bool rpcrdma_prepare_pagelist(str
 	return true;
 
 out_mapping_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_dma_maperr(sge->addr);
+#endif
 	return false;
 }
 
@@ -653,7 +678,9 @@ static bool rpcrdma_prepare_tail_iov(str
 	return true;
 
 out_mapping_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_dma_maperr(sge->addr);
+#endif
 	return false;
 }
 
@@ -833,7 +860,9 @@ inline int rpcrdma_prepare_send_sges(str
 out_unmap:
 	rpcrdma_sendctx_unmap(req->rl_sendctx);
 out_nosc:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_prepsend_failed(&req->rl_slot, ret);
+#endif
 	return ret;
 }
 
@@ -867,15 +896,23 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 	__be32 *p;
 	int ret;
 
+#ifdef HAVE_XDRBUF_SPARSE_PAGES
 	if (unlikely(rqst->rq_rcv_buf.flags & XDRBUF_SPARSE_PAGES)) {
+#endif
 		ret = rpcrdma_alloc_sparse_pages(&rqst->rq_rcv_buf);
 		if (ret)
 			return ret;
+#ifdef HAVE_XDRBUF_SPARSE_PAGES
 	}
+#endif
 
 	rpcrdma_set_xdrlen(&req->rl_hdrbuf, 0);
+#ifdef HAVE_XDR_INIT_ENCODE_RQST_ARG
 	xdr_init_encode(xdr, &req->rl_hdrbuf, rdmab_data(req->rl_rdmabuf),
 			rqst);
+#else
+	xdr_init_encode(xdr, &req->rl_hdrbuf, rdmab_data(req->rl_rdmabuf));
+#endif
 
 	/* Fixed header fields */
 	ret = -EMSGSIZE;
@@ -891,7 +928,7 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 	 * is not allowed.
 	 */
 	ddp_allowed = !test_bit(RPCAUTH_AUTH_DATATOUCH,
-				&rqst->rq_cred->cr_auth->au_flags);
+				(const void *)&rqst->rq_cred->cr_auth->au_flags);
 
 	/*
 	 * Chunks needed for results?
@@ -937,6 +974,14 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 		rtype = rpcrdma_areadch;
 	}
 
+#if !defined(HAVE_RPC_XPRT_OPS_FREE_SLOT) || !defined(HAVE_XPRT_PIN_RQST)
+	req->rl_xid = rqst->rq_xid;
+#endif
+
+#ifndef HAVE_XPRT_PIN_RQST
+	rpcrdma_insert_req(&r_xprt->rx_buf, req);
+#endif
+
 	/* This implementation supports the following combinations
 	 * of chunk lists in one RPC-over-RDMA Call message:
 	 *
@@ -974,11 +1019,20 @@ rpcrdma_marshal_req(struct rpcrdma_xprt
 	if (ret)
 		goto out_err;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_marshal(req, rtype, wtype);
+#endif
 	return 0;
 
 out_err:
+#ifndef HAVE_XPRT_WAIT_FOR_BUFFER_SPACE_RQST_ARG
+	if (ret == -EAGAIN)
+		xprt_wait_for_buffer_space(rqst->rq_task, NULL);
+#endif
+
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_marshal_failed(rqst, ret);
+#endif
 	r_xprt->rx_stats.failed_marshal_count++;
 	frwr_reset(req);
 	return ret;
@@ -1013,7 +1067,9 @@ void rpcrdma_reset_cwnd(struct rpcrdma_x
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
 
 	spin_lock(&xprt->transport_lock);
+#ifdef HAVE_XPRT_REQUEST_GET_CONG
 	xprt->cong = 0;
+#endif
 	__rpcrdma_update_cwnd_locked(xprt, &r_xprt->rx_buf, 1);
 	spin_unlock(&xprt->transport_lock);
 }
@@ -1107,8 +1163,10 @@ rpcrdma_inline_fixup(struct rpc_rqst *rq
 		rqst->rq_private_buf.tail[0].iov_base = srcp;
 	}
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	if (fixup_copy_count)
 		trace_xprtrdma_fixup(rqst, fixup_copy_count);
+#endif
 	return fixup_copy_count;
 }
 
@@ -1171,7 +1229,9 @@ static int decode_rdma_segment(struct xd
 		return -EIO;
 
 	xdr_decode_rdma_segment(p, &handle, length, &offset);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_decode_seg(handle, *length, offset);
+#endif
 	return 0;
 }
 
@@ -1324,13 +1384,19 @@ rpcrdma_decode_error(struct rpcrdma_xprt
 		p = xdr_inline_decode(xdr, 2 * sizeof(*p));
 		if (!p)
 			break;
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_err_vers(rqst, p, p + 1);
+#endif
 		break;
 	case err_chunk:
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_err_chunk(rqst);
+#endif
 		break;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	default:
 		trace_xprtrdma_err_unrecognized(rqst, p);
+#endif
 	}
 
 	return -EIO;
@@ -1346,16 +1412,28 @@ rpcrdma_decode_error(struct rpcrdma_xprt
  */
 void rpcrdma_unpin_rqst(struct rpcrdma_rep *rep)
 {
+#ifdef HAVE_XPRT_PIN_RQST
 	struct rpc_xprt *xprt = &rep->rr_rxprt->rx_xprt;
+#endif
 	struct rpc_rqst *rqst = rep->rr_rqst;
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 
 	req->rl_reply = NULL;
 	rep->rr_rqst = NULL;
 
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
 	xprt_unpin_rqst(rqst);
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#endif
 }
 
 /**
@@ -1369,7 +1447,9 @@ void rpcrdma_unpin_rqst(struct rpcrdma_r
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep)
 {
 	struct rpcrdma_xprt *r_xprt = rep->rr_rxprt;
+#ifdef HAVE_XPRT_PIN_RQST
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
+#endif
 	struct rpc_rqst *rqst = rep->rr_rqst;
 	int status;
 
@@ -1390,20 +1470,39 @@ void rpcrdma_complete_rqst(struct rpcrdm
 		goto out_badheader;
 
 out:
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
+
 	xprt_complete_rqst(rqst->rq_task, status);
+
+#ifdef HAVE_XPRT_PIN_RQST
 	xprt_unpin_rqst(rqst);
+
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
+
 	return;
 
 out_badheader:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_reply_hdr_err(rep);
+#endif
 	r_xprt->rx_stats.bad_reply_count++;
 	rqst->rq_task->tk_status = status;
 	status = 0;
 	goto out;
 }
 
+#ifdef HAVE_XPRT_PIN_RQST
 static void rpcrdma_reply_done(struct kref *kref)
 {
 	struct rpcrdma_req *req =
@@ -1411,6 +1510,7 @@ static void rpcrdma_reply_done(struct kr
 
 	rpcrdma_complete_rqst(req->rl_reply);
 }
+#endif
 
 /**
  * rpcrdma_reply_handler - Process received RPC/RDMA messages
@@ -1436,8 +1536,13 @@ void rpcrdma_reply_handler(struct rpcrdm
 		xprt->reestablish_timeout = 0;
 
 	/* Fixed transport header fields */
+#ifdef HAVE_XDR_INIT_DECODE_RQST_ARG
 	xdr_init_decode(&rep->rr_stream, &rep->rr_hdrbuf,
 			rep->rr_hdrbuf.head[0].iov_base, NULL);
+#else
+	xdr_init_decode(&rep->rr_stream, &rep->rr_hdrbuf,
+			rep->rr_hdrbuf.head[0].iov_base);
+#endif
 	p = xdr_inline_decode(&rep->rr_stream, 4 * sizeof(*p));
 	if (unlikely(!p))
 		goto out_shortreply;
@@ -1455,12 +1560,36 @@ void rpcrdma_reply_handler(struct rpcrdm
 	/* Match incoming rpcrdma_rep to an rpcrdma_req to
 	 * get context for handling any incoming chunks.
 	 */
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
 	rqst = xprt_lookup_rqst(xprt, rep->rr_xid);
 	if (!rqst)
 		goto out_norqst;
+
 	xprt_pin_rqst(rqst);
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+
+	req = rpcr_to_rdmar(rqst);
+#else /* HAVE_XPRT_PIN_RQST */
+	spin_lock(&buf->rb_lock);
+	req = rpcrdma_lookup_req_locked(&r_xprt->rx_buf, rep->rr_xid);
+	if (!req) {
+		spin_unlock(&buf->rb_lock);
+		goto out;
+	}
+
+	/* Avoid races with signals and duplicate replies
+	 * by marking this req as matched.
+	 */
+#endif /* HAVE_XPRT_PIN_RQST */
 
 	if (credits == 0)
 		credits = 1;	/* don't deadlock */
@@ -1471,14 +1600,25 @@ void rpcrdma_reply_handler(struct rpcrdm
 	if (buf->rb_credits != credits)
 		rpcrdma_update_cwnd(r_xprt, credits);
 
-	req = rpcr_to_rdmar(rqst);
 	if (unlikely(req->rl_reply))
+#ifdef HAVE_XPRT_PIN_RQST
 		rpcrdma_rep_put(buf, req->rl_reply);
+#else
+		rpcrdma_recv_buffer_put_locked(req->rl_reply);
+#endif
 	req->rl_reply = rep;
+
+#ifdef HAVE_XPRT_PIN_RQST
 	rep->rr_rqst = rqst;
+#else
+	spin_unlock(&buf->rb_lock);
+#endif
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_reply(rqst->rq_task, rep, credits);
+#endif
 
+#ifdef HAVE_XPRT_PIN_RQST
 	if (rep->rr_wc_flags & IB_WC_WITH_INVALIDATE)
 		frwr_reminv(rep, &req->rl_registered);
 	if (!list_empty(&req->rl_registered))
@@ -1486,19 +1626,57 @@ void rpcrdma_reply_handler(struct rpcrdm
 		/* LocalInv completion will complete the RPC */
 	else
 		kref_put(&req->rl_kref, rpcrdma_reply_done);
+#else
+#ifdef HAVE_RPC_XPRT_RECV_LOCK
+	spin_lock(&xprt->recv_lock);
+#else
+	spin_lock_bh(&xprt->transport_lock);
+#endif
+
+	rqst = xprt_lookup_rqst(xprt, rep->rr_xid);
+	if (!rqst) {
+#ifdef HAVE_RPC_XPRT_RECV_LOCK
+	spin_unlock(&xprt->recv_lock);
+#else
+	spin_unlock_bh(&xprt->transport_lock);
+#endif
+		goto out;
+	}
+
+	rep->rr_rqst = rqst;
+	rpcrdma_complete_rqst(rep);
+#ifdef HAVE_RPC_XPRT_RECV_LOCK
+	spin_unlock(&xprt->recv_lock);
+#else
+	spin_unlock_bh(&xprt->transport_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
+
 	return;
 
 out_badversion:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_reply_vers_err(rep);
+#endif
 	goto out;
 
+#ifdef HAVE_XPRT_PIN_RQST
 out_norqst:
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_reply_rqst_err(rep);
+#endif
 	goto out;
+#endif
 
 out_shortreply:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_reply_short_err(rep);
+#endif
 
 out:
 	rpcrdma_rep_put(buf, rep);
