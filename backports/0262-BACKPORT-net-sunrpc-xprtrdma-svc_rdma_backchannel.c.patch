From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/svc_rdma_backchannel.c

Change-Id: I7bf2b55e064b74340fa027b9830ed01666ae8b27
---
 net/sunrpc/xprtrdma/svc_rdma_backchannel.c | 75 +++++++++++++++++++---
 1 file changed, 67 insertions(+), 8 deletions(-)

--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -8,7 +8,9 @@
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 /**
  * svc_rdma_handle_bc_reply - Process incoming backchannel Reply
@@ -24,11 +26,21 @@ void svc_rdma_handle_bc_reply(struct svc
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	struct xdr_buf *rcvbuf = &rqstp->rq_arg;
 	struct kvec *dst, *src = &rcvbuf->head[0];
+#ifdef HAVE_SVC_FILL_WRITE_VECTOR
 	__be32 *rdma_resp = rctxt->rc_recv_buf;
+#else
+	__be32 *rdma_resp = (__be32 *)src->iov_base;
+#endif
 	struct rpc_rqst *req;
 	u32 credits;
 
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#elif defined HAVE_RPC_XPRT_RECV_LOCK
+	spin_lock(&xprt->recv_lock);
+#else /* HAVE_XPRT_PIN_RQST is undefined in this case */
+	spin_lock_bh(&xprt->transport_lock);
+#endif
 	req = xprt_lookup_rqst(xprt, *rdma_resp);
 	if (!req)
 		goto out_unlock;
@@ -38,25 +50,52 @@ void svc_rdma_handle_bc_reply(struct svc
 	if (dst->iov_len < src->iov_len)
 		goto out_unlock;
 	memcpy(dst->iov_base, src->iov_base, src->iov_len);
+
+#ifdef HAVE_XPRT_PIN_RQST
 	xprt_pin_rqst(req);
+
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#else
+	spin_unlock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
 
 	credits = be32_to_cpup(rdma_resp + 2);
 	if (credits == 0)
 		credits = 1;	/* don't deadlock */
 	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)
 		credits = r_xprt->rx_buf.rb_bc_max_requests;
+#if defined(HAVE_RPC_XPRT_RECV_LOCK)|| defined(HAVE_XPRT_QUEUE_LOCK)
 	spin_lock(&xprt->transport_lock);
+#endif
 	xprt->cwnd = credits << RPC_CWNDSHIFT;
+#if defined(HAVE_RPC_XPRT_RECV_LOCK)|| defined(HAVE_XPRT_QUEUE_LOCK)
 	spin_unlock(&xprt->transport_lock);
+#endif
 
+#ifdef HAVE_XPRT_PIN_RQST
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_lock(&xprt->queue_lock);
+#else
+	spin_lock(&xprt->recv_lock);
+#endif
+#endif /* HAVE_XPRT_PIN_RQST */
 	xprt_complete_rqst(req->rq_task, rcvbuf->len);
+
+#ifdef HAVE_XPRT_PIN_RQST
 	xprt_unpin_rqst(req);
+#endif
 	rcvbuf->len = 0;
 
 out_unlock:
+#ifdef HAVE_XPRT_QUEUE_LOCK
 	spin_unlock(&xprt->queue_lock);
+#elif defined HAVE_RPC_XPRT_RECV_LOCK
+	spin_unlock(&xprt->recv_lock);
+#else
+	spin_unlock_bh(&xprt->transport_lock);
+#endif
 }
 
 /* Send a reverse-direction RPC Call.
@@ -76,8 +115,9 @@ static int svc_rdma_bc_sendto(struct svc
 			      struct rpc_rqst *rqst,
 			      struct svc_rdma_send_ctxt *sctxt)
 {
-	struct svc_rdma_recv_ctxt *rctxt;
 	int ret;
+#ifdef HAVE_SVC_RDMA_PCL
+	struct svc_rdma_recv_ctxt *rctxt;
 
 	rctxt = svc_rdma_recv_ctxt_get(rdma);
 	if (!rctxt)
@@ -85,6 +125,10 @@ static int svc_rdma_bc_sendto(struct svc
 
 	ret = svc_rdma_map_reply_msg(rdma, sctxt, rctxt, &rqst->rq_snd_buf);
 	svc_rdma_recv_ctxt_put(rdma, rctxt);
+#else
+
+	ret = svc_rdma_map_reply_msg(rdma, sctxt, NULL, &rqst->rq_snd_buf);
+#endif
 	if (ret < 0)
 		return -EIO;
 
@@ -93,13 +137,7 @@ static int svc_rdma_bc_sendto(struct svc
 	 */
 	get_page(virt_to_page(rqst->rq_buffer));
 	sctxt->sc_send_wr.opcode = IB_WR_SEND;
-	ret = svc_rdma_send(rdma, sctxt);
-	if (ret < 0)
-		return ret;
-
-	ret = wait_for_completion_killable(&sctxt->sc_done);
-	svc_rdma_send_ctxt_put(rdma, sctxt);
-	return ret;
+	return svc_rdma_send(rdma, sctxt);
 }
 
 /* Server-side transport endpoint wants a whole page for its send
@@ -186,8 +224,14 @@ drop_connection:
  *   %0 if the message was sent successfully
  *   %ENOTCONN if the message was not sent
  */
+#ifdef HAVE_XPRT_OPS_SEND_REQUEST_RQST_ARG
 static int xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
 {
+#else
+static int xprt_rdma_bc_send_request(struct rpc_task *task)
+{
+	struct rpc_rqst *rqst = task->tk_rqstp;
+#endif
 	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
 	struct svcxprt_rdma *rdma =
 		container_of(sxprt, struct svcxprt_rdma, sc_xprt);
@@ -198,7 +242,11 @@ static int xprt_rdma_bc_send_request(str
 
 	ret = rpcrdma_bc_send_request(rdma, rqst);
 	if (ret == -ENOTCONN)
+#ifdef HAVE_SVC_XPRT_CLOSE
+		svc_xprt_close(sxprt);
+#else
 		svc_close_xprt(sxprt);
+#endif
 	return ret;
 }
 
@@ -216,16 +264,27 @@ xprt_rdma_bc_put(struct rpc_xprt *xprt)
 	xprt_free(xprt);
 }
 
+#ifdef HAVE_RPC_XPRT_OPS_CONST
 static const struct rpc_xprt_ops xprt_rdma_bc_procs = {
+#else
+static struct rpc_xprt_ops xprt_rdma_bc_procs = {
+#endif
 	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong,
 	.alloc_slot		= xprt_alloc_slot,
+#ifdef HAVE_RPC_XPRT_OPS_FREE_SLOT
 	.free_slot		= xprt_free_slot,
+#endif
 	.release_request	= xprt_release_rqst_cong,
 	.buf_alloc		= xprt_rdma_bc_allocate,
 	.buf_free		= xprt_rdma_bc_free,
 	.send_request		= xprt_rdma_bc_send_request,
+#ifdef HAVE_RPC_XPRT_OPS_SET_RETRANS_TIMEOUT
+	.set_retrans_timeout	= xprt_set_retrans_timeout_def,
+#endif
+#ifdef HAVE_RPC_XPRT_OPS_WAIT_FOR_REPLY_REQUEST
 	.wait_for_reply_request	= xprt_wait_for_reply_request_def,
+#endif
 	.close			= xprt_rdma_bc_close,
 	.destroy		= xprt_rdma_bc_put,
 	.print_stats		= xprt_rdma_print_stats
