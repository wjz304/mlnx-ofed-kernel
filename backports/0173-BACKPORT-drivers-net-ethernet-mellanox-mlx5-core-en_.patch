From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_rx.c

Change-Id: I96f06f3db19fecedaaad5be976d5b17d5c919930
---
 .../net/ethernet/mellanox/mlx5/core/en_rx.c   | 802 +++++++++++++++++-
 1 file changed, 765 insertions(+), 37 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_rx.c
@@ -36,12 +36,25 @@
 #include <linux/bitmap.h>
 #include <linux/filter.h>
 #include <net/ip6_checksum.h>
+#ifdef HAVE_BASECODE_EXTRAS
+#include <net/xdp.h>
+#endif
+#ifdef HAVE_NET_PAGE_POOL_OLD_H
 #include <net/page_pool.h>
+#endif
+#ifdef HAVE_NET_PAGE_POOL_TYPES_H
+#include <net/page_pool/types.h>
+#include <net/page_pool/helpers.h>
+#endif
 #include <net/inet_ecn.h>
+#ifdef HAVE_NET_GRO_H
 #include <net/gro.h>
+#endif
 #include <net/udp.h>
 #include <net/tcp.h>
+#ifdef HAVE_XDP_SOCK_DRV_H
 #include <net/xdp_sock_drv.h>
+#endif
 #include "en.h"
 #include "en/txrx.h"
 #include "en_tc.h"
@@ -60,6 +73,16 @@
 #include "devlink.h"
 #include "en/devlink.h"
 
+#ifdef HAVE_BASECODE_EXTRAS
+static inline void mlx5e_set_skb_driver_xmit_more(struct sk_buff *skb,
+						  struct mlx5e_rq *rq,
+						  bool xmit_more)
+{
+	if (test_bit(MLX5E_RQ_STATE_SKB_XMIT_MORE, &rq->state) && xmit_more)
+		skb->cb[47] = MLX5_XMIT_MORE_SKB_CB;
+}
+#endif
+
 static struct sk_buff *
 mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 				struct mlx5_cqe64 *cqe, u16 cqe_bcnt, u32 head_offset,
@@ -68,14 +91,30 @@ static struct sk_buff *
 mlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 				   struct mlx5_cqe64 *cqe, u16 cqe_bcnt, u32 head_offset,
 				   u32 page_idx);
-static void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
-static void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
-static void mlx5e_handle_rx_cqe_mpwrq_shampo(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
+static void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+				, bool xmit_more
+#endif
+				);
+static void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+				      , bool xmit_more
+#endif
+				      );
+#ifdef HAVE_SHAMPO_SUPPORT
+static void mlx5e_handle_rx_cqe_mpwrq_shampo(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+					     , bool xmit_more
+#endif
+					     );
+#endif
 
 const struct mlx5e_rx_handlers mlx5e_rx_handlers_nic = {
 	.handle_rx_cqe       = mlx5e_handle_rx_cqe,
 	.handle_rx_cqe_mpwqe = mlx5e_handle_rx_cqe_mpwrq,
+#ifdef HAVE_SHAMPO_SUPPORT
 	.handle_rx_cqe_mpwqe_shampo = mlx5e_handle_rx_cqe_mpwrq_shampo,
+#endif
 };
 
 static inline void mlx5e_read_cqe_slot(struct mlx5_cqwq *wq,
@@ -213,9 +252,22 @@ static u32 mlx5e_decompress_enhanced_cqe
 	memcpy(cqd->mini_arr, cqe, sizeof(struct mlx5_cqe64));
 	for (i = 0; i < left; i++, cqd->mini_arr_idx++, cqcc++) {
 		mlx5e_decompress_cqe_no_hash(rq, wq, cqcc);
+#ifdef HAVE_SHAMPO_SUPPORT
 		INDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
 				mlx5e_handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq_shampo,
-				rq, &cqd->title);
+				rq, &cqd->title
+#ifdef HAVE_BASECODE_EXTRAS
+				, i < left - 1
+#endif
+				);
+#else
+		INDIRECT_CALL_2(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
+				mlx5e_handle_rx_cqe, rq, &cqd->title
+#ifdef HAVE_BASECODE_EXTRAS
+				, i < left - 1
+#endif
+				);
+#endif
 	}
 	wq->cc = cqcc;
 	rq->stats->cqe_compress_pkts += left;
@@ -241,9 +293,22 @@ static inline u32 mlx5e_decompress_cqes_
 			mlx5e_read_mini_arr_slot(wq, cqd, cqcc);
 
 		mlx5e_decompress_cqe_no_hash(rq, wq, cqcc);
+#ifdef HAVE_SHAMPO_SUPPORT
 		INDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
 				mlx5e_handle_rx_cqe_mpwrq_shampo, mlx5e_handle_rx_cqe,
-				rq, &cqd->title);
+				rq, &cqd->title
+#ifdef HAVE_BASECODE_EXTRAS
+				, i < cqe_count - 1
+#endif
+				);
+#else
+		INDIRECT_CALL_2(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
+				mlx5e_handle_rx_cqe, rq, &cqd->title
+#ifdef HAVE_BASECODE_EXTRAS
+				, i < cqe_count - 1
+#endif
+				);
+#endif
 	}
 	mlx5e_cqes_update_owner(wq, cqcc - wq->cc);
 	wq->cc = cqcc;
@@ -263,14 +328,34 @@ static inline u32 mlx5e_decompress_cqes_
 	mlx5e_read_title_slot(rq, wq, cc);
 	mlx5e_read_mini_arr_slot(wq, cqd, cc + 1);
 	mlx5e_decompress_cqe(rq, wq, cc);
+#ifdef HAVE_SHAMPO_SUPPORT
 	INDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
 			mlx5e_handle_rx_cqe_mpwrq_shampo, mlx5e_handle_rx_cqe,
-			rq, &cqd->title);
+			rq, &cqd->title
+#ifdef HAVE_BASECODE_EXTRAS
+			, true
+#endif
+			);
+#else
+	INDIRECT_CALL_2(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
+			mlx5e_handle_rx_cqe, rq, &cqd->title
+#ifdef HAVE_BASECODE_EXTRAS
+			, true
+#endif
+			);
+#endif
 	cqd->mini_arr_idx++;
 
 	return mlx5e_decompress_cqes_cont(rq, wq, 1, budget_rem);
 }
 
+#ifndef HAVE_DEV_PAGE_IS_REUSABLE
+static inline bool mlx5e_page_is_reserved(struct page *page)
+{
+	return page_is_pfmemalloc(page) || page_to_nid(page) != numa_mem_id();
+}
+#endif
+
 static inline void mlx5e_rx_cache_page_swap(struct mlx5e_page_cache *cache,
 					    u32 a, u32 b)
 {
@@ -398,12 +483,28 @@ static inline bool mlx5e_rx_cache_put(st
 		}
 	}
 
+#ifdef HAVE_DEV_PAGE_IS_REUSABLE
 	if (!dev_page_is_reusable(au->page)) {
 		stats->cache_waive++;
 		return false;
 	}
+#else
+	if (unlikely(mlx5e_page_is_reserved(au->page))) {
+		stats->cache_waive++;
+		return false;
+	}
+#endif
 
 	cache->page_cache[++cache->head] = *au;
+#if !defined(HAVE_PAGE_POOL_GET_DMA_ADDR) || !defined(HAVE_XSK_BUFF_GET_FRAME_DMA)
+#if defined(HAVE_PAGE_DMA_ADDR_ARRAY)
+	cache->page_cache[cache->head].addr = au->page->dma_addr[0];
+#elif defined(HAVE_PAGE_DMA_ADDR)
+	cache->page_cache[cache->head].addr = au->page->dma_addr;
+#else
+	cache->page_cache[cache->head].addr = au->addr;
+#endif
+#endif
 	return true;
 }
 
@@ -426,7 +527,9 @@ static inline bool mlx5e_rx_cache_get(st
 {
 	struct mlx5e_page_cache *cache = &rq->page_cache;
 	struct mlx5e_rq_stats *stats = rq->stats;
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	dma_addr_t addr;
+#endif
 
 	if (unlikely(mlx5e_rx_cache_is_empty(cache)))
 		goto err_no_page;
@@ -439,12 +542,16 @@ static inline bool mlx5e_rx_cache_get(st
 		goto err_no_page;
 
 
-	au->page = cache->page_cache[cache->head--].page;
+	*au = cache->page_cache[cache->head--];
 	stats->cache_reuse++;
 
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	addr = page_pool_get_dma_addr(au->page);
 	/* Non-XSK always uses PAGE_SIZE. */
 	dma_sync_single_for_device(rq->pdev, addr, PAGE_SIZE, rq->buff.map_dir);
+#else
+	dma_sync_single_for_device(rq->pdev, au->addr, PAGE_SIZE, rq->buff.map_dir);
+#endif
 
 	if (unlikely(page_ref_count(au->page) <= PAGE_REF_THRSD))
 		page_ref_elev(au);
@@ -465,7 +572,11 @@ static inline int mlx5e_page_alloc_pool(
 	if (mlx5e_rx_cache_get(rq, au))
 		return 0;
 
+#ifdef HAVE_NET_PAGE_POOL_H
 	au->page = page_pool_dev_alloc_pages(rq->page_pool);
+#else
+	au->page = dev_alloc_page();
+#endif
 	if (unlikely(!au->page))
 		return -ENOMEM;
 
@@ -474,43 +585,176 @@ static inline int mlx5e_page_alloc_pool(
 
 	/* Non-XSK always uses PAGE_SIZE. */
 	addr = dma_map_page(rq->pdev, au->page, 0, PAGE_SIZE, rq->buff.map_dir);
+#if !defined(HAVE_PAGE_POOL_GET_DMA_ADDR) || !defined(HAVE_XSK_BUFF_GET_FRAME_DMA)
+	au->addr = addr;
+#endif
 	if (unlikely(dma_mapping_error(rq->pdev, addr))) {
-		page_pool_recycle_direct(rq->page_pool, au->page);
 		page_ref_sub(au->page, au->refcnt_bias);
+#ifdef HAVE_NET_PAGE_POOL_H
+		page_pool_recycle_direct(rq->page_pool, au->page);
+#else
+		mlx5e_put_page(au->page);
+#endif
 		au->page = NULL;
 		return -ENOMEM;
 	}
+
+#ifdef HAVE_PAGE_DMA_ADDR
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	page_pool_set_dma_addr(au->page, addr);
+#elif defined (HAVE_PAGE_DMA_ADDR_ARRAY)
+	au->page->dma_addr[0] = addr;
+#else
+	au->page->dma_addr = addr;
+#endif
+#endif /* HAVE_PAGE_DMA_ADDR */
 
 	return 0;
 }
 
-void mlx5e_page_dma_unmap(struct mlx5e_rq *rq, struct page *page)
+#ifndef HAVE_XSK_BUFF_ALLOC_BATCH
+static inline int mlx5e_page_alloc(struct mlx5e_rq *rq, struct mlx5e_alloc_unit *au)
 {
-	dma_addr_t dma_addr = page_pool_get_dma_addr(page);
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_XSK_BUFF_ALLOC
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+	if (rq->xsk_pool) {
+		au->xsk = xsk_buff_alloc(rq->xsk_pool);
+		return likely(au->xsk) ? 0 : -ENOMEM;
+#else
+	if (rq->umem) {
+		au->xsk = xsk_buff_alloc(rq->umem);
+		return likely(au->xsk) ? 0 : -ENOMEM;
+#endif /* HAVE_NETDEV_BPF_XSK_BUFF_POOL */
+#else
+	if (rq->umem) {
+		return mlx5e_xsk_page_alloc_pool(rq, au);
+#endif /* HAVE_XSK_BUFF_ALLOC */
+	}
+	else
+#endif /* HAVE_XSK_ZERO_COPY */
+		return mlx5e_page_alloc_pool(rq, au);
+}
+#endif /* HAVE_XSK_BUFF_ALLOC_BATCH */
+
+void mlx5e_page_dma_unmap(struct mlx5e_rq *rq,
+#ifdef HAVE_PAGE_DMA_ADDR
+			  struct page *page)
+#else
+			  struct mlx5e_alloc_unit *au)
+#endif
+{
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
+	dma_addr_t dma_addr = page_pool_get_dma_addr(page);
+#elif defined(HAVE_PAGE_DMA_ADDR_ARRAY)
+	dma_addr_t dma_addr = page->dma_addr[0];
+#elif defined(HAVE_PAGE_DMA_ADDR)
+	dma_addr_t dma_addr = page->dma_addr;
+#else
+	dma_addr_t dma_addr = au->addr;
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	dma_unmap_page_attrs(rq->pdev, dma_addr, PAGE_SIZE, rq->buff.map_dir,
 			     DMA_ATTR_SKIP_CPU_SYNC);
+#else
+	dma_unmap_page(rq->pdev, dma_addr, PAGE_SIZE, rq->buff.map_dir);
+#endif
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	page_pool_set_dma_addr(page, 0);
+#elif defined(HAVE_PAGE_DMA_ADDR_ARRAY)
+	page->dma_addr[0] = 0;
+#elif defined(HAVE_PAGE_DMA_ADDR)
+	page->dma_addr = 0;
+#else
+	au->addr = 0;
+#endif
 }
 
+#if !defined(HAVE_PAGE_POOL_RELEASE_PAGE) && defined(HAVE_PAGE_POOL_DEFRAG_PAGE)
+static void page_pool_clear_pp_info(struct page *page)
+{
+	page->pp_magic = 0;
+	page->pp = NULL;
+}
+
+/* Disconnects a page (from a page_pool).  API users can have a need
+ * to disconnect a page (from a page_pool), to allow it to be used as
+ * a regular page (that will eventually be returned to the normal
+ * page-allocator via put_page).
+ */
+void page_pool_release_page(struct page_pool *pool, struct page *page)
+{
+	dma_addr_t dma;
+	int count;
+
+	if (!(pool->p.flags & PP_FLAG_DMA_MAP))
+		/* Always account for inflight pages, even if we didn't
+		 * map them
+		 */
+		goto skip_dma_unmap;
+
+	dma = page_pool_get_dma_addr(page);
+
+	/* When page is unmapped, it cannot be returned to our pool */
+	dma_unmap_page_attrs(pool->p.dev, dma,
+			     PAGE_SIZE << pool->p.order, pool->p.dma_dir,
+			     DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING);
+	page_pool_set_dma_addr(page, 0);
+skip_dma_unmap:
+	page_pool_clear_pp_info(page);
+
+	/* This may be the last page returned, releasing the pool, so
+	 * it is not safe to reference pool afterwards.
+	 */
+	count = atomic_inc_return_relaxed(&pool->pages_state_release_cnt);
+}
+EXPORT_SYMBOL(page_pool_release_page);
+#endif
+
 void mlx5e_page_release_dynamic(struct mlx5e_rq *rq, struct mlx5e_alloc_unit *au, bool recycle)
 {
+#ifdef HAVE_NET_PAGE_POOL_H
 	if (likely(recycle)) {
 		if (mlx5e_rx_cache_put(rq, au))
 			return;
 
+#ifdef HAVE_PAGE_DMA_ADDR
 		mlx5e_page_dma_unmap(rq, au->page);
+#else
+		mlx5e_page_dma_unmap(rq, au);
+#endif
 		if (au->refcnt_bias)
 			page_ref_sub(au->page, au->refcnt_bias);
 		page_pool_recycle_direct(rq->page_pool, au->page);
 	} else {
+#ifdef HAVE_PAGE_DMA_ADDR
 		mlx5e_page_dma_unmap(rq, au->page);
+#else
+		mlx5e_page_dma_unmap(rq, au);
+#endif
+#ifdef HAVE_PAGE_POOL_RELEASE_PAGE
+		page_pool_release_page(rq->page_pool, au->page);
+#elif defined(HAVE_PAGE_POOL_DEFRAG_PAGE)
 		page_pool_release_page(rq->page_pool, au->page);
+#endif
+
 		if (au->refcnt_bias)
 			page_ref_sub(au->page, au->refcnt_bias);
 		mlx5e_put_page(au->page);
 	}
+#else
+	if (likely(recycle) && mlx5e_rx_cache_put(rq, au))
+		return;
+
+#ifdef HAVE_PAGE_DMA_ADDR
+	mlx5e_page_dma_unmap(rq, au->page);
+#else
+	mlx5e_page_dma_unmap(rq, au);
+#endif
+	page_ref_sub(au->page, au->refcnt_bias);
+	mlx5e_put_page(au->page);
+#endif
 }
 
 static inline int mlx5e_get_rx_frag(struct mlx5e_rq *rq,
@@ -558,7 +802,21 @@ static int mlx5e_alloc_rx_wqe(struct mlx
 			goto free_frags;
 
 		headroom = i == 0 ? rq->buff.headroom : 0;
-		addr = page_pool_get_dma_addr(frag->au->page);
+	 	addr = 
+#if defined(HAVE_PAGE_POOL_GET_DMA_ADDR) && defined(HAVE_XSK_BUFF_GET_FRAME_DMA)
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+		rq->xsk_pool ?
+			xsk_buff_xdp_get_frame_dma(frag->au->xsk) :
+#else
+			rq->umem ?
+			xsk_buff_xdp_get_frame_dma(frag->au->xsk) :
+#endif
+#endif /* HAVE_XSK_ZERO_COPY_SUPPORT*/
+			page_pool_get_dma_addr(frag->au->page);
+#else
+		frag->au->addr;
+#endif
 		wqe->data[i].addr = cpu_to_be64(addr + frag->offset + headroom);
 	}
 
@@ -577,6 +835,8 @@ static inline void mlx5e_free_rx_wqe(str
 {
 	int i;
 
+#ifdef HAVE_XDP_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	if (rq->xsk_pool) {
 		/* The `recycle` parameter is ignored, and the page is always
 		 * put into the Reuse Ring, because there is no way to return
@@ -585,6 +845,8 @@ static inline void mlx5e_free_rx_wqe(str
 		xsk_buff_free(wi->au->xsk);
 		return;
 	}
+#endif
+#endif
 
 	for (i = 0; i < rq->wqe.info.num_frags; i++, wi++)
 		mlx5e_put_rx_frag(rq, wi, recycle);
@@ -612,7 +874,7 @@ static int mlx5e_alloc_rx_wqes(struct ml
 			break;
 	}
 
-	return i;
+		return i;
 }
 
 static inline void
@@ -620,10 +882,16 @@ mlx5e_add_skb_frag(struct mlx5e_rq *rq,
 		   struct mlx5e_alloc_unit *au, u32 frag_offset, u32 len,
 		   unsigned int truesize)
 {
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	dma_addr_t addr = page_pool_get_dma_addr(au->page);
 
 	dma_sync_single_for_cpu(rq->pdev, addr + frag_offset, len,
 				rq->buff.map_dir);
+#else
+	dma_sync_single_for_cpu(rq->pdev, au->addr + frag_offset, len,
+				rq->buff.map_dir);
+#endif
+
 	au->refcnt_bias--;
 
 	skb_add_rx_frag(skb, skb_shinfo(skb)->nr_frags,
@@ -647,6 +915,7 @@ mlx5e_copy_skb_header(struct mlx5e_rq *r
 static void
 mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi, bool recycle)
 {
+#ifdef HAVE_XDP_SUPPORT
 	struct mlx5e_alloc_unit *alloc_units = wi->alloc_units;
 	bool no_xdp_xmit;
 	int i;
@@ -657,6 +926,7 @@ mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq,
 
 	no_xdp_xmit = bitmap_empty(wi->xdp_xmit_bitmap, rq->mpwqe.pages_per_wqe);
 
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	if (rq->xsk_pool) {
 		/* The `recycle` parameter is ignored, and the page is always
 		 * put into the Reuse Ring, because there is no way to return
@@ -665,11 +935,20 @@ mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq,
 		for (i = 0; i < rq->mpwqe.pages_per_wqe; i++)
 			if (no_xdp_xmit || !test_bit(i, wi->xdp_xmit_bitmap))
 				xsk_buff_free(alloc_units[i].xsk);
-	} else {
+	} else
+#endif
+	{
 		for (i = 0; i < rq->mpwqe.pages_per_wqe; i++)
 			if (no_xdp_xmit || !test_bit(i, wi->xdp_xmit_bitmap))
 				mlx5e_page_release_dynamic(rq, &alloc_units[i], recycle);
 	}
+#else
+	struct mlx5e_alloc_unit *alloc_units = &wi->alloc_units[0];
+	int i;
+
+	for (i = 0; i < rq->mpwqe.pages_per_wqe; i++, alloc_units++)
+		mlx5e_page_release_dynamic(rq, alloc_units, recycle);
+#endif
 }
 
 static void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq, u8 n)
@@ -683,7 +962,11 @@ static void mlx5e_post_rx_mpwqe(struct m
 	} while (--n);
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_ll_update_db_record(wq);
 
@@ -762,7 +1045,12 @@ static int mlx5e_build_shampo_hd_umr(str
 			if (unlikely(err))
 				goto err_unmap;
 			page = dma_info->page = au.page;
-			addr = dma_info->addr = page_pool_get_dma_addr(au.page);
+			addr = dma_info->addr =
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
+				page_pool_get_dma_addr(au.page);
+#else
+ 				au.addr;
+#endif
 		} else {
 			dma_info->addr = addr + header_offset;
 			dma_info->page = page;
@@ -857,6 +1145,22 @@ static int mlx5e_alloc_rx_mpwqe(struct m
 	int err;
 	int i;
 
+#if defined(HAVE_XSK_ZERO_COPY_SUPPORT) && !defined(HAVE_XSK_BUFF_ALLOC_BATCH)
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+	if (rq->xsk_pool &&
+	    unlikely(!xsk_buff_can_alloc(rq->xsk_pool, rq->mpwqe.pages_per_wqe))) {
+#elif defined(HAVE_XSK_BUFF_ALLOC)
+	if (rq->umem &&
+	    unlikely(!xsk_buff_can_alloc(rq->umem, rq->mpwqe.pages_per_wqe))) {
+
+#else
+	if (rq->umem &&
+	    unlikely(!mlx5e_xsk_pages_enough_umem(rq, rq->mpwqe.pages_per_wqe))) {
+#endif
+		err = -ENOMEM;
+			goto err;
+	}
+#endif /* HAVE_XSK_ZERO_COPY_SUPPORT && !HAVE_XSK_BUFF_ALLOC_BATCH */
 	if (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state)) {
 		err = mlx5e_alloc_rx_hd_mpwqe(rq);
 		if (unlikely(err))
@@ -867,17 +1171,71 @@ static int mlx5e_alloc_rx_mpwqe(struct m
 	umr_wqe = mlx5_wq_cyc_get_wqe(wq, pi);
 	memcpy(umr_wqe, &rq->mpwqe.umr_wqe, sizeof(struct mlx5e_umr_wqe));
 
+#ifndef HAVE_XSK_BUFF_ALLOC_BATCH
+	if (unlikely(rq->mpwqe.umr_mode)) {
+		for (i = 0; i < rq->mpwqe.pages_per_wqe; i++, au++) {
+			dma_addr_t addr;
+
+			err = mlx5e_page_alloc(rq, au);
+			if (unlikely(err))
+				goto err_unmap;
+			/* Unaligned means XSK. */
+			addr =
+#if defined(HAVE_PAGE_POOL_GET_DMA_ADDR) && defined(HAVE_XSK_BUFF_GET_FRAME_DMA)
+				xsk_buff_xdp_get_frame_dma(au->xsk);
+#else
+				au->addr;
+#endif
+			umr_wqe->inline_ksms[i] = (struct mlx5_ksm) {
+				.key = rq->mkey_be,
+				.va = cpu_to_be64(addr),
+			};
+		}
+	} else {
+		for (i = 0; i < rq->mpwqe.pages_per_wqe; i++, au++) {
+			dma_addr_t addr;
+
+			err = mlx5e_page_alloc(rq, au);
+			if (unlikely(err))
+				goto err_unmap;
+			addr =
+#if defined(HAVE_PAGE_POOL_GET_DMA_ADDR) && defined(HAVE_XSK_BUFF_GET_FRAME_DMA)
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+                 rq->xsk_pool ?
+                         xsk_buff_xdp_get_frame_dma(au->xsk) :
+#else
+		 rq->umem ?
+                         xsk_buff_xdp_get_frame_dma(au->xsk) :
+#endif
+#endif
+				page_pool_get_dma_addr(au->page);
+#else
+			au->addr;
+#endif
+		umr_wqe->inline_mtts[i] = (struct mlx5_mtt) {
+			.ptag = cpu_to_be64(addr | MLX5_EN_WR),
+		};
+		}
+	}
+#else
 	for (i = 0; i < rq->mpwqe.pages_per_wqe; i++, au++) {
 		dma_addr_t addr;
 
 		err = mlx5e_page_alloc_pool(rq, au);
 		if (unlikely(err))
 			goto err_unmap;
-		addr = page_pool_get_dma_addr(au->page);
+                 addr =
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
+		page_pool_get_dma_addr(au->page);
+#else
+		 au->addr;
+#endif
 		umr_wqe->inline_mtts[i] = (struct mlx5_mtt) {
 			.ptag = cpu_to_be64(addr | MLX5_EN_WR),
 		};
 	}
+#endif
 
 	/* Pad if needed, in case the value set to ucseg->xlt_octowords
 	 * in mlx5e_build_umr_wqe() needed alignment.
@@ -890,14 +1248,23 @@ static int mlx5e_alloc_rx_mpwqe(struct m
 		       sizeof(*umr_wqe->inline_mtts) * pad);
 	}
 
+#ifdef HAVE_XDP_SUPPORT
 	bitmap_zero(wi->xdp_xmit_bitmap, rq->mpwqe.pages_per_wqe);
+#endif
 	wi->consumed_strides = 0;
 
 	umr_wqe->ctrl.opmod_idx_opcode =
 		cpu_to_be32((sq->pc << MLX5_WQE_CTRL_WQE_INDEX_SHIFT) |
 			    MLX5_OPCODE_UMR);
 
-	offset = (ix * rq->mpwqe.mtts_per_wqe) * sizeof(struct mlx5_mtt) / MLX5_OCTWORD;
+#ifndef HAVE_XSK_BUFF_ALLOC_BATCH
+	offset = ix * rq->mpwqe.mtts_per_wqe;
+if (!rq->mpwqe.umr_mode)
+		offset = MLX5_ALIGNED_MTTS_OCTW(offset);
+#else
+		offset = (ix * rq->mpwqe.mtts_per_wqe) * sizeof(struct mlx5_mtt) / MLX5_OCTWORD;
+#endif
+
 	umr_wqe->uctrl.xlt_offset = cpu_to_be16(offset);
 
 	sq->db.wqe_info[pi] = (struct mlx5e_icosq_wqe_info) {
@@ -986,9 +1353,10 @@ INDIRECT_CALLABLE_SCOPE bool mlx5e_post_
 	if (mlx5_wq_cyc_missing(wq) < rq->wqe.info.wqe_bulk)
 		return false;
 
+#ifdef HAVE_PAGE_POLL_NID_CHANGED
 	if (rq->page_pool)
 		page_pool_nid_changed(rq->page_pool, numa_mem_id());
-
+#endif
 	wqe_bulk = mlx5_wq_cyc_missing(wq);
 	head = mlx5_wq_cyc_get_head(wq);
 
@@ -997,10 +1365,28 @@ INDIRECT_CALLABLE_SCOPE bool mlx5e_post_
 	 */
 	wqe_bulk -= (head + wqe_bulk) & rq->wqe.info.wqe_index_mask;
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	if (!rq->xsk_pool)
+#else
+	if (!rq->umem)
+#endif
 		count = mlx5e_alloc_rx_wqes(rq, head, wqe_bulk);
+#ifdef HAVE_NO_REFCNT_BIAS
+	else if (likely(!rq->xsk_pool->dma_need_sync))
+		count = mlx5e_xsk_alloc_rx_wqes_batched(rq, head, wqe_bulk);
+	/* If dma_need_sync is true, it's more efficient to call
+	 * xsk_buff_alloc in a loop, rather than xsk_buff_alloc_batch,
+	 * because the latter does the same check and returns only one
+	 * frame.
+	 */
+#endif
 	else
 		count = mlx5e_xsk_alloc_rx_wqes(rq, head, wqe_bulk);
+#else
+		count = mlx5e_alloc_rx_wqes(rq, head, wqe_bulk);
+#endif
+
 
 	mlx5_wq_cyc_push_n(wq, count);
 	if (unlikely(count != wqe_bulk)) {
@@ -1009,7 +1395,11 @@ INDIRECT_CALLABLE_SCOPE bool mlx5e_post_
 	}
 
 	/* ensure wqes are visible to device before updating doorbell record */
+#ifdef dma_wmb
 	dma_wmb();
+#else
+	wmb();
+#endif
 
 	mlx5_wq_cyc_update_db_record(wq);
 
@@ -1041,7 +1431,7 @@ void mlx5e_free_icosq_descs(struct mlx5e
 		ci = mlx5_wq_cyc_ctr2ix(&sq->wq, sqcc);
 		wi = &sq->db.wqe_info[ci];
 		sqcc += wi->num_wqebbs;
-#ifdef CONFIG_MLX5_EN_TLS
+#if defined(HAVE_KTLS_RX_SUPPORT) && defined (CONFIG_MLX5_EN_TLS)
 		switch (wi->wqe_type) {
 		case MLX5E_ICOSQ_WQE_SET_PSV_TLS:
 			mlx5e_ktls_handle_ctx_completion(wi);
@@ -1136,7 +1526,7 @@ int mlx5e_poll_ico_cq(struct mlx5e_cq *c
 			case MLX5E_ICOSQ_WQE_SHAMPO_HD_UMR:
 				mlx5e_handle_shampo_hd_umr(wi->shampo, sq);
 				break;
-#ifdef CONFIG_MLX5_EN_TLS
+#if defined(HAVE_KTLS_RX_SUPPORT) && defined (CONFIG_MLX5_EN_TLS)
 			case MLX5E_ICOSQ_WQE_UMR_TLS:
 				break;
 			case MLX5E_ICOSQ_WQE_SET_PSV_TLS:
@@ -1187,15 +1577,28 @@ INDIRECT_CALLABLE_SCOPE bool mlx5e_post_
 	if (likely(missing < rq->mpwqe.min_wqe_bulk))
 		return false;
 
+#ifdef HAVE_PAGE_POLL_NID_CHANGED
 	if (rq->page_pool)
 		page_pool_nid_changed(rq->page_pool, numa_mem_id());
+#endif
 
 	head = rq->mpwqe.actual_wq_head;
 	i = missing;
 	do {
-		alloc_err = rq->xsk_pool ? mlx5e_xsk_alloc_rx_mpwqe(rq, head) :
+		alloc_err =
+#if defined(HAVE_XSK_BUFF_ALLOC_BATCH) && defined(HAVE_XSK_ZERO_COPY_SUPPORT)
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+			rq->xsk_pool ?
+			mlx5e_xsk_alloc_rx_mpwqe(rq, head) :
 					   mlx5e_alloc_rx_mpwqe(rq, head);
-
+#else
+			rq->umem ?
+			mlx5e_xsk_alloc_rx_mpwqe(rq, head) :
+					   mlx5e_alloc_rx_mpwqe(rq, head);
+#endif
+#else
+					   mlx5e_alloc_rx_mpwqe(rq, head);
+#endif
 		if (unlikely(alloc_err))
 			break;
 		head = mlx5_wq_ll_get_wqe_next_ix(wq, head);
@@ -1216,8 +1619,14 @@ INDIRECT_CALLABLE_SCOPE bool mlx5e_post_
 	 * the driver when it refills the Fill Ring.
 	 * 2. Otherwise, busy poll by rescheduling the NAPI poll.
 	 */
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	if (unlikely(alloc_err == -ENOMEM && rq->xsk_pool))
+#else
+	if (unlikely(alloc_err == -ENOMEM && rq->umem))
+#endif
 		return true;
+#endif
 
 	return false;
 }
@@ -1292,6 +1701,7 @@ static void mlx5e_lro_update_hdr(struct
 	}
 }
 
+#ifdef HAVE_SHAMPO_SUPPORT
 static void *mlx5e_shampo_get_packet_hd(struct mlx5e_rq *rq, u16 header_index)
 {
 	struct mlx5e_dma_info *last_head = &rq->mpwqe.shampo->info[header_index];
@@ -1427,6 +1837,7 @@ static void mlx5e_shampo_update_hdr(stru
 			mlx5e_shampo_update_ipv6_udp_hdr(rq, ipv6);
 	}
 }
+#endif /* HAVE_SHAMPO_SUPPORT */
 
 static inline void mlx5e_skb_set_hash(struct mlx5_cqe64 *cqe,
 				      struct sk_buff *skb)
@@ -1465,7 +1876,11 @@ static inline void mlx5e_enable_ecn(stru
 
 	ip = skb->data + network_depth;
 	rc = ((proto == htons(ETH_P_IP)) ? IP_ECN_set_ce((struct iphdr *)ip) :
+#ifdef HAVE_IP6_SET_CE_2_PARAMS
 					 IP6_ECN_set_ce(skb, (struct ipv6hdr *)ip));
+#else
+					 IP6_ECN_set_ce((struct ipv6hdr *)ip));
+#endif
 
 	rq->stats->ecn_mark += !!rc;
 }
@@ -1632,6 +2047,10 @@ static inline void mlx5e_build_rx_skb(st
 	u8 lro_num_seg = be32_to_cpu(cqe->srqn) >> 24;
 	struct mlx5e_rq_stats *stats = rq->stats;
 	struct net_device *netdev = rq->netdev;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(netdev);
+	u8 l4_hdr_type;
+#endif
 
 	skb->mac_len = ETH_HLEN;
 
@@ -1654,6 +2073,12 @@ static inline void mlx5e_build_rx_skb(st
 		stats->packets += lro_num_seg - 1;
 		stats->lro_packets++;
 		stats->lro_bytes += cqe_bcnt;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4, 10, 0)
+		/* Flush GRO to avoid OOO packets, since GSO bypasses the
+		 * GRO queue. This was fixed in dev_gro_receive() in kernel 4.10
+		 */
+		napi_gro_flush(rq->cq.napi, false);
+#endif
 	}
 
 	if (unlikely(mlx5e_rx_hw_stamp(rq->tstamp)))
@@ -1672,7 +2097,16 @@ static inline void mlx5e_build_rx_skb(st
 
 	skb->mark = be32_to_cpu(cqe->sop_drop_qpn) & MLX5E_TC_FLOW_ID_MASK;
 
+#ifndef CONFIG_COMPAT_LRO_ENABLED_IPOIB
 	mlx5e_handle_csum(netdev, cqe, rq, skb, !!lro_num_seg);
+#else
+	l4_hdr_type = get_cqe_l4_hdr_type(cqe);
+	mlx5e_handle_csum(netdev, cqe, rq, skb,
+			  !!lro_num_seg ||
+			  (IS_SW_LRO(&priv->channels.params) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_NONE) &&
+			  (l4_hdr_type != CQE_L4_HDR_TYPE_UDP)));
+#endif
 	/* checking CE bit in cqe - MSB in ml_path field */
 	if (unlikely(cqe->ml_path & MLX5E_CE_BIT_MASK))
 		mlx5e_enable_ecn(rq, skb);
@@ -1683,6 +2117,7 @@ static inline void mlx5e_build_rx_skb(st
 		stats->mcast_packets++;
 }
 
+#ifdef HAVE_SHAMPO_SUPPORT
 static void mlx5e_shampo_complete_rx_cqe(struct mlx5e_rq *rq,
 					 struct mlx5_cqe64 *cqe,
 					 u32 cqe_bcnt,
@@ -1703,6 +2138,7 @@ static void mlx5e_shampo_complete_rx_cqe
 		rq->hw_gro_data->skb = NULL;
 	}
 }
+#endif
 
 static inline void mlx5e_complete_rx_cqe(struct mlx5e_rq *rq,
 					 struct mlx5_cqe64 *cqe,
@@ -1727,7 +2163,11 @@ struct sk_buff *mlx5e_build_linear_skb(s
 				       u32 frag_size, u16 headroom,
 				       u32 cqe_bcnt, u32 metasize)
 {
+#ifdef HAVE_NAPI_BUILD_SKB
 	struct sk_buff *skb = napi_build_skb(va, frag_size);
+#else
+	struct sk_buff *skb = build_skb(va, frag_size);
+#endif
 
 	if (unlikely(!skb)) {
 		rq->stats->buff_alloc_err++;
@@ -1737,8 +2177,10 @@ struct sk_buff *mlx5e_build_linear_skb(s
 	skb_reserve(skb, headroom);
 	skb_put(skb, cqe_bcnt);
 
+#ifdef HAVE_SKB_METADATA_SET
 	if (metasize)
 		skb_metadata_set(skb, metasize);
+#endif
 
 	return skb;
 }
@@ -1747,34 +2189,69 @@ static void mlx5e_fill_mxbuf(struct mlx5
 			     void *va, u16 headroom, u32 len,
 			     struct mlx5e_xdp_buff *mxbuf)
 {
+#ifdef HAVE_XDP_RXQ_INFO
 	xdp_init_buff(&mxbuf->xdp, rq->buff.frame0_sz, &rq->xdp_rxq);
+#else
+	xdp_init_buff(&mxbuf->xdp, rq->buff.frame0_sz);
+#endif
 	xdp_prepare_buff(&mxbuf->xdp, va, headroom, len, true);
 	mxbuf->cqe = cqe;
 	mxbuf->rq = rq;
 }
 
+#if defined(HAVE_XDP_SUPPORT) && !defined(HAVE_XSK_BUFF_ALLOC) && defined(HAVE_XSK_ZERO_COPY_SUPPORT)
+static void mlx5e_fill_xdp_buff(struct mlx5e_rq *rq, void *va, u16 headroom, u32 len,
+				struct xdp_buff *xdp)
+ {
+ #ifdef HAVE_XDP_RXQ_INFO
+         xdp_init_buff(xdp, rq->buff.frame0_sz, &rq->xdp_rxq);
+ #else
+         xdp_init_buff(xdp, rq->buff.frame0_sz);
+ #endif
+         xdp_prepare_buff(xdp, va, headroom, len, true);
+ }
+
+void mlx5e_fill_xdp_buff_for_old_xsk(struct mlx5e_rq *rq, void *va, u16 headroom,
+				     u32 len, struct xdp_buff *xdp,
+				     struct mlx5e_alloc_unit *au)
+{
+	mlx5e_fill_xdp_buff(rq, va, headroom, len, xdp);
+	xdp->handle = au->xsk.handle;
+}
+#endif
+
 static struct sk_buff *
 mlx5e_skb_from_cqe_linear(struct mlx5e_rq *rq, struct mlx5e_wqe_frag_info *wi,
 			  struct mlx5_cqe64 *cqe, u32 cqe_bcnt)
 {
 	struct mlx5e_alloc_unit *au = wi->au;
 	u16 rx_headroom = rq->buff.headroom;
+#ifdef HAVE_XDP_SUPPORT
 	struct bpf_prog *prog;
+#endif
 	struct sk_buff *skb;
 	u32 metasize = 0;
 	void *va, *data;
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	dma_addr_t addr;
+#endif
 	u32 frag_size;
 
 	va             = page_address(au->page) + wi->offset;
 	data           = va + rx_headroom;
 	frag_size      = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);
 
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	addr = page_pool_get_dma_addr(au->page);
 	dma_sync_single_range_for_cpu(rq->pdev, addr, wi->offset,
 				      frag_size, rq->buff.map_dir);
+#else
+	dma_sync_single_range_for_cpu(rq->pdev, au->addr, wi->offset,
+				      frag_size, rq->buff.map_dir);
+#endif
 	net_prefetch(data);
 
+#ifdef HAVE_XDP_SUPPORT
 	prog = rcu_dereference(rq->xdp_prog);
 	if (prog) {
 		struct mlx5e_xdp_buff mxbuf;
@@ -1785,9 +2262,12 @@ mlx5e_skb_from_cqe_linear(struct mlx5e_r
 			return NULL; /* page/packet was consumed by XDP */
 
 		rx_headroom = mxbuf.xdp.data - mxbuf.xdp.data_hard_start;
+#ifdef HAVE_SKB_METADATA_SET
 		metasize = mxbuf.xdp.data - mxbuf.xdp.data_meta;
+#endif
 		cqe_bcnt = mxbuf.xdp.data_end - mxbuf.xdp.data;
 	}
+#endif
 	frag_size = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);
 	skb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt, metasize);
 	if (unlikely(!skb))
@@ -1810,23 +2290,47 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5
 	struct skb_shared_info *sinfo;
 	struct mlx5e_xdp_buff mxbuf;
 	u32 frag_consumed_bytes;
+#ifdef HAVE_XDP_SUPPORT
 	struct bpf_prog *prog;
+#endif
 	struct sk_buff *skb;
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	dma_addr_t addr;
+#endif
+	void *hard_start;
+	u32 metasize;
 	u32 truesize;
 	void *va;
+#ifndef HAVE_XDP_HAS_FRAGS
+	bool frag_pfmemalloc = false;
+	bool has_frags = false;
+	u32 frag_size;
+#endif
 
 	va = page_address(au->page) + wi->offset;
 	frag_consumed_bytes = min_t(u32, frag_info->frag_size, cqe_bcnt);
 
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	addr = page_pool_get_dma_addr(au->page);
 	dma_sync_single_range_for_cpu(rq->pdev, addr, wi->offset,
 				      rq->buff.frame0_sz, rq->buff.map_dir);
+#else
+	dma_sync_single_range_for_cpu(rq->pdev, au->addr, wi->offset,
+				      rq->buff.frame0_sz, rq->buff.map_dir);
+#endif
 	net_prefetchw(va); /* xdp_frame data area */
 	net_prefetch(va + rx_headroom);
 
 	mlx5e_fill_mxbuf(rq, cqe, va, rx_headroom, frag_consumed_bytes, &mxbuf);
+#ifndef HAVE_XDP_BUFF_HAS_FRAME_SZ
+#ifndef HAVE_XDP_BUFF_DATA_HARD_START
+	sinfo = xdp_get_shared_info_from_buff(&mxbuf.xdp, rq->buff.frame0_sz, va);
+#else
+	sinfo = xdp_get_shared_info_from_buff(&mxbuf.xdp, rq->buff.frame0_sz);
+#endif
+#else
 	sinfo = xdp_get_shared_info_from_buff(&mxbuf.xdp);
+#endif
 	truesize = 0;
 
 	cqe_bcnt -= frag_consumed_bytes;
@@ -1840,28 +2344,53 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5
 
 		frag_consumed_bytes = min_t(u32, frag_info->frag_size, cqe_bcnt);
 
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 		addr = page_pool_get_dma_addr(au->page);
 		dma_sync_single_for_cpu(rq->pdev, addr + wi->offset,
 					frag_consumed_bytes, rq->buff.map_dir);
+#else
+		dma_sync_single_for_cpu(rq->pdev, au->addr + wi->offset,
+					frag_consumed_bytes, rq->buff.map_dir);
+#endif
 
+#ifdef HAVE_XDP_HAS_FRAGS
 		if (!xdp_buff_has_frags(&mxbuf.xdp)) {
+#else
+		if (!has_frags) {
+#endif
 			/* Init on the first fragment to avoid cold cache access
 			 * when possible.
 			 */
 			sinfo->nr_frags = 0;
+#ifdef HAVE_XDP_HAS_FRAGS
 			sinfo->xdp_frags_size = 0;
 			xdp_buff_set_frags_flag(&mxbuf.xdp);
+#else
+			frag_size = 0;
+			has_frags = true;
+#endif
 		}
 
 		frag = &sinfo->frags[sinfo->nr_frags++];
+#ifdef HAVE_SKB_FRAG_FILL_PAGE_DESC
+		skb_frag_fill_page_desc(frag, au->page, wi->offset, frag_consumed_bytes);
+#else
 		__skb_frag_set_page(frag, au->page);
 		skb_frag_off_set(frag, wi->offset);
 		skb_frag_size_set(frag, frag_consumed_bytes);
+#endif
 
 		if (page_is_pfmemalloc(au->page))
+#ifdef HAVE_XDP_HAS_FRAGS
 			xdp_buff_set_frag_pfmemalloc(&mxbuf.xdp);
-
+#else
+			frag_pfmemalloc = true;
+#endif
+#ifdef HAVE_XDP_HAS_FRAGS
 		sinfo->xdp_frags_size += frag_consumed_bytes;
+#else
+		frag_size += frag_consumed_bytes;
+#endif
 		truesize += frag_info->frag_stride;
 
 		cqe_bcnt -= frag_consumed_bytes;
@@ -1869,6 +2398,7 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5
 		wi++;
 	}
 
+#ifdef HAVE_XDP_SUPPORT
 	prog = rcu_dereference(rq->xdp_prog);
 	if (prog && mlx5e_xdp_handle(rq, head_wi->au, prog, &mxbuf)) {
 		if (test_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags)) {
@@ -1880,21 +2410,40 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5
 		return NULL; /* page/packet was consumed by XDP */
 	}
 
-	skb = mlx5e_build_linear_skb(rq, mxbuf.xdp.data_hard_start, rq->buff.frame0_sz,
-				     mxbuf.xdp.data - mxbuf.xdp.data_hard_start,
+#endif
+#ifdef HAVE_XDP_SET_DATA_META_INVALID
+	metasize = mxbuf.xdp.data - mxbuf.xdp.data_meta;
+#else
+	metasize = 0;
+#endif
+#ifdef HAVE_XDP_BUFF_DATA_HARD_START
+	hard_start = mxbuf.xdp.data_hard_start;
+#else
+	hard_start = va;
+#endif
+	skb = mlx5e_build_linear_skb(rq, hard_start, rq->buff.frame0_sz,
+				     mxbuf.xdp.data - hard_start,
 				     mxbuf.xdp.data_end - mxbuf.xdp.data,
-				     mxbuf.xdp.data - mxbuf.xdp.data_meta);
+				     metasize);
 	if (unlikely(!skb))
 		return NULL;
 
 	head_wi->au->refcnt_bias--;
+#ifdef HAVE_XDP_HAS_FRAGS
 	if (xdp_buff_has_frags(&mxbuf.xdp)) {
+#else
+	if (unlikely(has_frags)) {
+#endif
 		int i;
 
 		/* sinfo->nr_frags is reset by build_skb, calculate again. */
 		xdp_update_skb_shared_info(skb, wi - head_wi - 1,
+#ifdef HAVE_XDP_HAS_FRAGS
 					   sinfo->xdp_frags_size, truesize,
 					   xdp_buff_is_frag_pfmemalloc(&mxbuf.xdp));
+#else
+					   frag_size, truesize, frag_pfmemalloc);
+#endif
 
 		for (i = 0; i < sinfo->nr_frags; i++) {
 			skb_frag_t *frag = &sinfo->frags[i];
@@ -1924,8 +2473,15 @@ static void mlx5e_handle_rx_err_cqe(stru
 	rq->stats->wqe_err++;
 }
 
-static void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+			 , bool xmit_more
+#endif
+			 )
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 	struct mlx5e_wqe_frag_info *wi;
 	struct sk_buff *skb;
@@ -1941,11 +2497,18 @@ static void mlx5e_handle_rx_cqe(struct m
 		goto free_wqe;
 	}
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	skb = INDIRECT_CALL_3(rq->wqe.skb_from_cqe,
 			      mlx5e_skb_from_cqe_linear,
 			      mlx5e_skb_from_cqe_nonlinear,
 			      mlx5e_xsk_skb_from_cqe_linear,
 			      rq, wi, cqe, cqe_bcnt);
+#else
+	skb = INDIRECT_CALL_2(rq->wqe.skb_from_cqe,
+			      mlx5e_skb_from_cqe_linear,
+			      mlx5e_skb_from_cqe_nonlinear,
+			      rq, wi, cqe, cqe_bcnt);
+#endif
 	if (!skb) {
 		/* probably for XDP */
 		if (__test_and_clear_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags)) {
@@ -1959,12 +2522,21 @@ static void mlx5e_handle_rx_cqe(struct m
 
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
 
+#ifdef HAVE_BASECODE_EXTRAS
+	mlx5e_set_skb_driver_xmit_more(skb, rq, xmit_more);
+#endif
+
 	if (mlx5e_cqe_regb_chain(cqe))
 		if (!mlx5e_tc_update_skb_nic(cqe, skb)) {
 			dev_kfree_skb_any(skb);
 			goto free_wqe;
 		}
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+		lro_receive_skb(&rq->sw_lro->lro_mgr, skb, NULL);
+	else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 free_wqe:
@@ -2033,7 +2605,11 @@ static bool mlx5e_rep_lookup_and_update(
 	return true;
 }
 
-static void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+			     , bool xmit_more
+#endif
+			     )
 {
 	struct net_device *netdev = rq->netdev;
 	struct mlx5e_priv *priv = netdev_priv(netdev);
@@ -2085,7 +2661,11 @@ wq_cyc_pop:
 	mlx5_wq_cyc_pop(wq);
 }
 
-static void mlx5e_handle_rx_cqe_mpwrq_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+static void mlx5e_handle_rx_cqe_mpwrq_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+					 , bool xmit_more
+#endif
+					 )
 {
 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
@@ -2182,7 +2762,9 @@ mlx5e_skb_from_cqe_mpwrq_nonlinear(struc
 	u32 byte_cnt       = cqe_bcnt - headlen;
 	struct mlx5e_alloc_unit *head_au = au;
 	struct sk_buff *skb;
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	dma_addr_t addr;
+#endif
 
 	skb = napi_alloc_skb(rq->cq.napi,
 			     ALIGN(MLX5E_RX_MAX_HEAD, sizeof(long)));
@@ -2201,9 +2783,14 @@ mlx5e_skb_from_cqe_mpwrq_nonlinear(struc
 
 	mlx5e_fill_skb_data(skb, rq, au, byte_cnt, frag_offset);
 	/* copy header */
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	addr = page_pool_get_dma_addr(head_au->page);
 	mlx5e_copy_skb_header(rq, skb, head_au->page, addr,
 			      head_offset, head_offset, headlen);
+#else
+	mlx5e_copy_skb_header(rq, skb, head_au->page, head_au->addr,
+			      head_offset, head_offset, headlen);
+#endif
 	/* skb linear part was allocated with headlen and aligned to long */
 	skb->tail += headlen;
 	skb->len  += headlen;
@@ -2218,11 +2805,15 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct m
 {
 	struct mlx5e_alloc_unit *au = &wi->alloc_units[page_idx];
 	u16 rx_headroom = rq->buff.headroom;
+#ifdef HAVE_XDP_SUPPORT
 	struct bpf_prog *prog;
+#endif
 	struct sk_buff *skb;
 	u32 metasize = 0;
 	void *va, *data;
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	dma_addr_t addr;
+#endif
 	u32 frag_size;
 
 	/* Check packet size. Note LRO doesn't use linear SKB */
@@ -2235,11 +2826,17 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct m
 	data           = va + rx_headroom;
 	frag_size      = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);
 
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	addr = page_pool_get_dma_addr(au->page);
 	dma_sync_single_range_for_cpu(rq->pdev, addr, head_offset,
 				      frag_size, rq->buff.map_dir);
+#else
+	dma_sync_single_range_for_cpu(rq->pdev, au->addr, head_offset,
+				      frag_size, rq->buff.map_dir);
+#endif
 	net_prefetch(data);
 
+#ifdef HAVE_XDP_SUPPORT
 	prog = rcu_dereference(rq->xdp_prog);
 	if (prog) {
 		struct mlx5e_xdp_buff mxbuf;
@@ -2253,9 +2850,12 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct m
 		}
 
 		rx_headroom = mxbuf.xdp.data - mxbuf.xdp.data_hard_start;
+#ifdef HAVE_SKB_METADATA_SET
 		metasize = mxbuf.xdp.data - mxbuf.xdp.data_meta;
+#endif
 		cqe_bcnt = mxbuf.xdp.data_end - mxbuf.xdp.data;
 	}
+#endif
 	frag_size = MLX5_SKB_FRAG_SZ(rx_headroom + cqe_bcnt);
 	skb = mlx5e_build_linear_skb(rq, va, frag_size, rx_headroom, cqe_bcnt, metasize);
 	if (unlikely(!skb))
@@ -2267,6 +2867,7 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct m
 	return skb;
 }
 
+#ifdef HAVE_SHAMPO_SUPPORT
 static struct sk_buff *
 mlx5e_skb_from_cqe_shampo(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 			  struct mlx5_cqe64 *cqe, u16 header_index)
@@ -2348,7 +2949,14 @@ mlx5e_hw_gro_skb_has_enough_space(struct
 {
 	int nr_frags = skb_shinfo(skb)->nr_frags;
 
-	return PAGE_SIZE * nr_frags + data_bcnt <= GRO_LEGACY_MAX_SIZE;
+	return PAGE_SIZE * nr_frags + data_bcnt <=
+#ifdef HAVE_GRO_LEGACY_MAX_SIZE
+	GRO_LEGACY_MAX_SIZE;
+#elif defined(HAVE_GRO_MAX_SIZE)
+	GRO_MAX_SIZE;
+#else
+	GSO_MAX_SIZE;
+#endif
 }
 
 static void
@@ -2368,7 +2976,11 @@ mlx5e_free_rx_shampo_hd_entry(struct mlx
 	bitmap_clear(shampo->bitmap, header_index, 1);
 }
 
-static void mlx5e_handle_rx_cqe_mpwrq_shampo(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+static void mlx5e_handle_rx_cqe_mpwrq_shampo(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+					     , bool xmit_more
+#endif
+					     )
 {
 	u16 data_bcnt		= mpwrq_get_cqe_byte_cnt(cqe) - cqe->shampo.header_size;
 	u16 header_index	= mlx5e_shampo_get_cqe_header_index(rq, cqe);
@@ -2452,9 +3064,17 @@ mpwrq_cqe_out:
 	mlx5e_free_rx_mpwqe(rq, wi, true);
 	mlx5_wq_ll_pop(wq, cqe->wqe_id, &wqe->next.next_wqe_index);
 }
+#endif /* HAVE_SHAMPO_SUPPORT */
 
-static void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+			       , bool xmit_more
+#endif
+			       )
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = netdev_priv(rq->netdev);
+#endif
 	u16 cstrides       = mpwrq_get_cqe_consumed_strides(cqe);
 	u16 wqe_id         = be16_to_cpu(cqe->wqe_id);
 	struct mlx5e_mpw_info *wi = mlx5e_get_mpw_info(rq, wqe_id);
@@ -2484,15 +3104,27 @@ static void mlx5e_handle_rx_cqe_mpwrq(st
 
 	cqe_bcnt = mpwrq_get_cqe_byte_cnt(cqe);
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	skb = INDIRECT_CALL_3(rq->mpwqe.skb_from_cqe_mpwrq,
 			      mlx5e_skb_from_cqe_mpwrq_linear,
 			      mlx5e_skb_from_cqe_mpwrq_nonlinear,
 			      mlx5e_xsk_skb_from_cqe_mpwrq_linear,
 			      rq, wi, cqe, cqe_bcnt, head_offset,
 			      page_idx);
+#else
+	skb = INDIRECT_CALL_2(rq->mpwqe.skb_from_cqe_mpwrq,
+			      mlx5e_skb_from_cqe_mpwrq_linear,
+			      mlx5e_skb_from_cqe_mpwrq_nonlinear,
+			      rq, wi, cqe, cqe_bcnt, head_offset,
+			      page_idx);
+#endif
 	if (!skb)
 		goto mpwrq_cqe_out;
 
+#ifdef HAVE_BASECODE_EXTRAS
+	mlx5e_set_skb_driver_xmit_more(skb, rq, xmit_more);
+#endif
+
 	mlx5e_complete_rx_cqe(rq, cqe, cqe_bcnt, skb);
 
 	if (mlx5e_cqe_regb_chain(cqe))
@@ -2501,6 +3133,11 @@ static void mlx5e_handle_rx_cqe_mpwrq(st
 			goto mpwrq_cqe_out;
 		}
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+		lro_receive_skb(&rq->sw_lro->lro_mgr, skb, NULL);
+	else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 mpwrq_cqe_out:
@@ -2545,10 +3182,23 @@ static int mlx5e_rx_cq_process_enhanced_
 		}
 		title_cqe = cqe;
 		mlx5_cqwq_pop(cqwq);
-
+#ifdef HAVE_SHAMPO_SUPPORT
 		INDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
 				mlx5e_handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq_shampo,
-				rq, cqe);
+				rq, cqe
+#ifdef HAVE_BASECODE_EXTRAS
+				, work_done < budget_rem - 1
+#endif
+				);
+#else
+		INDIRECT_CALL_2(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
+				mlx5e_handle_rx_cqe, rq, cqe
+#ifdef HAVE_BASECODE_EXTRAS
+				, work_done < budget_rem - 1
+#endif
+				);
+#endif
+
 		work_done++;
 	} while (work_done < budget_rem &&
 		 (cqe = mlx5_cqwq_get_cqe_enahnced_comp(cqwq)));
@@ -2581,9 +3231,22 @@ static int mlx5e_rx_cq_process_basic_cqe
 		}
 
 		mlx5_cqwq_pop(cqwq);
+#ifdef HAVE_SHAMPO_SUPPORT
 		INDIRECT_CALL_3(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
 				mlx5e_handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq_shampo,
-				rq, cqe);
+				rq, cqe
+#ifdef HAVE_BASECODE_EXTRAS
+				, work_done < budget_rem - 1
+#endif
+				);
+#else
+		INDIRECT_CALL_2(rq->handle_rx_cqe, mlx5e_handle_rx_cqe_mpwrq,
+				mlx5e_handle_rx_cqe, rq, cqe
+#ifdef HAVE_BASECODE_EXTRAS
+				, work_done < budget_rem - 1
+#endif
+				);
+#endif
 		work_done++;
 	}
 
@@ -2595,6 +3258,15 @@ int mlx5e_poll_rx_cq(struct mlx5e_cq *cq
 	struct mlx5e_rq *rq = container_of(cq, struct mlx5e_rq, cq);
 	struct mlx5_cqwq *cqwq = &cq->wq;
 	int work_done;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv;
+#ifdef CONFIG_MLX5_CORE_IPOIB
+	if (MLX5_CAP_GEN(cq->mdev, port_type) != MLX5_CAP_PORT_TYPE_ETH)
+		priv = mlx5i_epriv(rq->netdev);
+	else
+#endif
+		priv = netdev_priv(rq->netdev);
+#endif
 
 	if (unlikely(!test_bit(MLX5E_RQ_STATE_ENABLED, &rq->state)))
 		return 0;
@@ -2609,16 +3281,24 @@ int mlx5e_poll_rx_cq(struct mlx5e_cq *cq
 	if (work_done == 0)
 		return 0;
 
+#ifdef HAVE_SHAMPO_SUPPORT
 	if (test_bit(MLX5E_RQ_STATE_SHAMPO, &rq->state) && rq->hw_gro_data->skb)
 		mlx5e_shampo_flush_skb(rq, NULL, false);
+#endif
 
+#ifdef HAVE_XDP_SUPPORT
 	if (rcu_access_pointer(rq->xdp_prog))
 		mlx5e_xdp_rx_poll_complete(rq);
+#endif
 
 	mlx5_cqwq_update_db_record(cqwq);
 
 	/* ensure cq space is freed before enabling more cqes */
 	wmb();
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (IS_SW_LRO(&priv->channels.params))
+		lro_flush_all(&rq->sw_lro->lro_mgr);
+#endif
 
 	return work_done;
 }
@@ -2644,6 +3324,9 @@ static inline void mlx5i_complete_rx_cqe
 	u32 qpn;
 	u8 *dgid;
 	u8 g;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+       struct mlx5e_priv *parent_priv = mlx5i_epriv(rq->netdev);
+#endif
 
 	qpn = be32_to_cpu(cqe->sop_drop_qpn) & 0xffffff;
 	netdev = mlx5i_pkey_get_netdev(rq->netdev, qpn);
@@ -2686,6 +3369,12 @@ static inline void mlx5i_complete_rx_cqe
 
 	skb->protocol = *((__be16 *)(skb->data));
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (parent_priv->netdev->features & NETIF_F_LRO) {
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+	} else
+#endif
+
 	if ((netdev->features & NETIF_F_RXCSUM) &&
 	    (likely((cqe->hds_ip_ext & CQE_L3_OK) &&
 		    (cqe->hds_ip_ext & CQE_L4_OK)))) {
@@ -2721,8 +3410,15 @@ static inline void mlx5i_complete_rx_cqe
 	}
 }
 
-static void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+void mlx5i_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+			 , bool xmit_more
+#endif
+			 )
 {
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_priv *priv = mlx5i_epriv(rq->netdev);
+#endif
 	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 	struct mlx5e_wqe_frag_info *wi;
 	struct sk_buff *skb;
@@ -2750,6 +3446,14 @@ static void mlx5i_handle_rx_cqe(struct m
 		dev_kfree_skb_any(skb);
 		goto wq_free_wqe;
 	}
+#ifdef HAVE_BASECODE_EXTRAS
+	mlx5e_set_skb_driver_xmit_more(skb, rq, xmit_more);
+#endif
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	if (priv->netdev->features & NETIF_F_LRO)
+		lro_receive_skb(&rq->sw_lro->lro_mgr, skb, NULL);
+	else
+#endif
 	napi_gro_receive(rq->cq.napi, skb);
 
 wq_free_wqe:
@@ -2777,11 +3481,19 @@ int mlx5e_rq_set_handlers(struct mlx5e_r
 
 	switch (rq->wq_type) {
 	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		rq->mpwqe.skb_from_cqe_mpwrq = xsk ?
 			mlx5e_xsk_skb_from_cqe_mpwrq_linear :
 			mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL) ?
 				mlx5e_skb_from_cqe_mpwrq_linear :
 				mlx5e_skb_from_cqe_mpwrq_nonlinear;
+#else
+		rq->mpwqe.skb_from_cqe_mpwrq =
+			mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL) ?
+			mlx5e_skb_from_cqe_mpwrq_linear :
+			mlx5e_skb_from_cqe_mpwrq_nonlinear;
+#endif
+
 		rq->post_wqes = mlx5e_post_rx_mpwqes;
 		rq->dealloc_wqe = mlx5e_dealloc_rx_mpwqe;
 
@@ -2801,11 +3513,17 @@ int mlx5e_rq_set_handlers(struct mlx5e_r
 
 		break;
 	default: /* MLX5_WQ_TYPE_CYCLIC */
-		rq->wqe.skb_from_cqe = xsk ?
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+                rq->wqe.skb_from_cqe = xsk ?
 			mlx5e_xsk_skb_from_cqe_linear :
 			mlx5e_rx_is_linear_skb(mdev, params, NULL) ?
 				mlx5e_skb_from_cqe_linear :
 				mlx5e_skb_from_cqe_nonlinear;
+#else
+		rq->wqe.skb_from_cqe = mlx5e_rx_is_linear_skb(mdev, params, NULL) ?
+			mlx5e_skb_from_cqe_linear :
+			mlx5e_skb_from_cqe_nonlinear;
+#endif
 		rq->post_wqes = mlx5e_post_rx_wqes;
 		rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
 		rq->handle_rx_cqe = priv->profile->rx_handlers->handle_rx_cqe;
@@ -2818,7 +3536,12 @@ int mlx5e_rq_set_handlers(struct mlx5e_r
 	return 0;
 }
 
-static void mlx5e_trap_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe)
+#ifdef HAVE_DEVLINK_TRAP_SUPPORT
+static void mlx5e_trap_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe
+#ifdef HAVE_BASECODE_EXTRAS
+				     , bool xmit_more
+#endif
+				     )
 {
 	struct mlx5_wq_cyc *wq = &rq->wqe.wq;
 	struct mlx5e_wqe_frag_info *wi;
@@ -2845,7 +3568,11 @@ static void mlx5e_trap_handle_rx_cqe(str
 	skb_push(skb, ETH_HLEN);
 
 	mlx5_devlink_trap_report(rq->mdev, trap_id, skb,
+#ifdef HAVE_NET_DEVICE_DEVLINK_PORT
 				 rq->netdev->devlink_port);
+#else
+				 mlx5e_devlink_get_dl_port(netdev_priv(rq->netdev)));
+#endif
 	dev_kfree_skb_any(skb);
 
 free_wqe:
@@ -2862,3 +3589,4 @@ void mlx5e_rq_set_trap_handlers(struct m
 	rq->dealloc_wqe = mlx5e_dealloc_rx_wqe;
 	rq->handle_rx_cqe = mlx5e_trap_handle_rx_cqe;
 }
+#endif /* HAVE_DEVLINK_TRAP_SUPPORT */
