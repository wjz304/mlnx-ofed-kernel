From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/eq.c

Change-Id: I4effb0101fd103348278e8a406f14e8645759d8e
---
 drivers/net/ethernet/mellanox/mlx5/core/eq.c | 69 +++++++++++++++++++-
 1 file changed, 66 insertions(+), 3 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/eq.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/eq.c
@@ -44,7 +44,9 @@ enum {
 	MLX5_EQ_POLLING_BUDGET	= 128,
 };
 
+#ifdef HAVE_STATIC_ASSERT
 static_assert(MLX5_EQ_POLLING_BUDGET <= MLX5_NUM_SPARE_EQE);
+#endif
 
 struct mlx5_eq_table {
 	struct list_head        comp_eqs_list;
@@ -125,7 +127,11 @@ static int mlx5_eq_comp_int(struct notif
 		/* Make sure we read EQ entry contents after we've
 		 * checked the ownership bit.
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 		/* Assume (eqe->type) is always MLX5_EVENT_TYPE_COMP */
 		cqn = be32_to_cpu(eqe->data.comp.cqn) & 0xffffff;
 
@@ -204,7 +210,7 @@ static int mlx5_eq_async_int(struct noti
 	struct mlx5_eq_table *eqt;
 	struct mlx5_core_dev *dev;
 	struct mlx5_eqe *eqe;
-	unsigned long flags;
+	unsigned long flags = 0;
 	int num_eqes = 0;
 	bool recovery;
 
@@ -223,7 +229,11 @@ static int mlx5_eq_async_int(struct noti
 		 * Make sure we read EQ entry contents after we've
 		 * checked the ownership bit.
 		 */
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 		atomic_notifier_call_chain(&eqt->nh[eqe->type], eqe->type, eqe);
 		atomic_notifier_call_chain(&eqt->nh[MLX5_EVENT_TYPE_NOTIFY_ANY], eqe->type, eqe);
@@ -338,7 +348,11 @@ create_map_eq(struct mlx5_core_dev *dev,
 
 	eq->vecidx = vecidx;
 	eq->eqn = MLX5_GET(create_eq_out, out, eq_number);
-	eq->irqn = pci_irq_vector(dev->pdev, vecidx);
+#ifdef HAVE_PCI_IRQ_API
+       eq->irqn = pci_irq_vector(dev->pdev, vecidx);
+#else
+	eq->irqn = mlx5_get_msix_vec(dev, vecidx);
+#endif
 	eq->dev = dev;
 	eq->doorbell = priv->uar->map + MLX5_EQ_DOORBEL_OFFSET;
 
@@ -658,13 +672,18 @@ static void cleanup_async_eq(struct mlx5
 			      name, err);
 }
 
+#ifdef HAVE_DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE  
 static u16 async_eq_depth_devlink_param_get(struct mlx5_core_dev *dev)
 {
 	struct devlink *devlink = priv_to_devlink(dev);
 	union devlink_param_value val;
 	int err;
 
+#ifdef HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET
 	err = devl_param_driverinit_value_get(devlink,
+#else
+	err = devlink_param_driverinit_value_get(devlink,
+#endif
 					      DEVLINK_PARAM_GENERIC_ID_EVENT_EQ_SIZE,
 					      &val);
 	if (!err)
@@ -672,6 +691,8 @@ static u16 async_eq_depth_devlink_param_
 	mlx5_core_dbg(dev, "Failed to get param. using default. err = %d\n", err);
 	return MLX5_NUM_ASYNC_EQE;
 }
+#endif
+
 static int create_async_eqs(struct mlx5_core_dev *dev)
 {
 	struct mlx5_eq_table *table = dev->priv.eq_table;
@@ -703,7 +724,11 @@ static int create_async_eqs(struct mlx5_
 
 	param = (struct mlx5_eq_param) {
 		.irq = table->ctrl_irq,
+#ifdef HAVE_DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE  
 		.nent = async_eq_depth_devlink_param_get(dev),
+#else
+		.nent = MLX5_NUM_ASYNC_EQE,
+#endif
 	};
 
 	if (mlx5_core_is_sf(dev) && dev->async_eq_depth)
@@ -821,7 +846,11 @@ struct mlx5_eqe *mlx5_eq_get_eqe(struct
 	 * checked the ownership bit.
 	 */
 	if (eqe)
+#ifdef dma_rmb
 		dma_rmb();
+#else
+		rmb();
+#endif
 
 	return eqe;
 }
@@ -874,12 +903,14 @@ static void comp_irqs_release(struct mlx
 static int comp_irqs_request(struct mlx5_core_dev *dev)
 {
 	struct mlx5_eq_table *table = dev->priv.eq_table;
+#if defined(for_each_numa_hop_mask) && defined(for_each_cpu_andnot)
 	const struct cpumask *prev = cpu_none_mask;
 	const struct cpumask *mask;
+	int cpu;
+#endif
 	int ncomp_eqs = table->num_comp_eqs;
 	u16 *cpus;
 	int ret;
-	int cpu;
 	int i;
 
 	ncomp_eqs = table->num_comp_eqs;
@@ -904,6 +935,7 @@ static int comp_irqs_request(struct mlx5
 		goto free_irqs;
 	}
 
+#if defined(for_each_numa_hop_mask) && defined(for_each_cpu_andnot)
 	i = 0;
 	rcu_read_lock();
 	for_each_numa_hop_mask(mask, dev->priv.numa_node) {
@@ -916,6 +948,10 @@ static int comp_irqs_request(struct mlx5
 	}
 spread_done:
 	rcu_read_unlock();
+#else
+	for (i = 0; i < ncomp_eqs; i++)
+		cpus[i] = cpumask_local_spread(i, dev->priv.numa_node);
+#endif
 	ret = mlx5_irqs_request_vectors(dev, cpus, ncomp_eqs, table->comp_irqs);
 	kfree(cpus);
 	if (ret < 0)
@@ -944,13 +980,18 @@ static void destroy_comp_eqs(struct mlx5
 	comp_irqs_release(dev);
 }
 
+#ifdef HAVE_DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE
 static u16 comp_eq_depth_devlink_param_get(struct mlx5_core_dev *dev)
 {
 	struct devlink *devlink = priv_to_devlink(dev);
 	union devlink_param_value val;
 	int err;
 
+#ifdef HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET
 	err = devl_param_driverinit_value_get(devlink,
+#else
+	err = devlink_param_driverinit_value_get(devlink,
+#endif
 					      DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE,
 					      &val);
 	if (!err)
@@ -958,6 +999,7 @@ static u16 comp_eq_depth_devlink_param_g
 	mlx5_core_dbg(dev, "Failed to get param. using default. err = %d\n", err);
 	return MLX5_COMP_EQ_SIZE;
 }
+#endif
 
 static int create_comp_eqs(struct mlx5_core_dev *dev)
 {
@@ -972,7 +1014,11 @@ static int create_comp_eqs(struct mlx5_c
 	if (ncomp_eqs < 0)
 		return ncomp_eqs;
 	INIT_LIST_HEAD(&table->comp_eqs_list);
+#ifdef HAVE_DEVLINK_PARAM_GENERIC_ID_IO_EQ_SIZE
 	nent = comp_eq_depth_devlink_param_get(dev);
+#else
+	nent = MLX5_COMP_EQ_SIZE;
+#endif
 
 	/* if user specified completion eq depth, honor that */
 	if (mlx5_core_is_sf(dev) && dev->cmpl_eq_depth)
@@ -990,7 +1036,12 @@ static int create_comp_eqs(struct mlx5_c
 		INIT_LIST_HEAD(&eq->tasklet_ctx.list);
 		INIT_LIST_HEAD(&eq->tasklet_ctx.process_list);
 		spin_lock_init(&eq->tasklet_ctx.lock);
+#ifdef HAVE_TASKLET_SETUP
 		tasklet_setup(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb);
+#else
+		tasklet_init(&eq->tasklet_ctx.task, mlx5_cq_tasklet_cb,
+				(unsigned long)&eq->tasklet_ctx);
+#endif
 
 		eq->irq_nb.notifier_call = mlx5_eq_comp_int;
 		param = (struct mlx5_eq_param) {
@@ -1125,8 +1176,13 @@ static int set_rmap(struct mlx5_core_dev
 	}
 
 	for (vecidx = 0; vecidx < eq_table->num_comp_eqs; vecidx++) {
+#ifdef HAVE_PCI_IRQ_API
 		err = irq_cpu_rmap_add(eq_table->rmap,
 				       pci_irq_vector(mdev->pdev, vecidx));
+#else
+		err = irq_cpu_rmap_add(eq_table->rmap,
+				       mdev->priv.msix_arr[vecidx].vector);
+#endif
 		if (err) {
 			mlx5_core_err(mdev, "irq_cpu_rmap_add failed. err %d",
 				      err);
@@ -1142,6 +1198,13 @@ err_out:
 	return err;
 }
 
+#ifndef HAVE_PCI_IRQ_API
+u32 mlx5_get_msix_vec(struct mlx5_core_dev *dev, int vecidx)
+{
+	return dev->priv.msix_arr[vecidx].vector;
+}
+#endif
+
 /* This function should only be called after mlx5_cmd_force_teardown_hca */
 void mlx5_core_eq_free_irqs(struct mlx5_core_dev *dev)
 {
