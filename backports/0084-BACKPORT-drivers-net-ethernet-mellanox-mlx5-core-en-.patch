From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en/params.c

Change-Id: I5259b3483b41af5f3e625b0a08a198bd68edcbeb
---
 .../ethernet/mellanox/mlx5/core/en/params.c   | 118 +++++++++++++++---
 1 file changed, 101 insertions(+), 17 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
@@ -6,8 +6,21 @@
 #include "en/port.h"
 #include "en_accel/en_accel.h"
 #include "en_accel/ipsec.h"
+#ifdef HAVE_NET_PAGE_POOL_OLD_H
+#include <net/page_pool.h>
+#endif
+#ifdef HAVE_NET_PAGE_POOL_TYPES_H
 #include <net/page_pool/types.h>
+#include <net/page_pool/helpers.h>
+#endif
+#include "en_accel/ktls.h"
+#ifdef HAVE_XDP_SOCK_DRV_H
 #include <net/xdp_sock_drv.h>
+#endif
+
+#ifndef HAVE_XDP_SOCK_DRV_H
+#define XDP_UMEM_MIN_CHUNK_SHIFT 11
+#endif
 
 static u8 mlx5e_mpwrq_min_page_shift(struct mlx5_core_dev *mdev)
 {
@@ -18,7 +31,13 @@ static u8 mlx5e_mpwrq_min_page_shift(str
 
 u8 mlx5e_mpwrq_page_shift(struct mlx5_core_dev *mdev, struct mlx5e_xsk_param *xsk)
 {
-	u8 req_page_shift = xsk ? order_base_2(xsk->chunk_size) : PAGE_SHIFT;
+	u8 req_page_shift =
+#ifdef HAVE_XDP_SUPPORT
+		xsk ? order_base_2(xsk->chunk_size) : PAGE_SHIFT;
+#else
+	PAGE_SHIFT;
+#endif
+
 	u8 min_page_shift = mlx5e_mpwrq_min_page_shift(mdev);
 
 	/* Regular RQ uses order-0 pages, the NIC must be able to map them. */
@@ -218,16 +237,27 @@ u8 mlx5e_mpwrq_max_log_rq_pkts(struct ml
 u16 mlx5e_get_linear_rq_headroom(struct mlx5e_params *params,
 				 struct mlx5e_xsk_param *xsk)
 {
-	u16 headroom;
+	u16 headroom = 0;
 
+#ifdef HAVE_XSK_BUFF_ALLOC
 	if (xsk)
 		return xsk->headroom;
+#endif
 
 	headroom = NET_IP_ALIGN;
-	if (params->xdp_prog)
+#ifdef HAVE_XDP_SUPPORT
+	if (params->xdp_prog) {
 		headroom += XDP_PACKET_HEADROOM;
-	else
+#ifndef HAVE_XSK_BUFF_ALLOC
+		if (xsk)
+			headroom += xsk->headroom;
+#endif
+	} else {
+#endif /* HAVE_XDP_SUPPORT */
 		headroom += MLX5_RX_HEADROOM;
+#ifdef HAVE_XDP_SUPPORT
+	}
+#endif
 
 	return headroom;
 }
@@ -266,7 +296,11 @@ static u32 mlx5e_rx_get_linear_stride_sz
 	if (xsk)
 		return mpwqe ? 1 << mlx5e_mpwrq_page_shift(mdev, xsk) : PAGE_SIZE;
 
+#ifdef HAVE_XDP_SUPPORT
 	no_head_tail_room = params->xdp_prog && mpwqe && !mlx5e_rx_is_linear_skb(mdev, params, xsk);
+#else
+	no_head_tail_room = false;
+#endif
 
 	/* When no_head_tail_room is set, headroom and tailroom are excluded from skb calculations.
 	 * no_head_tail_room should be set in the case of XDP with Striding RQ
@@ -277,7 +311,11 @@ static u32 mlx5e_rx_get_linear_stride_sz
 	/* XDP in mlx5e doesn't support multiple packets per page.
 	 * Do not assume sz <= PAGE_SIZE if params->xdp_prog is set.
 	 */
+#ifdef HAVE_XDP_SUPPORT
 	return params->xdp_prog && sz < PAGE_SIZE ? PAGE_SIZE : sz;
+#else
+	return sz;
+#endif
 }
 
 static u8 mlx5e_mpwqe_log_pkts_per_wqe(struct mlx5_core_dev *mdev,
@@ -434,10 +472,12 @@ u8 mlx5e_mpwqe_get_log_stride_size(struc
 	if (mlx5e_rx_mpwqe_is_linear_skb(mdev, params, xsk))
 		return order_base_2(mlx5e_rx_get_linear_stride_sz(mdev, params, xsk, true));
 
+#ifdef HAVE_XDP_SUPPORT
 	/* XDP in mlx5e doesn't support multiple packets per page. */
 	if (params->xdp_prog)
 		return PAGE_SHIFT;
-
+#endif
+	
 	return MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev);
 }
 
@@ -614,6 +654,7 @@ int mlx5e_mpwrq_validate_regular(struct
 	return 0;
 }
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 int mlx5e_mpwrq_validate_xsk(struct mlx5_core_dev *mdev, struct mlx5e_params *params,
 			     struct mlx5e_xsk_param *xsk)
 {
@@ -645,6 +686,7 @@ int mlx5e_mpwrq_validate_xsk(struct mlx5
 
 	return 0;
 }
+#endif
 
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params)
@@ -652,6 +694,9 @@ void mlx5e_init_rq_type_params(struct ml
 	params->log_rq_mtu_frames = is_kdump_kernel() ?
 		MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE :
 		MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
+#ifndef HAVE_DEVLINK_PER_AUXDEV
+	mlx5e_params_print_info(mdev, params);
+#endif
 }
 
 void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
@@ -704,6 +749,7 @@ static int mlx5e_max_nonlinear_mtu(int f
 	return first_frag_size + (MLX5E_MAX_RX_FRAGS - 2) * frag_size + PAGE_SIZE;
 }
 
+#ifdef HAVE_PAGE_POOL_DEFRAG_PAGE
 static void mlx5e_rx_compute_wqe_bulk_params(struct mlx5e_params *params,
 					     struct mlx5e_rq_frags_info *info)
 {
@@ -732,8 +778,13 @@ static void mlx5e_rx_compute_wqe_bulk_pa
 	 * keep bulk size smaller to avoid filling the page_pool cache on
 	 * every bulk refill.
 	 */
-	wqe_bulk_in_bytes = min_t(u32, MAX_WQE_BULK_BYTES(params->xdp_prog),
-				  bulk_bound_rq_size_in_bytes);
+
+#ifdef HAVE_XDP_SUPPORT
+	wqe_bulk_in_bytes = min_t(u32, MAX_WQE_BULK_BYTES(params->xdp_prog)
+			,bulk_bound_rq_size_in_bytes);
+#else
+	wqe_bulk_in_bytes = bulk_bound_rq_size_in_bytes;
+#endif
 	wqe_bulk = DIV_ROUND_UP(wqe_bulk_in_bytes, sum_frag_strides);
 
 	/* Make sure that allocations don't start when the page is still used
@@ -741,10 +792,16 @@ static void mlx5e_rx_compute_wqe_bulk_pa
 	 */
 	info->wqe_bulk = max_t(u16, info->wqe_index_mask + 1, wqe_bulk);
 
-	split_factor = DIV_ROUND_UP(MAX_WQE_BULK_BYTES(params->xdp_prog),
-				    PP_ALLOC_CACHE_REFILL * PAGE_SIZE);
+	split_factor = DIV_ROUND_UP(
+#ifdef HAVE_XDP_SUPPORT
+				MAX_WQE_BULK_BYTES(params->xdp_prog)
+#else
+				(512*1024)
+#endif
+				,PP_ALLOC_CACHE_REFILL * PAGE_SIZE);
 	info->refill_unit = DIV_ROUND_UP(info->wqe_bulk, split_factor);
 }
+#endif /*HAVE_PAGE_POOL_DEFRAG_PAGE*/
 
 #define DEFAULT_FRAG_SIZE (2048)
 
@@ -757,6 +814,11 @@ static int mlx5e_build_rq_frags_info(str
 	u32 byte_count = MLX5E_SW2HW_MTU(params, params->sw_mtu);
 	int frag_size_max = DEFAULT_FRAG_SIZE;
 	int first_frag_size_max;
+#ifdef HAVE_XDP_SUPPORT
+	bool xdp_prog = params->xdp_prog;
+#else
+	bool xdp_prog = false;
+#endif
 	u32 buf_size = 0;
 	u16 headroom;
 	int max_mtu;
@@ -785,13 +847,13 @@ static int mlx5e_build_rq_frags_info(str
 	first_frag_size_max = SKB_WITH_OVERHEAD(frag_size_max - headroom);
 
 	max_mtu = mlx5e_max_nonlinear_mtu(first_frag_size_max, frag_size_max,
-					  params->xdp_prog);
-	if (byte_count > max_mtu || params->xdp_prog) {
+					  xdp_prog);
+	if (byte_count > max_mtu || xdp_prog) {
 		frag_size_max = PAGE_SIZE;
 		first_frag_size_max = SKB_WITH_OVERHEAD(frag_size_max - headroom);
 
 		max_mtu = mlx5e_max_nonlinear_mtu(first_frag_size_max, frag_size_max,
-						  params->xdp_prog);
+						  xdp_prog);
 		if (byte_count > max_mtu) {
 			mlx5_core_err(mdev, "MTU %u is too big for non-linear legacy RQ (max %d)\n",
 				      params->sw_mtu, max_mtu);
@@ -811,7 +873,7 @@ static int mlx5e_build_rq_frags_info(str
 		info->arr[i].frag_size = frag_size;
 		buf_size += frag_size;
 
-		if (params->xdp_prog) {
+		if (xdp_prog) {
 			/* XDP multi buffer expects fragments of the same size. */
 			info->arr[i].frag_stride = frag_size_max;
 		} else {
@@ -854,6 +916,7 @@ static int mlx5e_build_rq_frags_info(str
 	}
 
 out:
+#ifdef HAVE_PAGE_POOL_DEFRAG_PAGE
 	/* Bulking optimization to skip allocation until a large enough number
 	 * of WQEs can be allocated in a row. Bulking also influences how well
 	 * deferred page release works.
@@ -862,10 +925,20 @@ out:
 
 	mlx5_core_dbg(mdev, "%s: wqe_bulk = %u, wqe_bulk_refill_unit = %u\n",
 		      __func__, info->wqe_bulk, info->refill_unit);
+#else
+	/* Bulking optimization to skip allocation until at least 8 WQEs can be
+	 * allocated in a row. At the same time, never start allocation when
+	 * the page is still used by older WQEs.
+	 */
+
+	info->wqe_bulk = max_t(u8, info->wqe_index_mask + 1, 8);
+#endif
 
 	info->log_num_frags = order_base_2(info->num_frags);
 
+#ifdef HAVE_XDP_SUPPORT
 	*xdp_frag_size = info->num_frags > 1 && params->xdp_prog ? PAGE_SIZE : 0;
+#endif
 
 	return 0;
 }
@@ -951,10 +1024,13 @@ static void mlx5e_build_rx_cq_param(stru
 
 static u8 rq_end_pad_mode(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
 {
-	bool lro_en = params->packet_merge.type == MLX5E_PACKET_MERGE_LRO;
 	bool ro = MLX5_CAP_GEN(mdev, relaxed_ordering_write);
 
-	return ro && lro_en ?
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	return ro && IS_HW_LRO(params)?
+#else
+	return ro && (params->packet_merge.type == MLX5E_PACKET_MERGE_LRO) ?
+#endif
 		MLX5_WQ_END_PAD_MODE_NONE : MLX5_WQ_END_PAD_MODE_ALIGN;
 }
 
@@ -1054,6 +1130,10 @@ void mlx5e_build_tx_cq_param(struct mlx5
 	void *cqc = param->cqc;
 
 	MLX5_SET(cqc, cqc, log_cq_size, params->log_sq_size);
+#ifdef HAVE_BASECODE_EXTRAS
+	if (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_TX_CQE_COMPRESS))
+		MLX5_SET(cqc, cqc, cqe_comp_en, 1);
+#endif
 
 	mlx5e_build_common_cq_param(mdev, param);
 	param->cq_period_mode = params->tx_cq_moderation.cq_period_mode;
@@ -1188,6 +1268,7 @@ static u8 mlx5e_build_icosq_log_wq_sz(st
 	/* UMR WQEs for the regular RQ. */
 	wqebbs = mlx5e_mpwrq_total_umr_wqebbs(mdev, params, NULL);
 
+#ifdef HAVE_XDP_SUPPORT
 	/* If XDP program is attached, XSK may be turned on at any time without
 	 * restarting the channel. ICOSQ must be big enough to fit UMR WQEs of
 	 * both regular RQ and XSK RQ.
@@ -1232,7 +1313,7 @@ static u8 mlx5e_build_icosq_log_wq_sz(st
 
 		wqebbs += max_xsk_wqebbs;
 	}
-
+#endif
 	if (params->packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO)
 		wqebbs += mlx5e_shampo_icosq_sz(mdev, params, rqp);
 
@@ -1255,7 +1336,6 @@ static u8 mlx5e_build_async_icosq_log_wq
 {
 	if (mlx5e_is_ktls_rx(mdev))
 		return MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
-
 	return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
 }
 
@@ -1290,6 +1370,7 @@ static void mlx5e_build_async_icosq_para
 	mlx5e_build_ico_cq_param(mdev, log_wq_size, &param->cqp);
 }
 
+#ifdef HAVE_XDP_SUPPORT
 void mlx5e_build_xdpsq_param(struct mlx5_core_dev *mdev,
 			     struct mlx5e_params *params,
 			     struct mlx5e_xsk_param *xsk,
@@ -1304,6 +1385,7 @@ void mlx5e_build_xdpsq_param(struct mlx5
 	param->is_xdp_mb = !mlx5e_rx_is_linear_skb(mdev, params, xsk);
 	mlx5e_build_tx_cq_param(mdev, params, &param->cqp);
 }
+#endif
 
 int mlx5e_build_channel_param(struct mlx5_core_dev *mdev,
 			      struct mlx5e_params *params,
@@ -1320,7 +1402,9 @@ int mlx5e_build_channel_param(struct mlx
 	async_icosq_log_wq_sz = mlx5e_build_async_icosq_log_wq_sz(mdev);
 
 	mlx5e_build_sq_param(mdev, params, &cparam->txq_sq);
+#ifdef HAVE_XDP_SUPPORT
 	mlx5e_build_xdpsq_param(mdev, params, NULL, &cparam->xdp_sq);
+#endif
 	mlx5e_build_icosq_param(mdev, icosq_log_wq_sz, &cparam->icosq);
 	mlx5e_build_async_icosq_param(mdev, async_icosq_log_wq_sz, &cparam->async_icosq);
 
