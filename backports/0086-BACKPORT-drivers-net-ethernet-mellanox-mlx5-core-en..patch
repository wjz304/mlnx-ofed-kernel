From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en.h

Change-Id: I75a9db15426bd2e37cd05bb32ba324c16fa4b7b0
---
 drivers/net/ethernet/mellanox/mlx5/core/en.h | 258 +++++++++++++++++--
 1 file changed, 239 insertions(+), 19 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -32,9 +32,13 @@
 #ifndef __MLX5_EN_H__
 #define __MLX5_EN_H__
 
+#ifdef HAVE_XDP_SUPPORT
+#include <linux/bpf.h>
+#endif
 #include <linux/if_vlan.h>
 #include <linux/etherdevice.h>
 #include <linux/timecounter.h>
+#include <linux/clocksource.h>
 #include <linux/net_tstamp.h>
 #include <linux/crash_dump.h>
 #include <linux/mlx5/driver.h>
@@ -45,12 +49,17 @@
 #include <linux/mlx5/transobj.h>
 #include <linux/mlx5/fs.h>
 #include <linux/rhashtable.h>
+#include <linux/ethtool.h>
+#ifdef HAVE_UDP_TUNNEL_NIC_INFO
 #include <net/udp_tunnel.h>
+#endif
 #include <net/switchdev.h>
 #include <net/xdp.h>
 #include <net/pkt_cls.h>
 #include <linux/dim.h>
+#ifdef HAVE_BITS_H
 #include <linux/bits.h>
+#endif
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
@@ -60,9 +69,20 @@
 #include "lib/hv_vhca.h"
 #include "lib/clock.h"
 #include "en/rx_res.h"
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#include <linux/inet_lro.h>
+#else
+#include <net/ip.h>
+#endif
 
+#ifndef HAVE_DEVLINK_HEALTH_REPORT_SUPPORT
+/* The intention is to pass NULL for backports of old kernels */
+struct devlink_health_reporter {};
+#endif /* HAVE_DEVLINK_HEALTH_REPORT_SUPPORT */
 extern const struct net_device_ops mlx5e_netdev_ops;
+#ifdef HAVE_NET_PAGE_POOL_H
 struct page_pool;
+#endif
 
 #define MLX5E_METADATA_ETHER_TYPE (0x8CE4)
 #define MLX5E_METADATA_ETHER_LEN 8
@@ -286,14 +306,22 @@ enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_BASED_MODER,
 	MLX5E_PFLAG_TX_CQE_BASED_MODER,
 	MLX5E_PFLAG_RX_CQE_COMPRESS,
+	MLX5E_PFLAG_TX_CQE_COMPRESS,
 	MLX5E_PFLAG_RX_STRIDING_RQ,
 	MLX5E_PFLAG_RX_NO_CSUM_COMPLETE,
+#ifdef HAVE_XDP_SUPPORT
 	MLX5E_PFLAG_XDP_TX_MPWQE,
+#endif
 	MLX5E_PFLAG_SKB_TX_MPWQE,
 	MLX5E_PFLAG_TX_PORT_TS,
 	MLX5E_PFLAG_DROPLESS_RQ,
 	MLX5E_PFLAG_PER_CH_STATS,
+	/* OFED-specific private flags */
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	MLX5E_PFLAG_HWLRO,
+#endif
 	MLX5E_PFLAG_TX_XDP_CSUM,
+	MLX5E_PFLAG_SKB_XMIT_MORE,
 	MLX5E_NUM_PFLAGS, /* Keep last */
 };
 
@@ -329,27 +357,38 @@ struct mlx5e_params {
 	u8  log_rx_page_cache_mult;
 	u16 num_channels;
 	struct {
+#ifdef HAVE_TC_MQPRIO_QOPT_OFFLOAD
 		u16 mode;
+#endif
 		u8 num_tc;
 		struct netdev_tc_txq tc_to_txq[TC_MAX_QUEUE];
+#ifdef HAVE_TC_MQPRIO_QOPT_OFFLOAD
 		struct {
 			u64 max_rate[TC_MAX_QUEUE];
 			u32 hw_id[TC_MAX_QUEUE];
 		} channel;
+#endif
 	} mqprio;
 	bool rx_cqe_compress_def;
 	bool tunneled_offload_en;
 	struct dim_cq_moder rx_cq_moderation;
 	struct dim_cq_moder tx_cq_moderation;
 	struct mlx5e_packet_merge_param packet_merge;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	bool lro_en;
+#endif
 	u8  tx_min_inline_mode;
 	bool vlan_strip_disable;
 	bool scatter_fcs_en;
 	bool rx_dim_enabled;
 	bool tx_dim_enabled;
 	u32 pflags;
+#ifdef HAVE_XDP_SUPPORT
 	struct bpf_prog *xdp_prog;
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_xsk *xsk;
+#endif
 	unsigned int sw_mtu;
 	int hard_mtu;
 	bool ptp_rx;
@@ -361,8 +400,12 @@ struct mlx5e_params {
 
 static inline u8 mlx5e_get_dcb_num_tc(struct mlx5e_params *params)
 {
+#ifdef HAVE_TC_MQPRIO_QOPT_OFFLOAD
 	return params->mqprio.mode == TC_MQPRIO_MODE_DCB ?
 		params->mqprio.num_tc : 1;
+#else
+	return params->mqprio.num_tc;
+#endif
 }
 
 enum {
@@ -374,7 +417,8 @@ enum {
 	MLX5E_RQ_STATE_FPGA_TLS, /* FPGA TLS enabled */
 	MLX5E_RQ_STATE_MINI_CQE_HW_STRIDX, /* set when mini_cqe_resp_stride_index cap is used */
 	MLX5E_RQ_STATE_SHAMPO, /* set when SHAMPO cap is used */
-	MLX5E_RQ_STATE_CACHE_REDUCE_PENDING
+	MLX5E_RQ_STATE_CACHE_REDUCE_PENDING,
+	MLX5E_RQ_STATE_SKB_XMIT_MORE
 };
 
 struct mlx5e_cq {
@@ -386,6 +430,9 @@ struct mlx5e_cq {
 	struct napi_struct        *napi;
 	struct mlx5_core_cq        mcq;
 	struct mlx5e_ch_stats     *ch_stats;
+#ifndef HAVE_NAPI_STATE_MISSED
+	unsigned long             *ch_flags;
+#endif
 
 	/* control */
 	struct net_device         *netdev;
@@ -425,6 +472,7 @@ enum {
 	MLX5E_SQ_STATE_PENDING_XSK_TX,
 	MLX5E_SQ_STATE_PENDING_TLS_RX_RESYNC,
 	MLX5E_SQ_STATE_TX_XDP_CSUM,
+	MLX5E_SQ_STATE_SKB_XMIT_MORE,
 };
 
 struct mlx5e_tx_mpwqe {
@@ -466,6 +514,7 @@ struct mlx5e_txqsq {
 	struct mlx5e_tx_mpwqe      mpwqe;
 
 	struct mlx5e_cq            cq;
+	struct mlx5e_cq_decomp     cqd;
 
 	/* read only */
 	struct mlx5_wq_cyc         wq;
@@ -506,13 +555,21 @@ struct mlx5e_dma_info {
 	u32 refcnt_bias;
 	union {
 		struct page *page;
+#ifdef HAVE_XSK_BUFF_ALLOC
 		struct xdp_buff *xsk;
+#else
+		struct {
+			u64 handle;
+			void *data;
+		} xsk;
+#endif
 	};
 };
 
 /* XDP packets can be transmitted in different ways. On completion, we need to
  * distinguish between them to clean up things in a proper way.
  */
+#ifdef HAVE_XDP_SUPPORT
 enum mlx5e_xdp_xmit_mode {
 	/* An xdp_frame was transmitted due to either XDP_REDIRECT from another
 	 * device or XDP_TX from an XSK RQ. The frame has to be unmapped and
@@ -526,7 +583,7 @@ enum mlx5e_xdp_xmit_mode {
 	MLX5E_XDP_XMIT_MODE_PAGE,
 
 	/* No xdp_frame was created at all, the transmit happened from a UMEM
-	 * page. The UMEM Completion Ring producer pointer has to be increased.
+ * page. The UMEM Completion Ring producer pointer has to be increased.
 	 */
 	MLX5E_XDP_XMIT_MODE_XSK,
 };
@@ -544,6 +601,7 @@ struct mlx5e_xdp_info {
 		} page;
 	};
 };
+#endif /* HAVE_XDP_SUPPORT */
 
 struct mlx5e_xmit_data {
 	dma_addr_t  dma_addr;
@@ -551,6 +609,7 @@ struct mlx5e_xmit_data {
 	u32         len;
 };
 
+#ifdef HAVE_XDP_SUPPORT
 struct mlx5e_xdp_info_fifo {
 	struct mlx5e_xdp_info *xi;
 	u32 *cc;
@@ -581,7 +640,13 @@ struct mlx5e_xdpsq {
 	struct mlx5e_cq            cq;
 
 	/* read only */
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	struct xsk_buff_pool      *xsk_pool;
+#else
+	struct xdp_umem           *umem;
+#endif
+#endif
 	struct mlx5_wq_cyc         wq;
 	struct mlx5e_xdpsq_stats  *stats;
 	mlx5e_fp_xmit_xdp_frame_check xmit_xdp_frame_check;
@@ -604,6 +669,8 @@ struct mlx5e_xdpsq {
 	struct mlx5_wq_ctrl        wq_ctrl;
 	struct mlx5e_channel      *channel;
 } ____cacheline_aligned_in_smp;
+#endif /* HAVE_XDP_SUPPORT */ 
+
 
 struct mlx5e_ktls_resync_resp;
 
@@ -648,8 +715,25 @@ struct mlx5e_umr_dma_info {
 struct mlx5e_mpw_info {
 	struct mlx5e_umr_dma_info umr;
 	u16 consumed_strides;
+#ifdef HAVE_XDP_SUPPORT
 	DECLARE_BITMAP(xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
+#endif
+};
+
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+#define IS_HW_LRO(params) \
+	((params)->lro_en && MLX5E_GET_PFLAG(params, MLX5E_PFLAG_HWLRO))
+
+#define IS_SW_LRO(params) \
+	((params)->lro_en && !MLX5E_GET_PFLAG(params, MLX5E_PFLAG_HWLRO))
+
+/* SW LRO defines for MLX5 */
+#define MLX5E_LRO_MAX_DESC	32
+struct mlx5e_sw_lro {
+	struct net_lro_mgr	lro_mgr;
+	struct net_lro_desc	lro_desc[MLX5E_LRO_MAX_DESC];
 };
+#endif
 
 #define MLX5E_MAX_RX_FRAGS 4
 
@@ -686,7 +770,8 @@ static inline void mlx5e_put_page(struct
 }
 
 struct mlx5e_rq;
-typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq*, struct mlx5_cqe64*);
+typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq*, struct mlx5_cqe64*,
+						       bool xmit_more);
 typedef struct sk_buff *
 (*mlx5e_fp_skb_from_cqe_mpwrq)(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 			       u16 cqe_bcnt, u32 head_offset, u32 page_idx);
@@ -763,6 +848,9 @@ struct mlx5e_rq {
 
 	};
 	struct {
+#if !defined(HAVE_XSK_BUFF_ALLOC) && defined(HAVE_XSK_ZERO_COPY_SUPPORT)
+		u16            umem_headroom;
+#endif
 		u16            headroom;
 		u32            frame0_sz;
 		u8             map_dir;   /* dma map direction */
@@ -792,15 +880,29 @@ struct mlx5e_rq {
 
 	struct mlx5e_dim       dim_obj; /* Adaptive Moderation */
 
+#ifdef HAVE_XDP_SUPPORT
 	/* XDP */
 	struct bpf_prog __rcu *xdp_prog;
 	struct mlx5e_xdpsq    *xdpsq;
+#endif
 	DECLARE_BITMAP(flags, 8);
+#ifdef HAVE_NET_PAGE_POOL_H
 	struct page_pool      *page_pool;
-
+#endif
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_sw_lro   *sw_lro;
+#endif
 	/* AF_XDP zero-copy */
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifndef HAVE_XSK_BUFF_ALLOC
+	struct zero_copy_allocator zca;
+#endif
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	struct xsk_buff_pool  *xsk_pool;
-
+#else
+	struct xdp_umem       *umem;
+#endif
+#endif /* HAVE_XSK_ZERO_COPY_SUPPORT */
 	struct work_struct     recover_work;
 
 	/* control */
@@ -813,8 +915,10 @@ struct mlx5e_rq {
 	u32  umr_mkey;
 	struct mlx5e_dma_info  wqe_overflow;
 
+#ifdef HAVE_XDP_RXQ_INFO
 	/* XDP read-mostly */
 	struct xdp_rxq_info    xdp_rxq;
+#endif
 	cqe_ts_to_ns           ptp_cyc2time;
 } ____cacheline_aligned_in_smp;
 
@@ -823,14 +927,24 @@ enum mlx5e_channel_state {
 	MLX5E_CHANNEL_NUM_STATES
 };
 
+#ifndef HAVE_NAPI_STATE_MISSED
+enum channel_flags {
+	MLX5E_CHANNEL_NAPI_SCHED = 1,
+};
+#endif
+
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
+#ifdef HAVE_XDP_SUPPORT
 	struct mlx5e_xdpsq         rq_xdpsq;
+#endif
 	struct mlx5e_txqsq         sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_icosq         icosq;   /* internal control operations */
 	struct mlx5e_txqsq __rcu * __rcu *qos_sqs;
+#ifdef HAVE_XDP_SUPPORT
 	bool                       xdp;
+#endif
 	struct napi_struct         napi;
 	struct device             *pdev;
 	struct net_device         *netdev;
@@ -838,14 +952,18 @@ struct mlx5e_channel {
 	u16                        qos_sqs_size;
 	u8                         num_tc;
 	u8                         lag_port;
-
+#ifndef HAVE_NAPI_STATE_MISSED
+	unsigned long              flags;
+#endif
+#ifdef HAVE_XDP_SUPPORT
 	/* XDP_REDIRECT */
 	struct mlx5e_xdpsq         xdpsq;
-
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	/* AF_XDP zero-copy */
 	struct mlx5e_rq            xskrq;
 	struct mlx5e_xdpsq         xsksq;
-
+#endif
 	/* Async ICOSQ */
 	struct mlx5e_icosq         async_icosq;
 	/* async_icosq can be accessed from any CPU - the spinlock protects it. */
@@ -880,10 +998,16 @@ struct mlx5e_channel_stats {
 	struct mlx5e_ch_stats ch;
 	struct mlx5e_sq_stats sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rq_stats rq;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_rq_stats xskrq;
+#endif
+#ifdef HAVE_XDP_SUPPORT
 	struct mlx5e_xdpsq_stats rq_xdpsq;
 	struct mlx5e_xdpsq_stats xdpsq;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_xdpsq_stats xsksq;
+#endif
+#endif
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_ptp_stats {
@@ -923,6 +1047,7 @@ struct mlx5e_hv_vhca_stats_agent {
 };
 #endif
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 struct mlx5e_xsk {
 	/* XSK buffer pools are stored separately from channels,
 	 * because we don't want to lose them when channels are
@@ -930,10 +1055,15 @@ struct mlx5e_xsk {
 	 * distinguish between zero-copy and non-zero-copy UMEMs, so
 	 * rely on our mechanism.
 	 */
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	struct xsk_buff_pool **pools;
+#else
+	struct xdp_umem **umems;
+#endif
 	u16 refcnt;
 	bool ever_used;
 };
+#endif
 
 /* Temporary storage for variables that are allocated when struct mlx5e_priv is
  * initialized, and used where we can't allocate them because that functions
@@ -956,9 +1086,9 @@ struct mlx5e_htb {
 	DECLARE_HASHTABLE(qos_tc2node, order_base_2(MLX5E_QOS_MAX_LEAF_NODES));
 	DECLARE_BITMAP(qos_used_qids, MLX5E_QOS_MAX_LEAF_NODES);
 	struct mlx5e_sq_stats **qos_sq_stats;
-	u16 max_qos_sqs;
-	u16 maj_id;
-	u16 defcls;
+	u32 max_qos_sqs;
+	u32 maj_id;
+	u32 defcls;
 	struct mlx5e_select_queue_params *final_selq;
 };
 
@@ -1011,7 +1141,13 @@ struct mlx5e_priv {
 	struct mlx5e_ptp_stats     ptp_stats;
 	u16                        stats_nch;
 	u16                        max_nch;
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	struct mlx5e_sw_lro        sw_lro[MLX5E_MAX_NUM_CHANNELS];
+#endif
 	u8                         max_opened_tc;
+#if !defined(HAVE_NDO_GET_STATS64) && !defined(HAVE_NDO_GET_STATS64_RET_VOID)
+	struct net_device_stats    netdev_stats;
+#endif
 	u8                         shared_rq:1;
 	bool                       tx_ptp_opened;
 	bool                       rx_ptp_opened;
@@ -1021,7 +1157,9 @@ struct mlx5e_priv {
 	struct notifier_block      events_nb;
 	struct notifier_block      blocking_events_nb;
 
+#ifdef HAVE_UDP_TUNNEL_NIC_INFO
 	struct udp_tunnel_nic_info nic_info;
+#endif
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct mlx5e_dcbx          dcbx;
 #endif
@@ -1041,7 +1179,9 @@ struct mlx5e_priv {
 	struct mlx5e_delay_drop delay_drop;
 	struct devlink_health_reporter *tx_reporter;
 	struct devlink_health_reporter *rx_reporter;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	struct mlx5e_xsk           xsk;
+#endif
 #if IS_ENABLED(CONFIG_PCI_HYPERV_INTERFACE)
 	struct mlx5e_hv_vhca_stats_agent stats_agent;
 #endif
@@ -1095,7 +1235,9 @@ struct mlx5e_profile {
 void mlx5e_create_debugfs(struct mlx5e_priv *priv);
 void mlx5e_destroy_debugfs(struct mlx5e_priv *priv);
 
+#ifdef __ETHTOOL_DECLARE_LINK_MODE_MASK
 void mlx5e_build_ptys2ethtool_map(void);
+#endif
 
 bool mlx5e_check_fragmented_striding_rq_cap(struct mlx5_core_dev *mdev);
 
@@ -1103,10 +1245,26 @@ void mlx5e_shampo_dealloc_hd(struct mlx5
 int mlx5e_sysfs_create(struct net_device *dev);
 void mlx5e_sysfs_remove(struct net_device *dev);
 
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
 int mlx5e_setup_tc_mqprio(struct mlx5e_priv *priv,
-			  struct tc_mqprio_qopt_offload *mqprio);
+#ifdef HAVE_TC_MQPRIO_QOPT_OFFLOAD
+			  struct tc_mqprio_qopt_offload *mqprio
+#else
+			  struct tc_mqprio_qopt *mqprio
+#endif
+);
+#else
+int mlx5e_setup_tc(struct net_device *netdev, u8 tc);
+#endif
 
+#ifdef HAVE_NDO_GET_STATS64_RET_VOID
 void mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
+#elif defined(HAVE_NDO_GET_STATS64)
+struct rtnl_link_stats64 * mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
+#else
+struct net_device_stats * mlx5e_get_stats(struct net_device *dev);
+#endif
+
 void mlx5e_fold_sw_stats64(struct mlx5e_priv *priv, struct rtnl_link_stats64 *s);
 
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
@@ -1119,6 +1277,7 @@ void mlx5e_set_rx_mode_work(struct work_
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val, bool rx_filter);
+int mlx5e_modify_tx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
 
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
@@ -1134,6 +1293,9 @@ struct mlx5e_create_cq_param {
 	struct mlx5e_ch_stats *ch_stats;
 	int node;
 	int ix;
+#ifndef HAVE_NAPI_STATE_MISSED
+	unsigned long             *ch_flags;
+#endif
 };
 int mlx5e_wait_for_min_rx_wqes(struct mlx5e_rq *rq, int wait_time);
 void mlx5e_close_rq(struct mlx5e_priv *priv, struct mlx5e_rq *rq);
@@ -1145,12 +1307,22 @@ int mlx5e_create_rq(struct mlx5e_rq *rq,
 void mlx5e_destroy_rq(struct mlx5e_rq *rq);
 
 struct mlx5e_sq_param;
-int mlx5e_open_xdpsq(struct mlx5e_channel *c, struct mlx5e_params *params,
-		     struct mlx5e_sq_param *param, struct xsk_buff_pool *xsk_pool,
-		     struct mlx5e_xdpsq *sq, bool is_redirect);
+#ifdef HAVE_XDP_SUPPORT
 void mlx5e_close_xdpsq(struct mlx5e_xdpsq *sq);
+int mlx5e_open_xdpsq(struct mlx5e_channel *c, struct mlx5e_params *params,
+		     struct mlx5e_sq_param *param,
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+		     struct xsk_buff_pool *xsk_pool,
+#else
+		     struct xdp_umem *umem,
+#endif
+#endif
+		     struct mlx5e_xdpsq *sq,
+                     bool is_redirect);
 void mlx5e_activate_xdpsq(struct mlx5e_xdpsq *sq);
 void mlx5e_deactivate_xdpsq(struct mlx5e_xdpsq *sq);
+#endif
 
 struct mlx5e_cq_param;
 int mlx5e_open_cq(struct mlx5e_priv *priv, struct dim_cq_moder moder,
@@ -1239,6 +1411,8 @@ void mlx5e_destroy_mdev_resources(struct
 int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb,
 		       bool enable_mc_lb);
 int mlx5e_modify_tirs_packet_merge(struct mlx5e_priv *priv);
+int mlx5e_modify_tirs_packet_merge_ctx(struct mlx5e_priv *priv, void *context);
+int mlx5e_update_lro(struct net_device *netdev, bool enable);
 void mlx5e_mkey_set_relaxed_ordering(struct mlx5_core_dev *mdev, void *mkc);
 
 /* common netdev helpers */
@@ -1267,7 +1441,9 @@ int mlx5e_set_dev_port_mtu(struct mlx5e_
 int mlx5e_set_dev_port_mtu_ctx(struct mlx5e_priv *priv, void *context);
 int mlx5e_change_mtu(struct net_device *netdev, int new_mtu,
 		     mlx5e_fp_preactivate preactivate);
+#if defined(HAVE_KERNEL_WITH_VXLAN_SUPPORT_ON) && defined(HAVE_UDP_TUNNEL_NIC_INFO)
 void mlx5e_vxlan_set_netdev_info(struct mlx5e_priv *priv);
+#endif
 
 /* ethtool helpers */
 void mlx5e_ethtool_get_drvinfo(struct mlx5e_priv *priv,
@@ -1286,19 +1462,36 @@ void mlx5e_ethtool_get_channels(struct m
 int mlx5e_ethtool_set_channels(struct mlx5e_priv *priv,
 			       struct ethtool_channels *ch);
 int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
+#ifdef HAVE_NDO_GET_COALESCE_GET_4_PARAMS
 			       struct ethtool_coalesce *coal,
 			       struct kernel_ethtool_coalesce *kernel_coal);
+#else
+			       struct ethtool_coalesce *coal);
+#endif
 int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
+#ifdef HAVE_NDO_GET_COALESCE_GET_4_PARAMS
 			       struct ethtool_coalesce *coal,
 			       struct kernel_ethtool_coalesce *kernel_coal,
 			       struct netlink_ext_ack *extack);
+#else
+			       struct ethtool_coalesce *coal);
+#endif
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 int mlx5e_ethtool_get_link_ksettings(struct mlx5e_priv *priv,
 				     struct ethtool_link_ksettings *link_ksettings);
 int mlx5e_ethtool_set_link_ksettings(struct mlx5e_priv *priv,
 				     const struct ethtool_link_ksettings *link_ksettings);
-int mlx5e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key, u8 *hfunc);
-int mlx5e_set_rxfh(struct net_device *dev, const u32 *indir, const u8 *key,
-		   const u8 hfunc);
+#endif
+#ifdef HAVE_ETHTOOL_GET_SET_SETTINGS
+int mlx5e_get_settings(struct net_device *netdev, struct ethtool_cmd *cmd);
+int mlx5e_set_settings(struct net_device *netdev, struct ethtool_cmd *cmd);
+#endif
+int mlx5e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key,
+                         u8 *hfunc);
+
+int mlx5e_set_rxfh(struct net_device *dev, const u32 *indir,
+                  const u8 *key, const u8 hfunc);
+
 int mlx5e_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info,
 		    u32 *rule_locs);
 int mlx5e_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd);
@@ -1308,6 +1501,12 @@ int mlx5e_ethtool_get_ts_info(struct mlx
 			      struct ethtool_ts_info *info);
 int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
+#ifdef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+#ifndef HAVE_TC_BLOCK_OFFLOAD
+int mlx5e_setup_tc(struct net_device *dev, enum tc_setup_type type,
+		   void *type_data);
+#endif
+#endif
 void mlx5e_ethtool_get_pauseparam(struct mlx5e_priv *priv,
 				  struct ethtool_pauseparam *pauseparam);
 int mlx5e_ethtool_set_pauseparam(struct mlx5e_priv *priv,
@@ -1336,7 +1535,11 @@ int mlx5e_netdev_change_profile(struct m
 				const struct mlx5e_profile *new_profile, void *new_ppriv);
 void mlx5e_netdev_attach_nic_profile(struct mlx5e_priv *priv);
 void mlx5e_set_netdev_mtu_boundaries(struct mlx5e_priv *priv);
-void mlx5e_build_nic_params(struct mlx5e_priv *priv, struct mlx5e_xsk *xsk, u16 mtu);
+void mlx5e_build_nic_params(struct mlx5e_priv *priv,
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+			   struct mlx5e_xsk *xsk,
+#endif
+			   u16 mtu);
 void mlx5e_build_txq_maps(struct mlx5e_priv *priv);
 
 int mlx5e_get_dump_flag(struct net_device *netdev, struct ethtool_dump *dump);
@@ -1353,19 +1556,36 @@ static inline bool mlx5e_dropless_rq_sup
 void mlx5e_rx_dim_work(struct work_struct *work);
 void mlx5e_tx_dim_work(struct work_struct *work);
 
+#ifdef HAVE_GET_SET_LINK_KSETTINGS
 int mlx5e_get_link_ksettings(struct net_device *netdev,
 			     struct ethtool_link_ksettings *link_ksettings);
 int mlx5e_set_link_ksettings(struct net_device *netdev,
 			     const struct ethtool_link_ksettings *link_ksettings);
+#endif
+
+#if defined(HAVE_NDO_UDP_TUNNEL_ADD) || defined(HAVE_NDO_UDP_TUNNEL_ADD_EXTENDED)
+void mlx5e_add_vxlan_port(struct net_device *netdev, struct udp_tunnel_info *ti);
+void mlx5e_del_vxlan_port(struct net_device *netdev, struct udp_tunnel_info *ti);
+#elif defined(HAVE_NDO_ADD_VXLAN_PORT)
+void mlx5e_add_vxlan_port(struct net_device *netdev, sa_family_t sa_family, __be16 port);
+void mlx5e_del_vxlan_port(struct net_device *netdev, sa_family_t sa_family, __be16 port);
+#endif
+
 netdev_features_t mlx5e_features_check(struct sk_buff *skb,
 				       struct net_device *netdev,
 				       netdev_features_t features);
+
+netdev_features_t mlx5e_features_check(struct sk_buff *skb, struct net_device *netdev,
+ 				       netdev_features_t features);
+
 int mlx5e_set_features(struct net_device *netdev, netdev_features_t features);
 #ifdef CONFIG_MLX5_ESWITCH
 int mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac);
 int mlx5e_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate, int max_tx_rate);
 int mlx5e_get_vf_config(struct net_device *dev, int vf, struct ifla_vf_info *ivi);
+#ifdef HAVE_NDO_GET_VF_STATS
 int mlx5e_get_vf_stats(struct net_device *dev, int vf, struct ifla_vf_stats *vf_stats);
+#endif
 bool mlx5e_is_rep_shared_rq(const struct mlx5e_priv *priv);
 #endif
 
