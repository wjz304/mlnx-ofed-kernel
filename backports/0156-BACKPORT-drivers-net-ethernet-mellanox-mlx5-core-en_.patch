From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT:
 drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.h

Change-Id: I1007f33fb6d3b4ea3ce4fe0c51a5fbf90996d74f
---
 .../mellanox/mlx5/core/en_accel/ipsec_rxtx.h  | 35 ++++++++++++++++---
 1 file changed, 30 insertions(+), 5 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/ipsec_rxtx.h
@@ -50,6 +50,9 @@ struct mlx5e_accel_tx_ipsec_state {
 	struct xfrm_state *x;
 	u32 tailen;
 	u32 plen;
+#ifndef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+	u8 inner_ipproto;
+#endif
 };
 
 #ifdef CONFIG_MLX5_EN_IPSEC
@@ -85,6 +88,9 @@ static inline bool mlx5e_ipsec_eseg_meta
 }
 
 void mlx5e_ipsec_tx_build_eseg(struct mlx5e_priv *priv, struct sk_buff *skb,
+#ifndef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+			       struct mlx5e_accel_tx_ipsec_state *ipsec_st,
+#endif
 			       struct mlx5_wqe_eth_seg *eseg);
 
 static inline netdev_features_t
@@ -92,6 +98,12 @@ mlx5e_ipsec_feature_check(struct sk_buff
 {
 	struct xfrm_offload *xo = xfrm_offload(skb);
 	struct sec_path *sp = skb_sec_path(skb);
+#ifdef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+	u8 inner_ipproto = xo->inner_ipproto;
+#else
+	u8 inner_ipproto = 0;
+#endif
+
 
 	if (sp && sp->len && xo) {
 		struct xfrm_state *x = sp->xvec[0];
@@ -99,7 +111,7 @@ mlx5e_ipsec_feature_check(struct sk_buff
 		if (!x || !x->xso.offload_handle)
 			goto out_disable;
 
-		if (xo->inner_ipproto) {
+		if (inner_ipproto) {
 			/* Cannot support tunnel packet over IPsec tunnel mode
 			 * because we cannot offload three IP header csum
 			 */
@@ -107,8 +119,8 @@ mlx5e_ipsec_feature_check(struct sk_buff
 				goto out_disable;
 
 			/* Only support UDP or TCP L4 checksum */
-			if (xo->inner_ipproto != IPPROTO_UDP &&
-			    xo->inner_ipproto != IPPROTO_TCP)
+			if (inner_ipproto != IPPROTO_UDP &&
+			    inner_ipproto != IPPROTO_TCP)
 				goto out_disable;
 		}
 
@@ -123,15 +135,28 @@ out_disable:
 
 static inline bool
 mlx5e_ipsec_txwqe_build_eseg_csum(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+#ifndef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+					struct mlx5e_accel_tx_ipsec_state *ipsec_st,
+#endif
 				  struct mlx5_wqe_eth_seg *eseg)
 {
-	u8 inner_ipproto;
+#ifdef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+	struct xfrm_offload *xo;
+	u32 inner_ipproto;
+#else
+	u8 inner_ipproto = ipsec_st->inner_ipproto;
+#endif
 
 	if (!mlx5e_ipsec_eseg_meta(eseg))
 		return false;
 
+#ifdef HAVE_XFRM_OFFLOAD_INNER_IPPROTO
+	xo = xfrm_offload(skb);
+	inner_ipproto = xo->inner_ipproto;
+#else
+	inner_ipproto = ipsec_st->inner_ipproto;
+#endif
 	eseg->cs_flags = MLX5_ETH_WQE_L3_CSUM;
-	inner_ipproto = xfrm_offload(skb)->inner_ipproto;
 	if (inner_ipproto) {
 		eseg->cs_flags |= MLX5_ETH_WQE_L3_INNER_CSUM;
 		if (inner_ipproto == IPPROTO_TCP || inner_ipproto == IPPROTO_UDP)
