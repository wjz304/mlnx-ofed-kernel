From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en/params.c

Change-Id: I0c59e7686c142f023d49958f3185d1c80ceadb07
---
 .../ethernet/mellanox/mlx5/core/en/params.c   | 81 ++++++++++++++++---
 1 file changed, 70 insertions(+), 11 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
@@ -6,7 +6,14 @@
 #include "en/port.h"
 #include "en_accel/en_accel.h"
 #include "en_accel/ipsec.h"
+#include "en_accel/ktls.h"
+#ifdef HAVE_XDP_SOCK_DRV_H
 #include <net/xdp_sock_drv.h>
+#endif
+
+#ifndef HAVE_XDP_SOCK_DRV_H
+#define XDP_UMEM_MIN_CHUNK_SHIFT 11
+#endif
 
 static u8 mlx5e_mpwrq_min_page_shift(struct mlx5_core_dev *mdev)
 {
@@ -17,7 +24,13 @@ static u8 mlx5e_mpwrq_min_page_shift(str
 
 u8 mlx5e_mpwrq_page_shift(struct mlx5_core_dev *mdev, struct mlx5e_xsk_param *xsk)
 {
-	u8 req_page_shift = xsk ? order_base_2(xsk->chunk_size) : PAGE_SHIFT;
+	u8 req_page_shift =
+#ifdef HAVE_XDP_SUPPORT
+		xsk ? order_base_2(xsk->chunk_size) : PAGE_SHIFT;
+#else
+	PAGE_SHIFT;
+#endif
+
 	u8 min_page_shift = mlx5e_mpwrq_min_page_shift(mdev);
 
 	/* Regular RQ uses order-0 pages, the NIC must be able to map them. */
@@ -217,16 +230,27 @@ u8 mlx5e_mpwrq_max_log_rq_pkts(struct ml
 u16 mlx5e_get_linear_rq_headroom(struct mlx5e_params *params,
 				 struct mlx5e_xsk_param *xsk)
 {
-	u16 headroom;
+	u16 headroom = 0;
 
+#ifdef HAVE_XSK_BUFF_ALLOC
 	if (xsk)
 		return xsk->headroom;
+#endif
 
 	headroom = NET_IP_ALIGN;
-	if (params->xdp_prog)
+#ifdef HAVE_XDP_SUPPORT
+	if (params->xdp_prog) {
 		headroom += XDP_PACKET_HEADROOM;
-	else
+#ifndef HAVE_XSK_BUFF_ALLOC
+		if (xsk)
+			headroom += xsk->headroom;
+#endif
+	} else {
+#endif /* HAVE_XDP_SUPPORT */
 		headroom += MLX5_RX_HEADROOM;
+#ifdef HAVE_XDP_SUPPORT
+	}
+#endif
 
 	return headroom;
 }
@@ -259,9 +283,11 @@ static u32 mlx5e_rx_get_linear_stride_sz
 	if (xsk)
 		return mpwqe ? 1 << mlx5e_mpwrq_page_shift(mdev, xsk) : PAGE_SIZE;
 
+#ifdef HAVE_XDP_SUPPORT
 	/* XDP in mlx5e doesn't support multiple packets per page. */
 	if (params->xdp_prog)
 		return PAGE_SIZE;
+#endif
 
 	return roundup_pow_of_two(mlx5e_rx_get_linear_sz_skb(params, false));
 }
@@ -575,12 +601,15 @@ int mlx5e_mpwrq_validate_regular(struct
 	if (!mlx5e_check_fragmented_striding_rq_cap(mdev, page_shift, umr_mode))
 		return -EOPNOTSUPP;
 
+#ifdef HAVE_XDP_SUPPORT
 	if (params->xdp_prog && !mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL))
 		return -EINVAL;
+#endif
 
 	return 0;
 }
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 int mlx5e_mpwrq_validate_xsk(struct mlx5_core_dev *mdev, struct mlx5e_params *params,
 			     struct mlx5e_xsk_param *xsk)
 {
@@ -612,6 +641,7 @@ int mlx5e_mpwrq_validate_xsk(struct mlx5
 
 	return 0;
 }
+#endif
 
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params)
@@ -619,6 +649,9 @@ void mlx5e_init_rq_type_params(struct ml
 	params->log_rq_mtu_frames = is_kdump_kernel() ?
 		MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE :
 		MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE;
+#ifndef HAVE_DEVLINK_PER_AUXDEV
+	mlx5e_params_print_info(mdev, params);
+#endif
 }
 
 void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
@@ -656,6 +689,9 @@ void mlx5e_build_create_cq_param(struct
 		.ch_stats = c->stats,
 		.node = cpu_to_node(c->cpu),
 		.ix = c->ix,
+#ifndef HAVE_NAPI_STATE_MISSED
+		.ch_flags = &c->flags,
+#endif
 	};
 }
 
@@ -680,6 +716,11 @@ static int mlx5e_build_rq_frags_info(str
 	u32 byte_count = MLX5E_SW2HW_MTU(params, params->sw_mtu);
 	int frag_size_max = DEFAULT_FRAG_SIZE;
 	int first_frag_size_max;
+#ifdef HAVE_XDP_SUPPORT
+	bool xdp_prog = params->xdp_prog;
+#else
+	bool xdp_prog = false;
+#endif
 	u32 buf_size = 0;
 	u16 headroom;
 	int max_mtu;
@@ -708,13 +749,13 @@ static int mlx5e_build_rq_frags_info(str
 	first_frag_size_max = SKB_WITH_OVERHEAD(frag_size_max - headroom);
 
 	max_mtu = mlx5e_max_nonlinear_mtu(first_frag_size_max, frag_size_max,
-					  params->xdp_prog);
-	if (byte_count > max_mtu || params->xdp_prog) {
+					  xdp_prog);
+	if (byte_count > max_mtu || xdp_prog) {
 		frag_size_max = PAGE_SIZE;
 		first_frag_size_max = SKB_WITH_OVERHEAD(frag_size_max - headroom);
 
 		max_mtu = mlx5e_max_nonlinear_mtu(first_frag_size_max, frag_size_max,
-						  params->xdp_prog);
+						  xdp_prog);
 		if (byte_count > max_mtu) {
 			mlx5_core_err(mdev, "MTU %u is too big for non-linear legacy RQ (max %d)\n",
 				      params->sw_mtu, max_mtu);
@@ -734,7 +775,7 @@ static int mlx5e_build_rq_frags_info(str
 		info->arr[i].frag_size = frag_size;
 		buf_size += frag_size;
 
-		if (params->xdp_prog) {
+		if (xdp_prog) {
 			/* XDP multi buffer expects fragments of the same size. */
 			info->arr[i].frag_stride = frag_size_max;
 		} else {
@@ -785,7 +826,9 @@ out:
 
 	info->log_num_frags = order_base_2(info->num_frags);
 
+#ifdef HAVE_XDP_SUPPORT
 	*xdp_frag_size = info->num_frags > 1 && params->xdp_prog ? PAGE_SIZE : 0;
+#endif
 
 	return 0;
 }
@@ -871,10 +914,13 @@ static void mlx5e_build_rx_cq_param(stru
 
 static u8 rq_end_pad_mode(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
 {
-	bool lro_en = params->packet_merge.type == MLX5E_PACKET_MERGE_LRO;
 	bool ro = MLX5_CAP_GEN(mdev, relaxed_ordering_write);
 
-	return ro && lro_en ?
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	return ro && IS_HW_LRO(params)?
+#else
+	return ro && (params->packet_merge.type == MLX5E_PACKET_MERGE_LRO) ?
+#endif
 		MLX5_WQ_END_PAD_MODE_NONE : MLX5_WQ_END_PAD_MODE_ALIGN;
 }
 
@@ -978,6 +1024,10 @@ void mlx5e_build_tx_cq_param(struct mlx5
 	void *cqc = param->cqc;
 
 	MLX5_SET(cqc, cqc, log_cq_size, params->log_sq_size);
+#ifdef HAVE_BASECODE_EXTRAS
+	if (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_TX_CQE_COMPRESS))
+		MLX5_SET(cqc, cqc, cqe_comp_en, 1);
+#endif
 
 	mlx5e_build_common_cq_param(mdev, param);
 	param->cq_period_mode = params->tx_cq_moderation.cq_period_mode;
@@ -1112,6 +1162,7 @@ static u8 mlx5e_build_icosq_log_wq_sz(st
 	/* UMR WQEs for the regular RQ. */
 	wqebbs = mlx5e_mpwrq_total_umr_wqebbs(mdev, params, NULL);
 
+#ifdef HAVE_XDP_SUPPORT
 	/* If XDP program is attached, XSK may be turned on at any time without
 	 * restarting the channel. ICOSQ must be big enough to fit UMR WQEs of
 	 * both regular RQ and XSK RQ.
@@ -1156,7 +1207,7 @@ static u8 mlx5e_build_icosq_log_wq_sz(st
 
 		wqebbs += max_xsk_wqebbs;
 	}
-
+#endif
 	if (params->packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO)
 		wqebbs += mlx5e_shampo_icosq_sz(mdev, params, rqp);
 
@@ -1177,8 +1228,10 @@ static u8 mlx5e_build_icosq_log_wq_sz(st
 
 static u8 mlx5e_build_async_icosq_log_wq_sz(struct mlx5_core_dev *mdev)
 {
+#ifdef HAVE_UAPI_LINUX_TLS_H
 	if (mlx5e_is_ktls_rx(mdev))
 		return MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
+#endif
 
 	return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
 }
@@ -1206,14 +1259,17 @@ static void mlx5e_build_async_icosq_para
 
 	mlx5e_build_sq_param_common(mdev, param);
 	param->stop_room = mlx5e_stop_room_for_wqe(mdev, 1); /* for XSK NOP */
+#ifdef HAVE_UAPI_LINUX_TLS_H
 	param->is_tls = mlx5e_is_ktls_rx(mdev);
 	if (param->is_tls)
 		param->stop_room += mlx5e_stop_room_for_wqe(mdev, 1); /* for TLS RX resync NOP */
+#endif
 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(mdev, reg_umr_sq));
 	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
 	mlx5e_build_ico_cq_param(mdev, log_wq_size, &param->cqp);
 }
 
+#ifdef HAVE_XDP_SUPPORT
 void mlx5e_build_xdpsq_param(struct mlx5_core_dev *mdev,
 			     struct mlx5e_params *params,
 			     struct mlx5e_xsk_param *xsk,
@@ -1228,6 +1284,7 @@ void mlx5e_build_xdpsq_param(struct mlx5
 	param->is_xdp_mb = !mlx5e_rx_is_linear_skb(mdev, params, xsk);
 	mlx5e_build_tx_cq_param(mdev, params, &param->cqp);
 }
+#endif
 
 int mlx5e_build_channel_param(struct mlx5_core_dev *mdev,
 			      struct mlx5e_params *params,
@@ -1245,7 +1302,9 @@ int mlx5e_build_channel_param(struct mlx
 	async_icosq_log_wq_sz = mlx5e_build_async_icosq_log_wq_sz(mdev);
 
 	mlx5e_build_sq_param(mdev, params, &cparam->txq_sq);
+#ifdef HAVE_XDP_SUPPORT
 	mlx5e_build_xdpsq_param(mdev, params, NULL, &cparam->xdp_sq);
+#endif
 	mlx5e_build_icosq_param(mdev, icosq_log_wq_sz, &cparam->icosq);
 	mlx5e_build_async_icosq_param(mdev, async_icosq_log_wq_sz, &cparam->async_icosq);
 
