From: Tom Wu <tomwu@nvidia.com>
Subject: [PATCH] BACKPORT: net/sunrpc/xprtrdma/frwr_ops.c

Change-Id: Idb8b54ab2d3e1067a5ef3b4245de2e2da89f1b12
Signed-off-by: Tom Wu <tomwu@nvidia.com>
---
 net/sunrpc/xprtrdma/frwr_ops.c | 46 ++++++++++++++++++++++++++++++++++
 1 file changed, 46 insertions(+)

--- a/net/sunrpc/xprtrdma/frwr_ops.c
+++ b/net/sunrpc/xprtrdma/frwr_ops.c
@@ -43,7 +43,9 @@
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
+#ifdef HAVE_TRACE_RPCRDMA_H
 #include <trace/events/rpcrdma.h>
+#endif
 
 #ifdef CONFIG_NVFS
 #define NVFS_FRWR
@@ -51,6 +53,12 @@
 #include "nvfs_rpc_rdma.h"
 #endif
 
+#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
+#ifndef RPCDBG_FACILITY
+#define RPCDBG_FACILITY    RPCDBG_TRANS
+#endif
+#endif
+
 static void frwr_cid_init(struct rpcrdma_ep *ep,
 			  struct rpcrdma_mr *mr)
 {
@@ -63,7 +71,9 @@ static void frwr_cid_init(struct rpcrdma
 static void frwr_mr_unmap(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr *mr)
 {
 	if (mr->mr_device) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_unmap(mr);
+#endif
 #ifdef CONFIG_NVFS
 		if (rpcrdma_nvfs_unmap_data(mr->mr_device->dma_device,
 					    mr->mr_sg, mr->mr_nents, mr->mr_dir))
@@ -90,8 +100,10 @@ void frwr_mr_release(struct rpcrdma_mr *
 	frwr_mr_unmap(mr->mr_xprt, mr);
 
 	rc = ib_dereg_mr(mr->mr_ibmr);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	if (rc)
 		trace_xprtrdma_frwr_dereg(mr, rc);
+#endif
 	kfree(mr->mr_sg);
 	kfree(mr);
 }
@@ -161,7 +173,9 @@ int frwr_mr_init(struct rpcrdma_xprt *r_
 
 out_mr_err:
 	kfree(sg);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_alloc(mr, PTR_ERR(frmr));
+#endif
 	return PTR_ERR(frmr);
 }
 
@@ -364,16 +378,22 @@ struct rpcrdma_mr_seg *frwr_map(struct r
 	mr->mr_handle = ibmr->rkey;
 	mr->mr_length = ibmr->length;
 	mr->mr_offset = ibmr->iova;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_mr_map(mr);
+#endif
 
 	return seg;
 
 out_dmamap_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_sgerr(mr, i);
+#endif
 	return ERR_PTR(-EIO);
 
 out_mapmr_err:
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_frwr_maperr(mr, n);
+#endif
 	return ERR_PTR(-EIO);
 }
 
@@ -386,11 +406,13 @@ out_mapmr_err:
  */
 static void frwr_wc_fastreg(struct ib_cq *cq, struct ib_wc *wc)
 {
+#ifdef HAVE_TRACE_RPCRDMA_H
 	struct ib_cqe *cqe = wc->wr_cqe;
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
 	trace_xprtrdma_wc_fastreg(wc, &mr->mr_cid);
+#endif
 
 	rpcrdma_flush_disconnect(cq->cq_context, wc);
 }
@@ -420,7 +442,9 @@ int frwr_send(struct rpcrdma_xprt *r_xpr
 	num_wrs = 1;
 	post_wr = send_wr;
 	list_for_each_entry(mr, &req->rl_registered, mr_list) {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_fastreg(mr);
+#endif
 
 		mr->mr_cqe.done = frwr_wc_fastreg;
 		mr->mr_regwr.wr.next = post_wr;
@@ -441,10 +465,14 @@ int frwr_send(struct rpcrdma_xprt *r_xpr
 		ep->re_send_count -= num_wrs;
 	}
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_post_send(req);
+#endif
 	ret = ib_post_send(ep->re_id->qp, post_wr, NULL);
+#ifdef HAVE_TRACE_RPCRDMA_H
 	if (ret)
 		trace_xprtrdma_post_send_err(r_xprt, req, ret);
+#endif
 	return ret;
 }
 
@@ -461,7 +489,9 @@ void frwr_reminv(struct rpcrdma_rep *rep
 	list_for_each_entry(mr, mrs, mr_list)
 		if (mr->mr_handle == rep->rr_inv_rkey) {
 			list_del_init(&mr->mr_list);
+#ifdef HAVE_TRACE_RPCRDMA_H
 			trace_xprtrdma_mr_reminv(mr);
+#endif
 			frwr_mr_put(mr);
 			break;	/* only one invalidated MR per RPC */
 		}
@@ -485,7 +515,9 @@ static void frwr_wc_localinv(struct ib_c
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_li(wc, &mr->mr_cid);
+#endif
 	frwr_mr_done(wc, mr);
 
 	rpcrdma_flush_disconnect(cq->cq_context, wc);
@@ -504,7 +536,9 @@ static void frwr_wc_localinv_wake(struct
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 
 	/* WARNING: Only wr_cqe and status are reliable at this point */
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_wc_li_wake(wc, &mr->mr_cid);
+#endif
 	frwr_mr_done(wc, mr);
 	complete(&mr->mr_linv_done);
 
@@ -538,7 +572,9 @@ void frwr_unmap_sync(struct rpcrdma_xprt
 	prev = &first;
 	mr = rpcrdma_mr_pop(&req->rl_registered);
 	do {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_localinv(mr);
+#endif
 		r_xprt->rx_stats.local_inv_needed++;
 
 		last = &mr->mr_invwr;
@@ -581,8 +617,10 @@ void frwr_unmap_sync(struct rpcrdma_xprt
 	if (!rc)
 		return;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	/* On error, the MRs get destroyed once the QP has drained. */
 	trace_xprtrdma_post_linv_err(req, rc);
+#endif
 
 	/* Force a connection loss to ensure complete recovery.
 	 */
@@ -601,8 +639,10 @@ static void frwr_wc_localinv_done(struct
 	struct rpcrdma_mr *mr = container_of(cqe, struct rpcrdma_mr, mr_cqe);
 	struct rpcrdma_rep *rep;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	/* WARNING: Only wr_cqe and status are reliable at this point */
 	trace_xprtrdma_wc_li_done(wc, &mr->mr_cid);
+#endif
 
 	/* Ensure that @rep is generated before the MR is released */
 	rep = mr->mr_req->rl_reply;
@@ -641,7 +681,9 @@ void frwr_unmap_async(struct rpcrdma_xpr
 	prev = &first;
 	mr = rpcrdma_mr_pop(&req->rl_registered);
 	do {
+#ifdef HAVE_TRACE_RPCRDMA_H
 		trace_xprtrdma_mr_localinv(mr);
+#endif
 		r_xprt->rx_stats.local_inv_needed++;
 
 		last = &mr->mr_invwr;
@@ -674,8 +716,10 @@ void frwr_unmap_async(struct rpcrdma_xpr
 	if (!rc)
 		return;
 
+#ifdef HAVE_TRACE_RPCRDMA_H
 	/* On error, the MRs get destroyed once the QP has drained. */
 	trace_xprtrdma_post_linv_err(req, rc);
+#endif
 
 	/* The final LOCAL_INV WR in the chain is supposed to
 	 * do the wake. If it was never posted, the wake does
@@ -712,7 +756,9 @@ int frwr_wp_create(struct rpcrdma_xprt *
 	seg.mr_offset = offset_in_page(ep->re_write_pad);
 	if (IS_ERR(frwr_map(r_xprt, &seg, 1, true, xdr_zero, mr)))
 		return -EIO;
+#ifdef HAVE_TRACE_RPCRDMA_H
 	trace_xprtrdma_mr_fastreg(mr);
+#endif
 
 	mr->mr_cqe.done = frwr_wc_fastreg;
 	mr->mr_regwr.wr.next = NULL;
