From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/nvme/target/tcp.c

Change-Id: I76919c53f32485e22b1728756f79e3aa633259b2
---
 drivers/nvme/target/tcp.c | 122 ++++++++++++++++++++++++++++++++++++--
 1 file changed, 118 insertions(+), 4 deletions(-)

--- a/drivers/nvme/target/tcp.c
+++ b/drivers/nvme/target/tcp.c
@@ -3,6 +3,9 @@
  * NVMe over Fabrics TCP target.
  * Copyright (c) 2018 Lightbits Labs. All rights reserved.
  */
+#ifdef pr_fmt
+#undef pr_fmt
+#endif
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 #include <linux/module.h>
 #include <linux/init.h>
@@ -321,18 +324,26 @@ static void nvmet_tcp_build_pdu_iovec(st
 
 	while (length) {
 		u32 iov_len = min_t(u32, length, sg->length - sg_offset);
-
+#ifdef HAVE_BVEC_SET_PAGE
 		bvec_set_page(iov, sg_page(sg), sg->length,
 				sg->offset + sg_offset);
-
+#else
+		iov->bv_page = sg_page(sg);
+		iov->bv_len = sg->length;
+		iov->bv_offset = sg->offset + sg_offset;
+#endif
 		length -= iov_len;
 		sg = sg_next(sg);
 		iov++;
 		sg_offset = 0;
 	}
-
+#ifdef HAVE_ITER_DEST
 	iov_iter_bvec(&cmd->recv_msg.msg_iter, ITER_DEST, cmd->iov,
 		      nr_pages, cmd->pdu_len);
+#else
+	iov_iter_bvec(&cmd->recv_msg.msg_iter, READ, cmd->iov,
+		      nr_pages, cmd->pdu_len);
+#endif
 }
 
 static void nvmet_tcp_fatal_error(struct nvmet_tcp_queue *queue)
@@ -550,13 +561,25 @@ static void nvmet_tcp_execute_request(st
 
 static int nvmet_try_send_data_pdu(struct nvmet_tcp_cmd *cmd)
 {
+#ifndef HAVE_PROTO_OPS_SENDPAGE
+	struct msghdr msg = {
+		.msg_flags = MSG_DONTWAIT | MSG_MORE | MSG_SPLICE_PAGES,
+	};
+	struct bio_vec bvec;
+#endif
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->data_pdu) - cmd->offset + hdgst;
 	int ret;
 
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 	ret = kernel_sendpage(cmd->queue->sock, virt_to_page(cmd->data_pdu),
 			offset_in_page(cmd->data_pdu) + cmd->offset,
 			left, MSG_DONTWAIT | MSG_MORE | MSG_SENDPAGE_NOTLAST);
+#else
+	bvec_set_virt(&bvec, (void *)cmd->data_pdu + cmd->offset, left);
+	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
+	ret = sock_sendmsg(cmd->queue->sock, &msg);
+#endif
 	if (ret <= 0)
 		return ret;
 
@@ -577,17 +600,35 @@ static int nvmet_try_send_data(struct nv
 	int ret;
 
 	while (cmd->cur_sg) {
+#ifndef HAVE_PROTO_OPS_SENDPAGE
+		struct msghdr msg = {
+			.msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES,
+		};
+		struct bio_vec bvec;
+#endif
 		struct page *page = sg_page(cmd->cur_sg);
 		u32 left = cmd->cur_sg->length - cmd->offset;
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 		int flags = MSG_DONTWAIT;
+#endif
 
 		if ((!last_in_batch && cmd->queue->send_list_len) ||
 		    cmd->wbytes_done + left < cmd->req.transfer_len ||
 		    queue->data_digest || !queue->nvme_sq.sqhd_disabled)
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 			flags |= MSG_MORE | MSG_SENDPAGE_NOTLAST;
+#else
+			msg.msg_flags |= MSG_MORE;
+#endif
 
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 		ret = kernel_sendpage(cmd->queue->sock, page, cmd->offset,
 					left, flags);
+#else
+		bvec_set_page(&bvec, page, left, cmd->offset);
+		iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
+		ret = sock_sendmsg(cmd->queue->sock, &msg);
+#endif
 		if (ret <= 0)
 			return ret;
 
@@ -623,18 +664,34 @@ static int nvmet_try_send_data(struct nv
 static int nvmet_try_send_response(struct nvmet_tcp_cmd *cmd,
 		bool last_in_batch)
 {
+#ifndef HAVE_PROTO_OPS_SENDPAGE
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES, };
+	struct bio_vec bvec;
+#endif
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->rsp_pdu) - cmd->offset + hdgst;
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 	int flags = MSG_DONTWAIT;
+#endif
 	int ret;
 
 	if (!last_in_batch && cmd->queue->send_list_len)
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 		flags |= MSG_MORE | MSG_SENDPAGE_NOTLAST;
 	else
 		flags |= MSG_EOR;
 
 	ret = kernel_sendpage(cmd->queue->sock, virt_to_page(cmd->rsp_pdu),
 		offset_in_page(cmd->rsp_pdu) + cmd->offset, left, flags);
+#else
+		msg.msg_flags |= MSG_MORE;
+	else
+		msg.msg_flags |= MSG_EOR;
+
+	bvec_set_virt(&bvec, (void *)cmd->rsp_pdu + cmd->offset, left);
+	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
+	ret = sock_sendmsg(cmd->queue->sock, &msg);
+#endif
 	if (ret <= 0)
 		return ret;
 	cmd->offset += ret;
@@ -651,18 +708,35 @@ static int nvmet_try_send_response(struc
 
 static int nvmet_try_send_r2t(struct nvmet_tcp_cmd *cmd, bool last_in_batch)
 {
+#ifndef HAVE_PROTO_OPS_SENDPAGE
+	struct msghdr msg = { .msg_flags = MSG_DONTWAIT | MSG_SPLICE_PAGES, };
+	struct bio_vec bvec;
+#endif
 	u8 hdgst = nvmet_tcp_hdgst_len(cmd->queue);
 	int left = sizeof(*cmd->r2t_pdu) - cmd->offset + hdgst;
+
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 	int flags = MSG_DONTWAIT;
+#endif
 	int ret;
 
 	if (!last_in_batch && cmd->queue->send_list_len)
+#ifdef HAVE_PROTO_OPS_SENDPAGE
 		flags |= MSG_MORE | MSG_SENDPAGE_NOTLAST;
 	else
 		flags |= MSG_EOR;
 
 	ret = kernel_sendpage(cmd->queue->sock, virt_to_page(cmd->r2t_pdu),
 		offset_in_page(cmd->r2t_pdu) + cmd->offset, left, flags);
+#else
+		msg.msg_flags |= MSG_MORE;
+	else
+		msg.msg_flags |= MSG_EOR;
+
+	bvec_set_virt(&bvec, (void *)cmd->r2t_pdu + cmd->offset, left);
+	iov_iter_bvec(&msg.msg_iter, ITER_SOURCE, &bvec, 1, left);
+	ret = sock_sendmsg(cmd->queue->sock, &msg);
+#endif
 	if (ret <= 0)
 		return ret;
 	cmd->offset += ret;
@@ -1470,7 +1544,9 @@ static void nvmet_tcp_data_ready(struct
 {
 	struct nvmet_tcp_queue *queue;
 
+#ifdef HAVE_TRACE_EVENTS_TRACE_SK_DATA_READY
 	trace_sk_data_ready(sk);
+#endif
 
 	read_lock_bh(&sk->sk_callback_lock);
 	queue = sk->sk_user_data;
@@ -1533,14 +1609,27 @@ static int nvmet_tcp_set_queue_sock(stru
 	struct socket *sock = queue->sock;
 	struct inet_sock *inet = inet_sk(sock->sk);
 	int ret;
+#ifndef HAVE_KERNEL_GETSOCKNAME_2_PARAMS
+	int len;
+#endif
 
+#ifdef HAVE_KERNEL_GETSOCKNAME_2_PARAMS
 	ret = kernel_getsockname(sock,
 		(struct sockaddr *)&queue->sockaddr);
+#else
+	ret = kernel_getsockname(sock,
+		(struct sockaddr *)&queue->sockaddr, &len);
+#endif
 	if (ret < 0)
 		return ret;
 
+#ifdef HAVE_KERNEL_GETSOCKNAME_2_PARAMS
 	ret = kernel_getpeername(sock,
 		(struct sockaddr *)&queue->sockaddr_peer);
+#else
+	ret = kernel_getpeername(sock,
+		(struct sockaddr *)&queue->sockaddr_peer, &len);
+#endif
 	if (ret < 0)
 		return ret;
 
@@ -1555,8 +1644,19 @@ static int nvmet_tcp_set_queue_sock(stru
 		sock_set_priority(sock->sk, so_priority);
 
 	/* Set socket type of service */
+#ifdef HAVE_IP_SOCK_SET_TOS
 	if (inet->rcv_tos > 0)
 		ip_sock_set_tos(sock->sk, inet->rcv_tos);
+#else
+	if (inet->rcv_tos > 0) {
+		int tos = inet->rcv_tos;
+
+		ret = kernel_setsockopt(sock, SOL_IP, IP_TOS,
+			(char *)&tos, sizeof(tos));
+		if (ret)
+			return ret;
+	}
+#endif
 
 	ret = 0;
 	write_lock_bh(&sock->sk->sk_callback_lock);
@@ -1669,8 +1769,9 @@ static void nvmet_tcp_listen_data_ready(
 {
 	struct nvmet_tcp_port *port;
 
+#ifdef HAVE_TRACE_EVENTS_TRACE_SK_DATA_READY
 	trace_sk_data_ready(sk);
-
+#endif
 	read_lock_bh(&sk->sk_callback_lock);
 	port = sk->sk_user_data;
 	if (!port)
@@ -1687,6 +1788,9 @@ static int nvmet_tcp_add_port(struct nvm
 	struct nvmet_tcp_port *port;
 	__kernel_sa_family_t af;
 	int ret;
+#ifndef HAVE_TCP_SOCK_SET_NODELAY
+	int opt;
+#endif
 
 	port = kzalloc(sizeof(*port), GFP_KERNEL);
 	if (!port)
@@ -1730,7 +1834,17 @@ static int nvmet_tcp_add_port(struct nvm
 	port->data_ready = port->sock->sk->sk_data_ready;
 	port->sock->sk->sk_data_ready = nvmet_tcp_listen_data_ready;
 	sock_set_reuseaddr(port->sock->sk);
+#ifdef HAVE_TCP_SOCK_SET_NODELAY
 	tcp_sock_set_nodelay(port->sock->sk);
+#else
+	opt = 1;
+	ret = kernel_setsockopt(port->sock, IPPROTO_TCP,
+			TCP_NODELAY, (char *)&opt, sizeof(opt));
+	if (ret) {
+		pr_err("failed to set TCP_NODELAY sock opt %d\n", ret);
+		goto err_sock;
+	}
+#endif
 	if (so_priority > 0)
 		sock_set_priority(port->sock->sk, so_priority);
 
