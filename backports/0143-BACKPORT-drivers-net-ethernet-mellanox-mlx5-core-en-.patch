From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c

Change-Id: I4d909bb550c8a94cd257d2fd2bfb2930048e5ada
---
 .../net/ethernet/mellanox/mlx5/core/en/xdp.c  | 379 +++++++++++++++++-
 1 file changed, 357 insertions(+), 22 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.c
@@ -30,8 +30,22 @@
  * SOFTWARE.
  */
 
+#ifdef HAVE_XDP_SUPPORT
 #include <linux/bpf_trace.h>
+#ifdef HAVE_NET_PAGE_POOL_OLD_H
+#include <net/page_pool.h>
+#endif
+#ifdef HAVE_NET_PAGE_POOL_TYPES_H
+#include <net/page_pool/types.h>
+#include <net/page_pool/helpers.h>
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifdef HAVE_XDP_SOCK_DRV_H
 #include <net/xdp_sock_drv.h>
+#else
+#include <net/xdp_sock.h>
+#endif
+#endif
 #include "en/xdp.h"
 #include "en/params.h"
 #include <linux/bitfield.h>
@@ -66,16 +80,23 @@ mlx5e_xmit_xdp_buff(struct mlx5e_xdpsq *
 	struct mlx5e_xdp_info xdpi;
 	struct xdp_frame *xdpf;
 	dma_addr_t dma_addr;
+#ifdef HAVE_XDP_HAS_FRAGS
 	int i;
+#endif
 
 	page = !au ? NULL : au->page;
+#ifdef HAVE_XDP_CONVERT_BUFF_TO_FRAME
 	xdpf = xdp_convert_buff_to_frame(xdp);
+#else
+	xdpf = convert_to_xdp_frame(xdp);
+#endif
 	if (unlikely(!xdpf))
 		return false;
 
 	xdptxd.data = xdpf->data;
 	xdptxd.len  = xdpf->len;
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	if (xdp->rxq->mem.type == MEM_TYPE_XSK_BUFF_POOL) {
 		/* The xdp_buff was in the UMEM and was copied into a newly
 		 * allocated page. The UMEM page was returned via the ZCA, and
@@ -109,6 +130,7 @@ mlx5e_xmit_xdp_buff(struct mlx5e_xdpsq *
 		mlx5e_xdpi_fifo_push(&sq->db.xdpi_fifo, &xdpi);
 		return true;
 	}
+#endif
 
 	/* Driver assumes that xdp_convert_buff_to_frame returns an xdp_frame
 	 * that points to the same memory region as the original xdp_buff. It
@@ -119,9 +141,18 @@ mlx5e_xmit_xdp_buff(struct mlx5e_xdpsq *
 	xdpi.mode = MLX5E_XDP_XMIT_MODE_PAGE;
 	xdpi.page.rq = rq;
 
+#ifdef HAVE_PAGE_POOL_GET_DMA_ADDR
 	dma_addr = page_pool_get_dma_addr(page) + (xdpf->data - (void *)xdpf);
+#elif defined(HAVE_PAGE_DMA_ADDR_ARRAY)
+	dma_addr = page->dma_addr[0] + (xdpf->data - (void *)xdpf);
+#elif defined(HAVE_PAGE_DMA_ADDR)
+	dma_addr = page->dma_addr + (xdpf->data - (void *)xdpf);
+#else
+	dma_addr = au->addr + (xdpf->data - (void *)xdpf);
+#endif
 	dma_sync_single_for_device(sq->pdev, dma_addr, xdptxd.len, DMA_BIDIRECTIONAL);
 
+#ifdef HAVE_XDP_HAS_FRAGS
 	if (unlikely(xdp_frame_has_frags(xdpf))) {
 		sinfo = xdp_get_shared_info_from_frame(xdpf);
 
@@ -137,6 +168,7 @@ mlx5e_xmit_xdp_buff(struct mlx5e_xdpsq *
 						   DMA_BIDIRECTIONAL);
 		}
 	}
+#endif
 
 	xdptxd.dma_addr = dma_addr;
 
@@ -147,6 +179,7 @@ mlx5e_xmit_xdp_buff(struct mlx5e_xdpsq *
 	xdpi.page.au = *au;
 	mlx5e_xdpi_fifo_push(&sq->db.xdpi_fifo, &xdpi);
 
+#ifdef HAVE_XDP_HAS_FRAGS
 	if (unlikely(xdp_frame_has_frags(xdpf))) {
 		for (i = 0; i < sinfo->nr_frags; i++) {
 			skb_frag_t *frag = &sinfo->frags[i];
@@ -155,10 +188,12 @@ mlx5e_xmit_xdp_buff(struct mlx5e_xdpsq *
 			mlx5e_xdpi_fifo_push(&sq->db.xdpi_fifo, &xdpi);
 		}
 	}
+#endif
 
 	return true;
 }
 
+#ifdef HAVE_XDP_METADATA_OPS
 static int mlx5e_xdp_rx_timestamp(const struct xdp_md *ctx, u64 *timestamp)
 {
 	const struct mlx5e_xdp_buff *_ctx = (void *)ctx;
@@ -244,6 +279,7 @@ const struct xdp_metadata_ops mlx5e_xdp_
 	.xmo_rx_timestamp		= mlx5e_xdp_rx_timestamp,
 	.xmo_rx_hash			= mlx5e_xdp_rx_hash,
 };
+#endif
 
 /* returns true if packet was consumed by xdp */
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_alloc_unit *au,
@@ -252,10 +288,103 @@ bool mlx5e_xdp_handle(struct mlx5e_rq *r
 	struct xdp_buff *xdp = &mxbuf->xdp;
 	struct page *page;
 	u32 act;
+#ifdef HAVE_XDP_SUPPORT
 	int err;
+#endif
 
 	page = !au ? NULL : au->page;
 	act = bpf_prog_run_xdp(prog, xdp);
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifndef HAVE_XSK_BUFF_ALLOC
+	if (xdp->rxq->mem.type == MEM_TYPE_XSK_BUFF_POOL) {
+		u64 off = xdp->data - xdp->data_hard_start;
+
+#ifdef HAVE_XSK_UMEM_ADJUST_OFFSET
+		xdp->handle = xsk_umem_adjust_offset(rq->umem, xdp->handle, off);
+#else
+		xdp->handle = xdp->handle + off;
+#endif
+	}
+#endif
+#endif
+	switch (act) {
+	case XDP_PASS:
+		return false;
+	case XDP_TX:
+		if (unlikely(!mlx5e_xmit_xdp_buff(rq->xdpsq, rq, au, xdp)))
+			goto xdp_abort;
+		__set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags); /* non-atomic */
+		return true;
+#ifdef HAVE_XDP_SUPPORT
+	case XDP_REDIRECT:
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+		if (xdp->rxq->mem.type != MEM_TYPE_XSK_BUFF_POOL)
+#endif
+		{
+			page_ref_sub(page, au->refcnt_bias);
+			au->refcnt_bias = 0;
+		}
+		/* When XDP enabled then page-refcnt==1 here */
+		err = xdp_do_redirect(rq->netdev, xdp, prog);
+		if (unlikely(err))
+			goto xdp_abort;
+		__set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags);
+		__set_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags);
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+		if (xdp->rxq->mem.type != MEM_TYPE_XSK_BUFF_POOL)
+#endif
+#ifdef HAVE_PAGE_DMA_ADDR
+			mlx5e_page_dma_unmap(rq, virt_to_page(xdp->data));
+#else
+			mlx5e_page_dma_unmap(rq, au);
+#endif
+		rq->stats->xdp_redirect++;
+		return true;
+#endif
+	default:
+#ifdef HAVE_BPF_WARN_IVALID_XDP_ACTION_GET_3_PARAMS
+		bpf_warn_invalid_xdp_action(rq->netdev, prog, act);
+#else
+		bpf_warn_invalid_xdp_action(act);
+#endif
+		fallthrough;
+	case XDP_ABORTED:
+xdp_abort:
+#if defined(HAVE_TRACE_XDP_EXCEPTION) && !defined(MLX_DISABLE_TRACEPOINTS)
+		trace_xdp_exception(rq->netdev, prog, act);
+		fallthrough;
+#endif
+	case XDP_DROP:
+		rq->stats->xdp_drop++;
+		return true;
+	}
+}
+
+#ifndef HAVE_XSK_BUFF_ALLOC
+bool mlx5e_xdp_handle_old(struct mlx5e_rq *rq, struct mlx5e_alloc_unit *au,
+		      struct bpf_prog *prog, struct xdp_buff *xdp)
+{
+	struct page *page;
+	u32 act;
+#ifdef HAVE_XDP_SUPPORT
+	int err;
+#endif
+
+	page = !au ? NULL : au->page;
+	act = bpf_prog_run_xdp(prog, xdp);
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+#ifndef HAVE_XSK_BUFF_ALLOC
+	if (xdp->rxq->mem.type == MEM_TYPE_XSK_BUFF_POOL) {
+		u64 off = xdp->data - xdp->data_hard_start;
+
+#ifdef HAVE_XSK_UMEM_ADJUST_OFFSET
+		xdp->handle = xsk_umem_adjust_offset(rq->umem, xdp->handle, off);
+#else
+		xdp->handle = xdp->handle + off;
+#endif
+	}
+#endif
+#endif
 	switch (act) {
 	case XDP_PASS:
 		return false;
@@ -264,8 +393,12 @@ bool mlx5e_xdp_handle(struct mlx5e_rq *r
 			goto xdp_abort;
 		__set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags); /* non-atomic */
 		return true;
+#ifdef HAVE_XDP_SUPPORT
 	case XDP_REDIRECT:
-		if (au && xdp->rxq->mem.type != MEM_TYPE_XSK_BUFF_POOL) {
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+       		if (au && xdp->rxq->mem.type != MEM_TYPE_XSK_BUFF_POOL)
+#endif
+		{
 			page_ref_sub(page, au->refcnt_bias);
 			au->refcnt_bias = 0;
 		}
@@ -275,22 +408,36 @@ bool mlx5e_xdp_handle(struct mlx5e_rq *r
 			goto xdp_abort;
 		__set_bit(MLX5E_RQ_FLAG_XDP_XMIT, rq->flags);
 		__set_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags);
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		if (xdp->rxq->mem.type != MEM_TYPE_XSK_BUFF_POOL)
+#endif
+#ifdef HAVE_PAGE_DMA_ADDR
 			mlx5e_page_dma_unmap(rq, virt_to_page(xdp->data));
+#else
+			mlx5e_page_dma_unmap(rq, au);
+#endif
 		rq->stats->xdp_redirect++;
 		return true;
+#endif
 	default:
+#ifdef HAVE_BPF_WARN_IVALID_XDP_ACTION_GET_3_PARAMS
 		bpf_warn_invalid_xdp_action(rq->netdev, prog, act);
+#else
+		bpf_warn_invalid_xdp_action(act);
+#endif
 		fallthrough;
 	case XDP_ABORTED:
 xdp_abort:
+#if defined(HAVE_TRACE_XDP_EXCEPTION) && !defined(MLX_DISABLE_TRACEPOINTS)
 		trace_xdp_exception(rq->netdev, prog, act);
 		fallthrough;
+#endif
 	case XDP_DROP:
 		rq->stats->xdp_drop++;
 		return true;
 	}
 }
+#endif
 
 static u16 mlx5e_xdpsq_get_next_pi(struct mlx5e_xdpsq *sq, u16 size)
 {
@@ -463,25 +610,40 @@ mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq
 		     struct skb_shared_info *sinfo, int check_result)
 {
 	struct mlx5_wq_cyc       *wq   = &sq->wq;
+#ifdef HAVE_XDP_HAS_FRAGS
 	struct mlx5_wqe_ctrl_seg *cseg;
 	struct mlx5_wqe_data_seg *dseg;
 	struct mlx5_wqe_eth_seg *eseg;
 	struct mlx5e_tx_wqe *wqe;
+#else
+	u16                       pi   = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+	struct mlx5e_tx_wqe      *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+
+	struct mlx5_wqe_ctrl_seg *cseg = &wqe->ctrl;
+	struct mlx5_wqe_eth_seg  *eseg = &wqe->eth;
+	struct mlx5_wqe_data_seg *dseg = wqe->data;
+#endif
 
 	dma_addr_t dma_addr = xdptxd->dma_addr;
 	u32 dma_len = xdptxd->len;
 	u16 ds_cnt, inline_hdr_sz;
+#ifdef HAVE_XDP_HAS_FRAGS
 	u8 num_wqebbs = 1;
 	int num_frags = 0;
 	u16 pi;
+#endif
 
 	struct mlx5e_xdpsq_stats *stats = sq->stats;
+#ifndef HAVE_XDP_HAS_FRAGS
+	net_prefetchw(wqe);
+#endif
 
 	if (unlikely(dma_len < MLX5E_XDP_MIN_INLINE || sq->hw_mtu < dma_len)) {
 		stats->err++;
 		return false;
 	}
 
+#ifdef HAVE_XDP_HAS_FRAGS
 	ds_cnt = MLX5E_TX_WQE_EMPTY_DS_COUNT + 1;
 	if (sq->min_inline_mode != MLX5_INLINE_MODE_NONE)
 		ds_cnt++;
@@ -502,9 +664,15 @@ mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq
 
 		check_result = mlx5e_xmit_xdp_frame_check_stop_room(sq, stop_room);
 	}
+#else
+	if (!check_result)
+		check_result = mlx5e_xmit_xdp_frame_check(sq);
+#endif
 	if (unlikely(check_result < 0))
 		return false;
 
+
+#ifdef HAVE_XDP_HAS_FRAGS
 	pi = mlx5e_xdpsq_get_next_pi(sq, num_wqebbs);
 	wqe = mlx5_wq_cyc_get_wqe(wq, pi);
 	net_prefetchw(wqe);
@@ -512,7 +680,9 @@ mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq
 	cseg = &wqe->ctrl;
 	eseg = &wqe->eth;
 	dseg = wqe->data;
-
+#else
+	ds_cnt = MLX5E_TX_WQE_EMPTY_DS_COUNT + 1;
+#endif
 	inline_hdr_sz = 0;
 
 	/* copy the inline part if required */
@@ -524,6 +694,9 @@ mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq
 		dma_addr += MLX5E_XDP_MIN_INLINE;
 		inline_hdr_sz = MLX5E_XDP_MIN_INLINE;
 		dseg++;
+#ifndef HAVE_XDP_HAS_FRAGS
+		ds_cnt++;
+#endif
 	}
 
 	if (test_bit(MLX5E_SQ_STATE_TX_XDP_CSUM, &sq->state))
@@ -536,15 +709,25 @@ mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq
 	cseg->opmod_idx_opcode = cpu_to_be32((sq->pc << 8) | MLX5_OPCODE_SEND);
 
 	if (unlikely(test_bit(MLX5E_SQ_STATE_XDP_MULTIBUF, &sq->state))) {
+#ifdef HAVE_XDP_HAS_FRAGS
 		u8 num_pkts = 1 + num_frags;
 		int i;
-
+#else
+		u8 num_pkts = 1;
+		u8 num_wqebbs;
+#endif
+#ifdef struct_group
 		memset(&cseg->trailer, 0, sizeof(cseg->trailer));
+#else
+		memset(&cseg->signature, 0, sizeof(*cseg) -
+				sizeof(cseg->opmod_idx_opcode) - sizeof(cseg->qpn_ds));
+#endif
+
 		memset(eseg, 0, sizeof(*eseg) - sizeof(eseg->trailer));
 
 		eseg->inline_hdr.sz = cpu_to_be16(inline_hdr_sz);
 		dseg->lkey = sq->mkey_be;
-
+#ifdef HAVE_XDP_HAS_FRAGS
 		for (i = 0; i < num_frags; i++) {
 			skb_frag_t *frag = &sinfo->frags[i];
 			dma_addr_t addr;
@@ -557,9 +740,12 @@ mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq
 			dseg->byte_count = cpu_to_be32(skb_frag_size(frag));
 			dseg->lkey = sq->mkey_be;
 		}
+#endif
 
 		cseg->qpn_ds = cpu_to_be32((sq->sqn << 8) | ds_cnt);
-
+#ifndef HAVE_XDP_HAS_FRAGS
+		num_wqebbs = DIV_ROUND_UP(ds_cnt, MLX5_SEND_WQEBB_NUM_DS);
+#endif
 		sq->db.wqe_info[pi] = (struct mlx5e_xdp_wqe_info) {
 			.num_wqebbs = num_wqebbs,
 			.num_pkts = num_pkts,
@@ -580,9 +766,15 @@ mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq
 
 static void mlx5e_free_xdpsq_desc(struct mlx5e_xdpsq *sq,
 				  struct mlx5e_xdp_wqe_info *wi,
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 				  u32 *xsk_frames,
-				  bool recycle,
-				  struct xdp_frame_bulk *bq)
+#endif
+				  bool recycle
+#ifdef HAVE_XDP_FRAME_BULK
+				  , struct xdp_frame_bulk *bq)
+#else
+				  )
+#endif
 {
 	struct mlx5e_xdp_info_fifo *xdpi_fifo = &sq->db.xdpi_fifo;
 	u16 i;
@@ -595,16 +787,25 @@ static void mlx5e_free_xdpsq_desc(struct
 			/* XDP_TX from the XSK RQ and XDP_REDIRECT */
 			dma_unmap_single(sq->pdev, xdpi.frame.dma_addr,
 					 xdpi.frame.xdpf->len, DMA_TO_DEVICE);
+#ifdef HAVE_XDP_FRAME_BULK
 			xdp_return_frame_bulk(xdpi.frame.xdpf, bq);
+#elif defined(HAVE_XDP_FRAME)
+			xdp_return_frame(xdpi.frame.xdpf);
+#else
+			/* Assumes order0 page*/
+			put_page(virt_to_page(xdpi.frame.xdpf->data));
+#endif
 			break;
 		case MLX5E_XDP_XMIT_MODE_PAGE:
 			/* XDP_TX from the regular RQ */
 			mlx5e_page_release_dynamic(xdpi.page.rq, &xdpi.page.au, recycle);
 			break;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 		case MLX5E_XDP_XMIT_MODE_XSK:
 			/* AF_XDP send */
 			(*xsk_frames)++;
 			break;
+#endif
 		default:
 			WARN_ON_ONCE(true);
 		}
@@ -613,14 +814,20 @@ static void mlx5e_free_xdpsq_desc(struct
 
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq)
 {
+#ifdef HAVE_XDP_FRAME_BULK
 	struct xdp_frame_bulk bq;
+#endif
 	struct mlx5e_xdpsq *sq;
 	struct mlx5_cqe64 *cqe;
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	u32 xsk_frames = 0;
+#endif
 	u16 sqcc;
 	int i;
 
+#ifdef HAVE_XDP_FRAME_BULK
 	xdp_frame_bulk_init(&bq);
+#endif
 
 	sq = container_of(cq, struct mlx5e_xdpsq, cq);
 
@@ -653,7 +860,15 @@ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq
 
 			sqcc += wi->num_wqebbs;
 
-			mlx5e_free_xdpsq_desc(sq, wi, &xsk_frames, true, &bq);
+			mlx5e_free_xdpsq_desc(sq, wi
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+					     , &xsk_frames
+#endif
+					     , true
+#ifdef HAVE_XDP_FRAME_BULK
+					     , &bq
+#endif
+					     );
 		} while (!last_wqe);
 
 		if (unlikely(get_cqe_opcode(cqe) != MLX5_CQE_REQ)) {
@@ -666,10 +881,18 @@ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq
 		}
 	} while ((++i < MLX5E_TX_CQ_POLL_BUDGET) && (cqe = mlx5_cqwq_get_cqe(&cq->wq)));
 
+#ifdef HAVE_XDP_FRAME_BULK
 	xdp_flush_frame_bulk(&bq);
+#endif
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	if (xsk_frames)
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 		xsk_tx_completed(sq->xsk_pool, xsk_frames);
+#else
+		xsk_umem_complete_tx(sq->umem, xsk_frames);
+#endif
+#endif
 
 	sq->stats->cqes += i;
 
@@ -684,12 +907,18 @@ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq
 
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq)
 {
+#ifdef HAVE_XDP_FRAME_BULK
 	struct xdp_frame_bulk bq;
+#endif
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	u32 xsk_frames = 0;
+#endif
 
+#ifdef HAVE_XDP_FRAME_BULK
 	xdp_frame_bulk_init(&bq);
 
 	rcu_read_lock(); /* need for xdp_return_frame_bulk */
+#endif
 
 	while (sq->cc != sq->pc) {
 		struct mlx5e_xdp_wqe_info *wi;
@@ -700,16 +929,57 @@ void mlx5e_free_xdpsq_descs(struct mlx5e
 
 		sq->cc += wi->num_wqebbs;
 
-		mlx5e_free_xdpsq_desc(sq, wi, &xsk_frames, false, &bq);
+		mlx5e_free_xdpsq_desc(sq, wi
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+				     , &xsk_frames
+#endif
+				     , false
+#ifdef HAVE_XDP_FRAME_BULK
+				     , &bq
+#endif
+				     );
 	}
 
+#ifdef HAVE_XDP_FRAME_BULK
 	xdp_flush_frame_bulk(&bq);
+
 	rcu_read_unlock();
+#endif
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 	if (xsk_frames)
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 		xsk_tx_completed(sq->xsk_pool, xsk_frames);
+#else
+		xsk_umem_complete_tx(sq->umem, xsk_frames);
+#endif
+#endif
+}
+
+void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq)
+{
+	struct mlx5e_xdpsq *xdpsq = rq->xdpsq;
+
+	if (xdpsq->mpwqe.wqe)
+		mlx5e_xdp_mpwqe_complete(xdpsq);
+
+	mlx5e_xmit_xdp_doorbell(xdpsq);
+	if (test_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags)) {
+		xdp_do_flush_map();
+		__clear_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags);
+	}
+}
+
+void mlx5e_set_xmit_fp(struct mlx5e_xdpsq *sq, bool is_mpw)
+{
+	sq->xmit_xdp_frame_check = is_mpw ?
+		mlx5e_xmit_xdp_frame_check_mpwqe : mlx5e_xmit_xdp_frame_check;
+	sq->xmit_xdp_frame = is_mpw ?
+		mlx5e_xmit_xdp_frame_mpwqe : mlx5e_xmit_xdp_frame;
 }
 
+#ifdef HAVE_NDO_XDP_XMIT
+#ifndef HAVE_NDO_XDP_FLUSH
 int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 		   u32 flags)
 {
@@ -771,25 +1041,90 @@ int mlx5e_xdp_xmit(struct net_device *de
 	return nxmit;
 }
 
-void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq)
+#else
+int mlx5e_xdp_xmit(struct net_device *dev, struct xdp_buff *xdp)
 {
-	struct mlx5e_xdpsq *xdpsq = rq->xdpsq;
+	struct mlx5e_priv *priv = netdev_priv(dev);
+	struct mlx5e_xmit_data xdptxd;
+	struct mlx5e_xdp_info xdpi;
+	struct xdp_frame *xdpf;
+	struct mlx5e_xdpsq *sq;
+	int sq_num;
+	int err = 0;
 
-	if (xdpsq->mpwqe.wqe)
-		mlx5e_xdp_mpwqe_complete(xdpsq);
+	/* this flag is sufficient, no need to test internal sq state */
+	if (unlikely(!mlx5e_xdp_tx_is_enabled(priv)))
+		return -ENETDOWN;
 
-	mlx5e_xmit_xdp_doorbell(xdpsq);
+	sq_num = smp_processor_id();
 
-	if (test_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags)) {
-		xdp_do_flush_map();
-		__clear_bit(MLX5E_RQ_FLAG_XDP_REDIRECT, rq->flags);
+	if (unlikely(sq_num >= priv->channels.num))
+		return -ENXIO;
+
+	sq = &priv->channels.c[sq_num]->xdpsq;
+
+	xdpf = convert_to_xdp_frame(xdp);
+
+	if (unlikely(!xdpf))
+		return -EINVAL;
+
+	xdptxd.data = xdpf->data;
+	xdptxd.len  = xdpf->len;
+
+	xdptxd.dma_addr = dma_map_single(sq->pdev, xdptxd.data,
+					 xdptxd.len, DMA_TO_DEVICE);
+
+	if (unlikely(dma_mapping_error(sq->pdev, xdptxd.dma_addr))) {
+		err = -ENOMEM;
+		goto err_release_page;
 	}
+
+	xdpi.mode = MLX5E_XDP_XMIT_MODE_FRAME;
+	xdpi.frame.xdpf = xdpf;
+	xdpi.frame.dma_addr = xdptxd.dma_addr;
+
+	if (unlikely(!sq->xmit_xdp_frame(sq, &xdptxd, NULL, 0))) {
+		dma_unmap_single(sq->pdev, xdptxd.dma_addr,
+				 xdptxd.len, DMA_TO_DEVICE);
+		err = -ENOSPC;
+		goto err_release_page;
+	}
+
+	return 0;
+
+err_release_page:
+#ifdef HAVE_XDP_FRAME
+	xdp_return_frame_rx_napi(xdpf);
+#else
+	/* Assumes order0 page */
+	put_page(virt_to_page(xdpf->data));
+#endif
+
+	return err;
 }
 
-void mlx5e_set_xmit_fp(struct mlx5e_xdpsq *sq, bool is_mpw)
+void mlx5e_xdp_flush(struct net_device *dev)
 {
-	sq->xmit_xdp_frame_check = is_mpw ?
-		mlx5e_xmit_xdp_frame_check_mpwqe : mlx5e_xmit_xdp_frame_check;
-	sq->xmit_xdp_frame = is_mpw ?
-		mlx5e_xmit_xdp_frame_mpwqe : mlx5e_xmit_xdp_frame;
+	struct mlx5e_priv *priv = netdev_priv(dev);
+	struct mlx5e_xdpsq *sq;
+	int sq_num;
+
+	/* this flag is sufficient, no need to test internal sq state */
+	if (unlikely(!mlx5e_xdp_tx_is_enabled(priv)))
+		return;
+
+	sq_num = smp_processor_id();
+
+	if (unlikely(sq_num >= priv->channels.num))
+		return;
+
+	sq = &priv->channels.c[sq_num]->xdpsq;
+
+	if (sq->mpwqe.wqe)
+		mlx5e_xdp_mpwqe_complete(sq);
+	mlx5e_xmit_xdp_doorbell(sq);
 }
+#endif
+#endif
+#endif
+
