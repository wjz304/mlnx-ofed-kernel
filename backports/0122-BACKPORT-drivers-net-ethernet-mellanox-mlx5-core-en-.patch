From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h

Change-Id: I2c30893cb783fc2d5a5b001afa702be9171770d2
---
 .../ethernet/mellanox/mlx5/core/en/xsk/rx.h   | 41 +++++++++++++++++--
 1 file changed, 38 insertions(+), 3 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xsk/rx.h
@@ -4,11 +4,25 @@
 #ifndef __MLX5_EN_XSK_RX_H__
 #define __MLX5_EN_XSK_RX_H__
 
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
+
 #include "en.h"
+#ifdef HAVE_NDO_XSK_WAKEUP
+#ifdef HAVE_XDP_SOCK_DRV_H
 #include <net/xdp_sock_drv.h>
+#else
+#include <net/xdp_sock.h>
+#endif
+#endif
 
 /* RX data path */
 
+#ifndef HAVE_XSK_BUFF_ALLOC
+bool mlx5e_xsk_pages_enough_umem(struct mlx5e_rq *rq, int count);
+void mlx5e_xsk_page_release(struct mlx5e_rq *rq,
+			    struct mlx5e_dma_info *dma_info);
+void mlx5e_xsk_zca_free(struct zero_copy_allocator *zca, unsigned long handle);
+#endif
 struct sk_buff *mlx5e_xsk_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq,
 						    struct mlx5e_mpw_info *wi,
 						    u16 cqe_bcnt,
@@ -19,10 +33,16 @@ struct sk_buff *mlx5e_xsk_skb_from_cqe_l
 					      struct mlx5e_wqe_frag_info *wi,
 					      u32 cqe_bcnt);
 
+#ifdef HAVE_XSK_BUFF_ALLOC
 static inline int mlx5e_xsk_page_alloc_pool(struct mlx5e_rq *rq,
 					    struct mlx5e_dma_info *dma_info)
 {
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
 	dma_info->xsk = xsk_buff_alloc(rq->xsk_pool);
+#else
+	dma_info->xsk = xsk_buff_alloc(rq->umem);
+#endif
+
 	if (!dma_info->xsk)
 		return -ENOMEM;
 
@@ -35,18 +55,33 @@ static inline int mlx5e_xsk_page_alloc_p
 
 	return 0;
 }
+#else
+int mlx5e_xsk_page_alloc_pool(struct mlx5e_rq *rq,
+			      struct mlx5e_dma_info *dma_info);
+#endif /* HAVE_XSK_BUFF_ALLOC */
 
+#ifdef HAVE_NDO_XSK_WAKEUP
 static inline bool mlx5e_xsk_update_rx_wakeup(struct mlx5e_rq *rq, bool alloc_err)
 {
-	if (!xsk_uses_need_wakeup(rq->xsk_pool))
+#ifdef HAVE_NETDEV_BPF_XSK_BUFF_POOL
+	struct xsk_buff_pool *xsk_pool = rq->xsk_pool;
+
+	if (!xsk_uses_need_wakeup(xsk_pool))
+#else
+	struct xdp_umem *xsk_pool = rq->umem;
+
+	if (!xsk_umem_uses_need_wakeup(xsk_pool))
+#endif
 		return alloc_err;
 
 	if (unlikely(alloc_err))
-		xsk_set_rx_need_wakeup(rq->xsk_pool);
+		xsk_set_rx_need_wakeup(xsk_pool);
 	else
-		xsk_clear_rx_need_wakeup(rq->xsk_pool);
+		xsk_clear_rx_need_wakeup(xsk_pool);
 
 	return false;
 }
+#endif
 
+#endif /* HAVE_XSK_ZERO_COPY_SUPPORT */
 #endif /* __MLX5_EN_XSK_RX_H__ */
