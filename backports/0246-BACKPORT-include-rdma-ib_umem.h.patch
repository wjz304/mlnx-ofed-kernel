From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: include/rdma/ib_umem.h

Change-Id: I696e534e2654b20074d3c7ee0cf76a40dbe054be
---
 include/rdma/ib_umem.h | 40 ++++++++++++++++++++++++++++++++++++++++
 1 file changed, 40 insertions(+)

--- a/include/rdma/ib_umem.h
+++ b/include/rdma/ib_umem.h
@@ -7,6 +7,8 @@
 #ifndef IB_UMEM_H
 #define IB_UMEM_H
 
+#include "../../compat/config.h"
+
 #include <linux/list.h>
 #include <linux/scatterlist.h>
 #include <linux/workqueue.h>
@@ -17,18 +19,31 @@ struct ib_umem_odp;
 struct dma_buf_attach_ops;
 
 struct ib_umem {
+#ifdef HAVE_MMU_NOTIFIER_OPS_HAS_FREE_NOTIFIER
 	struct ib_device       *ibdev;
+#else
+	struct ib_ucontext     *context;
+#endif
 	struct mm_struct       *owning_mm;
 	u64 iova;
 	size_t			length;
 	unsigned long		address;
 	u32 writable : 1;
+#if !defined(HAVE_FOLL_LONGTERM) && !defined(HAVE_GET_USER_PAGES_LONGTERM)
+	u32 hugetlb : 1;
+#endif
 	u32 is_odp : 1;
 	u32 is_dmabuf : 1;
 	/* Placing at the end of the bitfield list is ABI preserving on LE */
 	u32 is_peer : 1;
 	struct work_struct	work;
+#ifdef HAVE_SG_APPEND_TABLE
 	struct sg_append_table sgt_append;
+#else
+	struct sg_table sg_head;
+	int             nmap;
+	unsigned int    sg_nents;
+#endif
 };
 
 struct ib_umem_dmabuf {
@@ -63,8 +78,13 @@ static inline int ib_umem_offset(struct
 static inline unsigned long ib_umem_dma_offset(struct ib_umem *umem,
 					       unsigned long pgsz)
 {
+#ifdef HAVE_SG_APPEND_TABLE
 	return (sg_dma_address(umem->sgt_append.sgt.sgl) + ib_umem_offset(umem)) &
 	       (pgsz - 1);
+#else
+	return (sg_dma_address(umem->sg_head.sgl) + ib_umem_offset(umem)) &
+		               (pgsz - 1);
+#endif
 }
 
 static inline size_t ib_umem_num_dma_blocks(struct ib_umem *umem,
@@ -84,8 +104,12 @@ static inline void __rdma_umem_block_ite
 						struct ib_umem *umem,
 						unsigned long pgsz)
 {
+#ifdef HAVE_SG_APPEND_TABLE
 	__rdma_block_iter_start(biter, umem->sgt_append.sgt.sgl,
 				umem->sgt_append.sgt.nents, pgsz);
+#else
+	__rdma_block_iter_start(biter, umem->sg_head.sgl, umem->nmap, pgsz);
+#endif
 }
 
 /**
@@ -105,7 +129,11 @@ static inline void __rdma_umem_block_ite
 
 #ifdef CONFIG_INFINIBAND_USER_MEM
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 struct ib_umem *ib_umem_get(struct ib_device *device, unsigned long addr,
+#else
+struct ib_umem *ib_umem_get(struct ib_udata *udata, unsigned long addr,
+#endif
 			    size_t size, int access);
 void ib_umem_release(struct ib_umem *umem);
 int ib_umem_copy_from(void *dst, struct ib_umem *umem, size_t offset,
@@ -136,7 +164,11 @@ static inline unsigned long ib_umem_find
 						    unsigned long pgsz_bitmap,
 						    u64 pgoff_bitmask)
 {
+#ifdef HAVE_SG_APPEND_TABLE
 	struct scatterlist *sg = umem->sgt_append.sgt.sgl;
+#else
+	struct scatterlist *sg = umem->sg_head.sgl;
+#endif
 	dma_addr_t dma_addr;
 
 	dma_addr = sg_dma_address(sg) + (umem->address & ~PAGE_MASK);
@@ -155,7 +187,11 @@ struct ib_umem_dmabuf *ib_umem_dmabuf_ge
 int ib_umem_dmabuf_map_pages(struct ib_umem_dmabuf *umem_dmabuf);
 void ib_umem_dmabuf_unmap_pages(struct ib_umem_dmabuf *umem_dmabuf);
 void ib_umem_dmabuf_release(struct ib_umem_dmabuf *umem_dmabuf);
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 struct ib_umem *ib_umem_get_peer(struct ib_device *device, unsigned long addr,
+#else
+struct ib_umem *ib_umem_get_peer(struct ib_udata *udata, unsigned long addr,
+#endif
 				 size_t size, int access,
 				 unsigned long peer_mem_flags);
 void ib_umem_activate_invalidation_notifier(struct ib_umem *umem,
@@ -167,7 +203,11 @@ void ib_umem_stop_invalidation_notifier(
 
 #include <linux/err.h>
 
+#ifdef HAVE_MMU_INTERVAL_NOTIFIER
 static inline struct ib_umem *ib_umem_get(struct ib_device *device,
+#else
+static inline struct ib_umem *ib_umem_get(struct ib_udata *udata,
+#endif
 					  unsigned long addr, size_t size,
 					  int access)
 {
