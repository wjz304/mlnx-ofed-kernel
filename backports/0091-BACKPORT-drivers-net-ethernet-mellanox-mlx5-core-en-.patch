From: Avihai Horon <avihaih@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en/params.c

Change-Id: Ib7844117a626f0b92cb7d0513ea929df9b482450
---
 .../ethernet/mellanox/mlx5/core/en/params.c   | 55 ++++++++++++++++---
 1 file changed, 48 insertions(+), 7 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/params.c
@@ -7,26 +7,40 @@
 #include "en_accel/en_accel.h"
 #include "accel/ipsec.h"
 #include "fpga/ipsec.h"
+#include "en_accel/tls.h"
 
+#ifdef HAVE_XDP_SUPPORT
 static bool mlx5e_rx_is_xdp(struct mlx5e_params *params,
 			    struct mlx5e_xsk_param *xsk)
 {
 	return params->xdp_prog || xsk;
 }
+#endif
 
 u16 mlx5e_get_linear_rq_headroom(struct mlx5e_params *params,
 				 struct mlx5e_xsk_param *xsk)
 {
-	u16 headroom;
+	u16 headroom = 0;
 
+#ifdef HAVE_XSK_BUFF_ALLOC
 	if (xsk)
 		return xsk->headroom;
-
+#endif
 	headroom = NET_IP_ALIGN;
-	if (mlx5e_rx_is_xdp(params, xsk))
+
+#ifdef HAVE_XDP_SUPPORT
+	if (mlx5e_rx_is_xdp(params, xsk)) {
 		headroom += XDP_PACKET_HEADROOM;
-	else
+#ifndef HAVE_XSK_BUFF_ALLOC
+		if (xsk)
+			headroom += xsk->headroom;
+#endif
+	} else {
+#endif /* HAVE_XDP_SUPPORT */
 		headroom += MLX5_RX_HEADROOM;
+#ifdef HAVE_XDP_SUPPORT
+	}
+#endif
 
 	return headroom;
 }
@@ -58,8 +72,10 @@ static u32 mlx5e_rx_get_linear_frag_sz(s
 	 * The latter is important, because frames may come in a random order,
 	 * and we will have trouble assemblying a real page of multiple frames.
 	 */
+#ifdef HAVE_XDP_SUPPORT
 	if (mlx5e_rx_is_xdp(params, xsk))
 		frag_sz = max_t(u32, frag_sz, PAGE_SIZE);
+#endif
 
 	/* Even if we can go with a smaller fragment size, we must not put
 	 * multiple packets into a single frame.
@@ -87,8 +103,12 @@ bool mlx5e_rx_is_linear_skb(struct mlx5e
 	u32 linear_frag_sz = max(mlx5e_rx_get_linear_frag_sz(params, xsk),
 				 mlx5e_rx_get_linear_frag_sz(params, NULL));
 
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	return !IS_HW_LRO(params) && linear_frag_sz <= PAGE_SIZE;
+#else
 	return params->packet_merge.type == MLX5E_PACKET_MERGE_NONE &&
 		linear_frag_sz <= PAGE_SIZE;
+#endif
 }
 
 static bool mlx5e_verify_rx_mpwqe_strides(struct mlx5_core_dev *mdev,
@@ -320,12 +340,14 @@ int mlx5e_mpwrq_validate_regular(struct
 	if (mlx5_fpga_is_ipsec_device(mdev))
 		return -EOPNOTSUPP;
 
+#ifdef HAVE_XDP_SUPPORT
 	if (params->xdp_prog && !mlx5e_rx_mpwqe_is_linear_skb(mdev, params, NULL))
 		return -EINVAL;
+#endif
 
 	return 0;
 }
-
+#ifdef HAVE_XSK_ZERO_COPY_SUPPORT
 int mlx5e_mpwrq_validate_xsk(struct mlx5_core_dev *mdev, struct mlx5e_params *params,
 			     struct mlx5e_xsk_param *xsk)
 {
@@ -340,6 +362,7 @@ int mlx5e_mpwrq_validate_xsk(struct mlx5
 
 	return 0;
 }
+#endif
 
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params)
@@ -392,6 +415,9 @@ void mlx5e_build_create_cq_param(struct
 		.ch_stats = c->stats,
 		.node = cpu_to_node(c->cpu),
 		.ix = c->ix,
+#ifndef HAVE_NAPI_STATE_MISSED
+		.ch_flags = &c->flags,
+#endif
 	};
 }
 
@@ -537,10 +563,13 @@ void mlx5e_build_aso_cq_param(struct mlx
 
 static u8 rq_end_pad_mode(struct mlx5_core_dev *mdev, struct mlx5e_params *params)
 {
-	bool lro_en = params->packet_merge.type == MLX5E_PACKET_MERGE_LRO;
 	bool ro = MLX5_CAP_GEN(mdev, relaxed_ordering_write);
 
-	return ro && lro_en ?
+#ifdef CONFIG_COMPAT_LRO_ENABLED_IPOIB
+	return ro && IS_HW_LRO(params)?
+#else
+	return ro && (params->packet_merge.type == MLX5E_PACKET_MERGE_LRO) ?
+#endif
 		MLX5_WQ_END_PAD_MODE_NONE : MLX5_WQ_END_PAD_MODE_ALIGN;
 }
 
@@ -636,6 +665,8 @@ void mlx5e_build_tx_cq_param(struct mlx5
 	void *cqc = param->cqc;
 
 	MLX5_SET(cqc, cqc, log_cq_size, params->log_sq_size);
+	if (MLX5E_GET_PFLAG(params, MLX5E_PFLAG_TX_CQE_COMPRESS))
+		MLX5_SET(cqc, cqc, cqe_comp_en, 1);
 
 	mlx5e_build_common_cq_param(mdev, param);
 	param->cq_period_mode = params->tx_cq_moderation.cq_period_mode;
@@ -771,8 +802,10 @@ static u8 mlx5e_build_icosq_log_wq_sz(st
 	 * doesn't affect its return value, as long as params->xdp_prog != NULL,
 	 * so we can just multiply by 2.
 	 */
+#ifdef HAVE_XDP_SUPPORT
 	if (params->xdp_prog)
 		wqebbs *= 2;
+#endif
 
 	if (params->packet_merge.type == MLX5E_PACKET_MERGE_SHAMPO)
 		wqebbs += mlx5e_shampo_icosq_sz(mdev, params, rqp);
@@ -782,8 +815,10 @@ static u8 mlx5e_build_icosq_log_wq_sz(st
 
 static u8 mlx5e_build_async_icosq_log_wq_sz(struct mlx5_core_dev *mdev)
 {
+#ifdef HAVE_UAPI_LINUX_TLS_H
 	if (mlx5e_accel_is_ktls_rx(mdev))
 		return MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE;
+#endif
 
 	return MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE;
 }
@@ -811,14 +846,17 @@ static void mlx5e_build_async_icosq_para
 
 	mlx5e_build_sq_param_common(mdev, param);
 	param->stop_room = mlx5e_stop_room_for_wqe(mdev, 1); /* for XSK NOP */
+#ifdef HAVE_UAPI_LINUX_TLS_H
 	param->is_tls = mlx5e_accel_is_ktls_rx(mdev);
 	if (param->is_tls)
 		param->stop_room += mlx5e_stop_room_for_wqe(mdev, 1); /* for TLS RX resync NOP */
+#endif
 	MLX5_SET(sqc, sqc, reg_umr, MLX5_CAP_ETH(mdev, reg_umr_sq));
 	MLX5_SET(wq, wq, log_wq_sz, log_wq_size);
 	mlx5e_build_ico_cq_param(mdev, log_wq_size, &param->cqp);
 }
 
+#ifdef HAVE_XDP_SUPPORT
 void mlx5e_build_xdpsq_param(struct mlx5_core_dev *mdev,
 			     struct mlx5e_params *params,
 			     struct mlx5e_sq_param *param)
@@ -831,6 +869,7 @@ void mlx5e_build_xdpsq_param(struct mlx5
 	param->is_mpw = MLX5E_GET_PFLAG(params, MLX5E_PFLAG_XDP_TX_MPWQE);
 	mlx5e_build_tx_cq_param(mdev, params, &param->cqp);
 }
+#endif
 
 int mlx5e_build_channel_param(struct mlx5_core_dev *mdev,
 			      struct mlx5e_params *params,
@@ -848,7 +887,9 @@ int mlx5e_build_channel_param(struct mlx
 	async_icosq_log_wq_sz = mlx5e_build_async_icosq_log_wq_sz(mdev);
 
 	mlx5e_build_sq_param(mdev, params, &cparam->txq_sq);
+#ifdef HAVE_XDP_SUPPORT
 	mlx5e_build_xdpsq_param(mdev, params, &cparam->xdp_sq);
+#endif
 	mlx5e_build_icosq_param(mdev, icosq_log_wq_sz, &cparam->icosq);
 	mlx5e_build_async_icosq_param(mdev, async_icosq_log_wq_sz, &cparam->async_icosq);
 
