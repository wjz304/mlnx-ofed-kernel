From: Valentine Fatiev <valentinef@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h

Change-Id: I5a5e24f1b4f6df61a853a921c3e0c54a62134773
---
 drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h | 16 +++++++++++++++-
 1 file changed, 15 insertions(+), 1 deletion(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -46,10 +46,13 @@
 
 struct mlx5e_xdp_buff {
 	struct xdp_buff xdp;
+#ifdef HAVE_XDP_METADATA_OPS
 	struct mlx5_cqe64 *cqe;
 	struct mlx5e_rq *rq;
+#endif
 };
 
+#ifdef HAVE_XDP_SUPPORT
 /* XDP packets can be transmitted in different ways. On completion, we need to
  * distinguish between them to clean up things in a proper way.
  */
@@ -95,14 +98,25 @@ union mlx5e_xdp_info {
 	union {
 		struct mlx5e_rq *rq;
 		u8 num;
+#ifdef HAVE_PAGE_POOL_DEFRAG_PAGE
 		struct page *page;
+#else
+		struct mlx5e_alloc_unit au;
+#endif
 	} page;
 };
 
 struct mlx5e_xsk_param;
 int mlx5e_xdp_max_mtu(struct mlx5e_params *params, struct mlx5e_xsk_param *xsk);
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq,
+#ifndef HAVE_PAGE_POOL_DEFRAG_PAGE
+		      struct mlx5e_alloc_unit *au,
+#endif
 		      struct bpf_prog *prog, struct mlx5e_xdp_buff *mlctx);
+#ifndef HAVE_XSK_BUFF_ALLOC
+bool mlx5e_xdp_handle_old(struct mlx5e_rq *rq, struct mlx5e_alloc_unit *au,
+		      struct bpf_prog *prog, struct xdp_buff *xdp);
+#endif
 void mlx5e_xdp_mpwqe_complete(struct mlx5e_xdpsq *sq);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
@@ -157,7 +171,6 @@ static inline void mlx5e_xmit_xdp_doorbe
 		sq->doorbell_cseg = NULL;
 	}
 }
-
 /* Enable inline WQEs to shift some load from a congested HCA (HW) to
  * a less congested cpu (SW).
  */
@@ -239,3 +252,4 @@ mlx5e_xdpi_fifo_pop(struct mlx5e_xdp_inf
 	return fifo->xi[(*fifo->cc)++ & fifo->mask];
 }
 #endif
+#endif
