From: Shay Drory <shayd@nvidia.com>
Subject: [PATCH] BACKPORT: drivers/net/ethernet/mellanox/mlx5/core/en_tc.c

Change-Id: I304040eb4dfbffd0705103187690459972ea3886
---
 .../net/ethernet/mellanox/mlx5/core/en_tc.c   | 511 +++++++++++++++++-
 1 file changed, 486 insertions(+), 25 deletions(-)

--- a/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_tc.c
@@ -39,6 +39,10 @@
 #include <linux/rhashtable.h>
 #include <linux/refcount.h>
 #include <linux/completion.h>
+#ifdef HAVE_BASECODE_EXTRAS
+#include <linux/seq_file.h>
+#include <linux/tc_act/tc_pedit.h>
+#endif
 #include <net/arp.h>
 #include <net/ipv6_stubs.h>
 #include <net/bareudp.h>
@@ -46,6 +50,7 @@
 #include <net/dst_metadata.h>
 #include "devlink.h"
 #include "en.h"
+#include "en/devlink.h"
 #include "en/tc/post_act.h"
 #include "en/tc/act_stats.h"
 #include "en_rep.h"
@@ -72,6 +77,14 @@
 #include "lag/mp.h"
 #include "esw/vf_meter.h"
 #include "en/fs.h"
+#include "compat.h"
+
+#include <net/netlink.h>
+#ifndef HAVE_TC_CLS_OFFLOAD_EXTACK
+/* To avoid declaring extack unused local variables in functions,
+ * use a global one. */
+static struct netlink_ext_ack *extack = NULL;
+#endif
 
 #define MLX5E_TC_TABLE_NUM_GROUPS 4
 #define MLX5E_TC_TABLE_MAX_GROUP_SIZE BIT(18)
@@ -1530,8 +1543,14 @@ static int debugfs_hairpin_num_active_ge
 
 	return 0;
 }
+
+#ifdef HAVE_DEBUGFS_CREATE_FILE_UNSAFE
 DEFINE_DEBUGFS_ATTRIBUTE(fops_hairpin_num_active,
 			 debugfs_hairpin_num_active_get, NULL, "%llu\n");
+#else
+DEFINE_SIMPLE_ATTRIBUTE(fops_hairpin_num_active,
+			 debugfs_hairpin_num_active_get, NULL, "%llu\n");
+#endif
 
 static int debugfs_hairpin_table_dump_show(struct seq_file *file, void *priv)
 
@@ -1591,9 +1610,14 @@ static struct mlx5e_hairpin_entry *
 mlx5e_get_hairpin_entry(struct mlx5e_priv *priv, struct mlx5_core_dev *peer_mdev,
 			u16 match_prio)
 {
+#if defined(HAVE_DEVLINK_PARAM_REGISTER) || defined(HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET)
 	struct devlink *devlink = priv_to_devlink(priv->mdev);
-	struct mlx5e_tc_table *tc = mlx5e_fs_get_tc(priv->fs);
 	union devlink_param_value val = {};
+#else
+	u32 link_speed = 0;
+	u64 link_speed64;
+#endif
+	struct mlx5e_tc_table *tc = mlx5e_fs_get_tc(priv->fs);
 	struct mlx5_hairpin_params params;
 	struct mlx5e_hairpin_entry *hpe;
 	struct mlx5e_hairpin *hp;
@@ -1621,7 +1645,12 @@ mlx5e_get_hairpin_entry(struct mlx5e_pri
 	refcount_set(&hpe->refcnt, 1);
 	init_completion(&hpe->res_ready);
 
+#if defined(HAVE_DEVLINK_PARAM_REGISTER) || defined(HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET)
+#ifdef HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET
 	if (devl_param_driverinit_value_get(
+#else
+        if (devlink_param_driverinit_value_get(
+#endif
 		devlink, MLX5_DEVLINK_PARAM_ID_HAIRPIN_QUEUE_SIZE, &val)) {
 		kfree(hpe);
 		hpe = ERR_PTR(-ENOMEM);
@@ -1629,26 +1658,54 @@ mlx5e_get_hairpin_entry(struct mlx5e_pri
 	}
 
 	params.log_num_packets = ilog2(val.vu32);
+#endif
 	if (!tc->num_prio_hp) {
+#if defined(HAVE_DEVLINK_PARAM_REGISTER) || defined(HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET)
 		params.log_data_size =
 			clamp_t(u32,
 			params.log_num_packets + MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(priv->mdev),
 			MLX5_CAP_GEN(priv->mdev, log_min_hairpin_wq_data_sz),
 			MLX5_CAP_GEN(priv->mdev, log_max_hairpin_wq_data_sz));
 
-		if (devl_param_driverinit_value_get(
-			devlink, MLX5_DEVLINK_PARAM_ID_HAIRPIN_NUM_QUEUES, &val)) {
+#else
+		params.log_data_size = 16;
+		params.log_data_size = min_t(u8, params.log_data_size,
+					     MLX5_CAP_GEN(priv->mdev, log_max_hairpin_wq_data_sz));
+		params.log_data_size = max_t(u8, params.log_data_size,
+					     MLX5_CAP_GEN(priv->mdev, log_min_hairpin_wq_data_sz));
+#endif
+#if defined(HAVE_DEVLINK_PARAM_REGISTER) || defined(HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET)
+#ifdef HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET
+	if (devl_param_driverinit_value_get(devlink,
+#else
+        if (devlink_param_driverinit_value_get(devlink,
+#endif
+						MLX5_DEVLINK_PARAM_ID_HAIRPIN_NUM_QUEUES, &val)) {
 			kfree(hpe);
 			hpe = ERR_PTR(-ENOMEM);
 			goto complete;
 		}
 
 		params.num_channels = val.vu32;
+#else
+		mlx5_port_max_linkspeed(priv->mdev, &link_speed);
+		link_speed = max_t(u32, link_speed, 50000);
+		link_speed64 = link_speed;
+		do_div(link_speed64, 50000);
+		params.num_channels = link_speed64;
+
+#endif
 	} else { /* prio hp uses max buffer with 1 channel */
 		params.log_data_size = MLX5_CAP_GEN(priv->mdev, log_max_hairpin_wq_data_sz);
 		params.num_channels = 1;
 	}
 
+#if !defined(HAVE_DEVLINK_PARAM_REGISTER) && !defined(HAVE_DEVL_PARAM_DRIVERINIT_VALUE_GET)
+		params.log_num_packets = params.log_data_size -
+				 MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(priv->mdev);
+		params.log_num_packets = min_t(u8, params.log_num_packets,
+				       MLX5_CAP_GEN(priv->mdev, log_max_hairpin_num_packets));
+#endif
 	params.q_counter = priv->q_counter;
 
 	hp = mlx5e_hairpin_create(priv, &params, peer_mdev);
@@ -2982,7 +3039,7 @@ static bool flow_requires_tunnel_mapping
 		return false;
 
 	flow_action_for_each(i, act, flow_action) {
-		switch (act->id) {
+		switch ((int) act->id) {
 		case FLOW_ACTION_GOTO:
 			return true;
 		case FLOW_ACTION_SAMPLE:
@@ -3047,7 +3104,9 @@ static int mlx5e_get_flow_tunnel_id(stru
 				    struct net_device *filter_dev)
 {
 	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5e_tc_mod_hdr_acts *mod_hdr_acts;
 	struct flow_match_enc_opts enc_opts_match;
 	struct tunnel_match_enc_opts tun_enc_opts;
@@ -3066,6 +3125,7 @@ static int mlx5e_get_flow_tunnel_id(stru
 	uplink_priv = &uplink_rpriv->uplink_priv;
 
 	memset(&tunnel_key, 0, sizeof(tunnel_key));
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_CONTROL
 	COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_CONTROL,
 		       &tunnel_key.enc_control);
 	if (tunnel_key.enc_control.addr_type == FLOW_DISSECTOR_KEY_IPV4_ADDRS)
@@ -3079,6 +3139,7 @@ static int mlx5e_get_flow_tunnel_id(stru
 		       &tunnel_key.enc_tp);
 	COPY_DISSECTOR(rule, FLOW_DISSECTOR_KEY_ENC_KEYID,
 		       &tunnel_key.enc_key_id);
+#endif
 	tunnel_key.filter_ifindex = filter_dev->ifindex;
 
 	err = mapping_add(uplink_priv->tunnel_mapping, &tunnel_key, &tun_id);
@@ -3234,7 +3295,9 @@ static int mlx5e_tc_verify_tunnel_ecn(st
 {
 	u8 outer_ecn_mask = 0, outer_ecn_key = 0, inner_ecn_mask = 0, inner_ecn_key = 0;
 	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct flow_match_ip match;
 
 	*match_inner_ecn = true;
@@ -3309,6 +3372,7 @@ static int mlx5e_tc_verify_tunnel_ecn(st
 	return 0;
 }
 
+#ifdef HAVE_TCF_TUNNEL_INFO
 static int parse_tunnel_attr(struct mlx5e_priv *priv,
 			     struct mlx5e_tc_flow *flow,
 			     struct mlx5_flow_spec *spec,
@@ -3319,7 +3383,9 @@ static int parse_tunnel_attr(struct mlx5
 {
 	struct mlx5e_tc_tunnel *tunnel = mlx5e_get_tc_tun(filter_dev);
 	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	bool needs_mapping, sets_mapping;
 	int err;
 
@@ -3355,7 +3421,9 @@ static int parse_tunnel_attr(struct mlx5
 		/* With mpls over udp we decapsulate using packet reformat
 		 * object
 		 */
+#ifdef HAVE_NET_BAREUDP_H
 		if (!netif_is_bareudp(filter_dev))
+#endif
 			flow->attr->action |= MLX5_FLOW_CONTEXT_ACTION_DECAP;
 		err = mlx5e_tc_set_attr_rx_tun(flow, spec);
 		if (err)
@@ -3389,6 +3457,7 @@ static int parse_tunnel_attr(struct mlx5
 
 	return mlx5e_get_flow_tunnel_id(priv, flow, f, filter_dev);
 }
+#endif /* HAVE_TCF_TUNNEL_INFO */
 
 static void *get_match_inner_headers_criteria(struct mlx5_flow_spec *spec)
 {
@@ -3432,7 +3501,9 @@ static int mlx5e_flower_parse_meta(struc
 				   struct flow_cls_offload *f)
 {
 	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct net_device *ingress_dev;
 	struct flow_match_meta match;
 
@@ -3473,9 +3544,10 @@ static bool skip_key_basic(struct net_de
 	 * label fields.  However, the actual ethertype is IP so we want to
 	 * avoid matching on this, otherwise we'll fail the match.
 	 */
+#ifdef HAVE_NET_BAREUDP_H
 	if (netif_is_bareudp(filter_dev) && f->common.chain_index == 0)
 		return true;
-
+#endif
 	return false;
 }
 
@@ -3486,7 +3558,9 @@ static int __parse_cls_flower(struct mlx
 			      struct net_device *filter_dev,
 			      u8 *inner_match_level, u8 *outer_match_level)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	void *headers_c = MLX5_ADDR_OF(fte_match_param, spec->match_criteria,
 				       outer_headers);
 	void *headers_v = MLX5_ADDR_OF(fte_match_param, spec->match_value,
@@ -3503,6 +3577,7 @@ static int __parse_cls_flower(struct mlx
 	struct flow_dissector *dissector = rule->match.dissector;
 	enum fs_flow_table_type fs_type;
 	bool match_inner_ecn = true;
+	u64 used_keys_u64 = dissector->used_keys;
 	u16 addr_type = 0;
 	u8 ip_proto = 0;
 	u8 *match_level;
@@ -3511,7 +3586,7 @@ static int __parse_cls_flower(struct mlx
 	fs_type = mlx5e_is_eswitch_flow(flow) ? FS_FT_FDB : FS_FT_NIC_RX;
 	match_level = outer_match_level;
 
-	if (dissector->used_keys &
+	if (used_keys_u64 &
 	    ~(BIT_ULL(FLOW_DISSECTOR_KEY_META) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_CONTROL) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_BASIC) |
@@ -3521,25 +3596,42 @@ static int __parse_cls_flower(struct mlx
 	      BIT_ULL(FLOW_DISSECTOR_KEY_IPV4_ADDRS) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_IPV6_ADDRS) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_PORTS) |
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_CONTROL
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ENC_KEYID) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ENC_PORTS)	|
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ENC_CONTROL) |
+#endif
 	      BIT_ULL(FLOW_DISSECTOR_KEY_TCP) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_IP)  |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_CT) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ENC_IP) |
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ENC_OPTS) |
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_CONTROL
 	      BIT_ULL(FLOW_DISSECTOR_KEY_ICMP) |
+#endif
 	      BIT_ULL(FLOW_DISSECTOR_KEY_MPLS))) {
 		NL_SET_ERR_MSG_MOD(extack, "Unsupported key");
 		netdev_dbg(priv->netdev, "Unsupported key used: 0x%llx\n",
-			   dissector->used_keys);
+			   used_keys_u64);
 		return -EOPNOTSUPP;
 	}
 
-	if (mlx5e_get_tc_tun(filter_dev)) {
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_CONTROL
+#if !defined(HAVE_TC_INDR_API) && !defined(CONFIG_COMPAT_KERNEL_4_14)
+	/* for old kernels we dont have real filter_dev,
+	 * and mlx5e_get_tc_tun always return vxlan
+	 */
+	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_IPV4_ADDRS) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_IPV6_ADDRS) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_KEYID) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_PORTS) ||
+	    flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ENC_OPTS))
+#else
+	if (mlx5e_get_tc_tun(filter_dev))
+#endif
+	{
 		bool match_inner = false;
 
 		err = parse_tunnel_attr(priv, flow, spec, f, filter_dev,
@@ -3561,6 +3653,7 @@ static int __parse_cls_flower(struct mlx
 		if (err)
 			return err;
 	}
+#endif
 
 	err = mlx5e_flower_parse_meta(filter_dev, f);
 	if (err)
@@ -3577,6 +3670,13 @@ static int __parse_cls_flower(struct mlx
 
 		if (match.mask->n_proto)
 			*match_level = MLX5_MATCH_L2;
+
+#ifndef HAVE_FLOW_DISSECTOR_KEY_CVLAN
+		if (match.key->n_proto == htons(ETH_P_8021AD)) {
+			NL_SET_ERR_MSG_MOD(extack, "Matching on CVLAN is not supported");
+			return -EOPNOTSUPP;
+		}
+#endif
 	}
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_VLAN) ||
 	    is_vlan_dev(filter_dev)) {
@@ -3622,6 +3722,7 @@ static int __parse_cls_flower(struct mlx
 
 			*match_level = MLX5_MATCH_L2;
 
+#ifdef HAVE_FLOW_DISSECTOR_KEY_VLAN_ETH_TYPE
 			if (!flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_CVLAN) &&
 			    match.mask->vlan_eth_type &&
 			    MLX5_CAP_FLOWTABLE_TYPE(priv->mdev,
@@ -3632,6 +3733,7 @@ static int __parse_cls_flower(struct mlx
 				spec->match_criteria_enable |=
 					MLX5_MATCH_MISC_PARAMETERS;
 			}
+#endif
 		}
 	} else if (*match_level != MLX5_MATCH_NONE) {
 		/* cvlan_tag enabled in match criteria and
@@ -3877,6 +3979,7 @@ static int __parse_cls_flower(struct mlx
 		if (match.mask->flags)
 			*match_level = MLX5_MATCH_L4;
 	}
+#ifdef HAVE_FLOW_DISSECTOR_KEY_ENC_CONTROL
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_ICMP)) {
 		struct flow_match_icmp match;
 
@@ -3926,6 +4029,8 @@ static int __parse_cls_flower(struct mlx
 			spec->match_criteria_enable |= MLX5_MATCH_MISC_PARAMETERS_3;
 		}
 	}
+#endif
+#ifdef HAVE_NET_BAREUDP_H
 	/* Currently supported only for MPLS over UDP */
 	if (flow_rule_match_key(rule, FLOW_DISSECTOR_KEY_MPLS) &&
 	    !netif_is_bareudp(filter_dev)) {
@@ -3935,6 +4040,7 @@ static int __parse_cls_flower(struct mlx
 			   "Matching on MPLS is supported only for MPLS over UDP\n");
 		return -EOPNOTSUPP;
 	}
+#endif
 
 	return 0;
 }
@@ -3946,7 +4052,9 @@ static int parse_cls_flower(struct mlx5e
 			    struct net_device *filter_dev)
 {
 	u8 inner_match_level, outer_match_level, non_tunnel_match_level;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5_core_dev *dev = priv->mdev;
 	struct mlx5_eswitch *esw = dev->priv.eswitch;
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
@@ -4010,6 +4118,7 @@ struct mlx5_fields {
 								 matchmaskx)); \
 })
 
+#if defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX) || defined(HAVE_TCF_PEDIT_PARMS_TCFP_KEYS_EX)
 static bool cmp_val_mask(void *valp, void *maskp, void *matchvalp,
 			 void *matchmaskp, u8 bsize)
 {
@@ -4241,12 +4350,14 @@ static int verify_offload_pedit_fields(s
 
 	return 0;
 }
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX || HAVE_TCF_PEDIT_PARMS_TCFP_KEYS_EX */
 
 static int alloc_tc_pedit_action(struct mlx5e_priv *priv, int namespace,
 				 struct mlx5e_tc_flow_parse_attr *parse_attr,
 				 u32 *action_flags,
 				 struct netlink_ext_ack *extack)
 {
+#if defined(HAVE_TCF_PEDIT_TCFP_KEYS_EX) || defined(HAVE_TCF_PEDIT_PARMS_TCFP_KEYS_EX)
 	int err;
 
 	err = offload_pedit_fields(priv, namespace, parse_attr, action_flags, extack);
@@ -4262,6 +4373,9 @@ static int alloc_tc_pedit_action(struct
 out_dealloc_parsed_actions:
 	mlx5e_mod_hdr_dealloc(&parse_attr->mod_hdr_acts);
 	return err;
+#else /* HAVE_TCF_PEDIT_TCFP_KEYS_EX || HAVE_TCF_PEDIT_PARMS_TCFP_KEYS_EX */
+	return -EOPNOTSUPP;
+#endif /* HAVE_TCF_PEDIT_TCFP_KEYS_EX || HAVE_TCF_PEDIT_PARMS_TCFP_KEYS_EX */
 }
 
 struct ip_ttl_word {
@@ -4678,7 +4792,7 @@ alloc_branch_attr(struct mlx5e_tc_flow *
 
 	attr = *cond_attr;
 
-	switch (cond->act_id) {
+	switch ((int) cond->act_id) {
 	case FLOW_ACTION_DROP:
 		attr->action |= MLX5_FLOW_CONTEXT_ACTION_DROP;
 		break;
@@ -4714,13 +4828,14 @@ dec_jump_count(struct flow_action_entry
 	       struct mlx5_flow_attr *attr, struct mlx5e_priv *priv,
 	       struct mlx5e_tc_jump_state *jump_state)
 {
+#ifdef HAVE_FLOW_ACTION_ENTRY_HW_INDEX
 	if (!jump_state->jump_count)
 		return;
 
 	/* Single tc action can instantiate multiple offload actions (e.g. pedit)
 	 * Jump only over a tc action
 	 */
-	if (act->id == jump_state->last_id && act->hw_index == jump_state->last_index)
+	if (act->id == (int) jump_state->last_id && act->hw_index == jump_state->last_index)
 		return;
 
 	jump_state->last_id = act->id;
@@ -4751,6 +4866,7 @@ dec_jump_count(struct flow_action_entry
 		 */
 		attr->jumping_attr = jump_state->jumping_attr;
 	}
+#endif /* HAVE_FLOW_ACTION_ENTRY_HW_INDEX */
 }
 
 static int
@@ -4788,7 +4904,9 @@ parse_branch_ctrl(struct flow_action_ent
 
 	/* branching action requires its own counter */
 	attr->action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
+#ifdef HAVE_USE_ACT_STATS
 	flow_flag_set(flow, USE_ACT_STATS);
+#endif
 
 	return 0;
 
@@ -4872,11 +4990,25 @@ parse_tc_actions(struct mlx5e_tc_act_par
 		if (is_missable) {
 			/* Add counter to prev, and assign act to new (next) attr */
 			prev_attr->action |= MLX5_FLOW_CONTEXT_ACTION_COUNT;
+#ifdef HAVE_USE_ACT_STATS
 			flow_flag_set(flow, USE_ACT_STATS);
+#endif
 
-			attr->tc_act_cookies[attr->tc_act_cookies_count++] = act->cookie;
+#if defined(HAVE_FLOW_ACTION_ENTRY_COOKIE)
+			attr->tc_act_cookies[attr->tc_act_cookies_count++] = (unsigned long) act->cookie;
+#elif defined(HAVE_FLOW_ACTION_ENTRY_ACT_COOKIE)
+			attr->tc_act_cookies[attr->tc_act_cookies_count++] = (unsigned long) act->act_cookie;
+#elif defined(HAVE_FLOW_ACTION_ENTRY_ACT_POINTER)
+			attr->tc_act_cookies[attr->tc_act_cookies_count++] = (unsigned long) act->act;
+#endif
 		} else if (!tc_act->stats_action) {
-			prev_attr->tc_act_cookies[attr->tc_act_cookies_count++] = act->cookie;
+#if defined(HAVE_FLOW_ACTION_ENTRY_COOKIE)
+			prev_attr->tc_act_cookies[prev_attr->tc_act_cookies_count++] = (unsigned long) act->cookie;
+#elif defined(HAVE_FLOW_ACTION_ENTRY_ACT_COOKIE)
+			prev_attr->tc_act_cookies[prev_attr->tc_act_cookies_count++] = (unsigned long) act->act_cookie;
+#elif defined(HAVE_FLOW_ACTION_ENTRY_ACT_POINTER)
+			prev_attr->tc_act_cookies[prev_attr->tc_act_cookies_count++] = (unsigned long) act->act;
+#endif
 		}
 	}
 
@@ -4896,6 +5028,28 @@ out_free_post_acts:
 	return err;
 }
 
+/* BACKPORT: Hooks parse_tc_actions,
+ * Must follow original definition and before any call
+ * Reorders ct action if miss to action isn't supported, and verifies CT + other
+ * acts support.
+ */
+#define parse_tc_actions(s, a) ({ \
+	struct flow_action *__flow_action_reorder = NULL; \
+	struct mlx5e_tc_act_parse_state *__parse_state = (s); \
+	struct flow_action *__flow_action = (a); \
+	int __ret = 0; \
+\
+	if (!__ret && !mlx5e_tc_act_verify_actions(__flow_action)) \
+		__ret = -EOPNOTSUPP; \
+	if (!__ret && !mlx5e_tc_act_reorder_flow_actions(&__flow_action_reorder, &__flow_action)) \
+		__ret = -ENOMEM; \
+	if (!__ret) \
+		__ret = parse_tc_actions(__parse_state, __flow_action); \
+	kfree(__flow_action_reorder); \
+\
+	__ret; \
+})
+
 static int
 flow_action_supported(struct flow_action *flow_action,
 		      struct netlink_ext_ack *extack)
@@ -4905,11 +5059,13 @@ flow_action_supported(struct flow_action
 		return -EINVAL;
 	}
 
+#ifdef HAVE_FLOW_ACTION_HW_STATS_CHECK
 	if (!flow_action_hw_stats_check(flow_action, extack,
 					FLOW_ACTION_HW_STATS_DELAYED_BIT)) {
 		NL_SET_ERR_MSG_MOD(extack, "Flow action HW stats type is not supported");
 		return -EOPNOTSUPP;
 	}
+#endif
 
 	return 0;
 }
@@ -5129,14 +5285,48 @@ static const struct rhashtable_params tc
 	.automatic_shrinking = true,
 };
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+static void get_new_flags(struct mlx5e_priv *priv, unsigned long *flags)
+{
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+
+	if (mlx5e_eswitch_rep(priv->netdev) &&
+	    MLX5_VPORT_MANAGER(priv->mdev) && esw->mode == MLX5_ESWITCH_OFFLOADS)
+		*flags |= MLX5_TC_FLAG(ESW_OFFLOAD);
+}
+#elif !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+static void get_new_flags(struct mlx5e_priv *priv, unsigned long *flags)
+{
+	struct mlx5_eswitch *esw = priv->mdev->priv.eswitch;
+
+	if (esw && esw->mode == MLX5_ESWITCH_OFFLOADS)
+		*flags |= MLX5_TC_FLAG(ESW_OFFLOAD);
+}
+#endif
+
 static struct rhashtable *get_tc_ht(struct mlx5e_priv *priv,
 				    unsigned long flags)
 {
 	struct mlx5e_tc_table *tc = mlx5e_fs_get_tc(priv->fs);
 	struct mlx5e_rep_priv *rpriv;
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+	if (mlx5e_eswitch_rep(priv->netdev) &&
+	    MLX5_VPORT_MANAGER(priv->mdev) &&
+	    priv->mdev->priv.eswitch->mode == MLX5_ESWITCH_OFFLOADS) {
+#elif !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+	if ((flags & MLX5_TC_FLAG(ESW_OFFLOAD)) ||
+	    (priv->mdev->priv.eswitch &&
+	     priv->mdev->priv.eswitch->mode == MLX5_ESWITCH_OFFLOADS)) {
+#else
 	if (flags & MLX5_TC_FLAG(ESW_OFFLOAD)) {
+#endif
 		rpriv = priv->ppriv;
+#if !defined(CONFIG_COMPAT_CLS_FLOWER_MOD) && \
+    !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+		if (!rpriv || !rpriv->tc_ht.tbl)
+			return &tc->ht;
+#endif
 		return &rpriv->tc_ht;
 	} else /* NIC offload */
 		return &tc->ht;
@@ -5145,26 +5335,36 @@ static struct rhashtable *get_tc_ht(stru
 static bool is_peer_flow_needed(struct mlx5e_tc_flow *flow)
 {
 	struct mlx5_esw_flow_attr *esw_attr = flow->attr->esw_attr;
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	struct mlx5_flow_attr *attr = flow->attr;
 	bool is_rep_ingress = esw_attr->in_rep->vport != MLX5_VPORT_UPLINK &&
 		flow_flag_test(flow, INGRESS);
 	bool act_is_encap = !!(attr->action &
 			       MLX5_FLOW_CONTEXT_ACTION_PACKET_REFORMAT);
+#endif
 	bool esw_paired = mlx5_devcom_comp_is_ready(esw_attr->in_mdev->priv.devcom,
 						    MLX5_DEVCOM_ESW_OFFLOADS);
 
 	if (!esw_paired)
 		return false;
 
+#ifdef HAVE_QDISC_SUPPORTS_BLOCK_SHARING
 	if ((mlx5_lag_is_sriov(esw_attr->in_mdev) ||
 	     mlx5_lag_is_multipath(esw_attr->in_mdev)) &&
-	    (is_rep_ingress || act_is_encap))
+	    (is_rep_ingress || act_is_encap
+#ifdef HAVE_TC_SETUP_CB_EGDEV_REGISTER
+	     || (flow->flags & MLX5_TC_FLAG(EGRESS))
+#endif
+	    ))
 		return true;
 
 	if (mlx5_lag_is_mpesw(esw_attr->in_mdev))
 		return true;
 
 	return false;
+#else
+	return (mlx5_lag_is_sriov(esw_attr->in_mdev) ||  mlx5_lag_is_multipath(esw_attr->in_mdev));
+#endif
 }
 
 struct mlx5_flow_attr *
@@ -5277,8 +5477,16 @@ mlx5e_flow_attr_init(struct mlx5_flow_at
 		     struct flow_cls_offload *f)
 {
 	attr->parse_attr = parse_attr;
+#ifdef HAVE_PRIO_CHAIN_SUPPORT
 	attr->chain = f->common.chain_index;
+#ifdef CONFIG_COMPAT_TC_PRIO_IS_MAJOR
 	attr->prio = f->common.prio;
+#else
+	attr->prio = TC_H_MAJ(f->common.prio) >> 16;
+#endif
+#else
+	attr->prio = 1;
+#endif
 }
 
 static void
@@ -5312,12 +5520,22 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 		     struct mlx5_eswitch_rep *in_rep,
 		     struct mlx5_core_dev *in_mdev)
 {
-	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+	struct flow_rule *rule;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
 	int attr_size, err;
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = alloc_flow_rule(&f);
+	if (IS_ERR(rule))
+		return ERR_PTR(PTR_ERR(rule));
+#else
+	rule = flow_cls_offload_flow_rule(f);
+#endif
+
 	flow_flags |= BIT(MLX5E_TC_FLOW_FLAG_ESWITCH);
 	attr_size  = sizeof(struct mlx5_esw_flow_attr);
 	err = mlx5e_alloc_flow(priv, attr_size, f, flow_flags,
@@ -5354,11 +5572,18 @@ __mlx5e_add_fdb_flow(struct mlx5e_priv *
 		add_unready_flow(flow);
 	}
 
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
+
 	return flow;
 
 err_free:
 	mlx5e_flow_put(priv, flow);
 out:
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+        free_flow_rule(rule);
+#endif
 	return ERR_PTR(err);
 }
 
@@ -5470,18 +5695,30 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 		   struct net_device *filter_dev,
 		   struct mlx5e_tc_flow **__flow)
 {
-	struct flow_rule *rule = flow_cls_offload_flow_rule(f);
+	struct flow_rule *rule;
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct mlx5e_tc_flow_parse_attr *parse_attr;
 	struct mlx5e_tc_flow *flow;
-	int attr_size, err;
+	int attr_size, err = -EOPNOTSUPP;
+
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = alloc_flow_rule(&f);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+#else
+	rule = flow_cls_offload_flow_rule(f);
+#endif
 
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && defined(HAVE_PRIO_CHAIN_SUPPORT)
 	if (!MLX5_CAP_FLOWTABLE_NIC_RX(priv->mdev, ignore_flow_level)) {
 		if (!tc_cls_can_offload_and_chain0(priv->netdev, &f->common))
-			return -EOPNOTSUPP;
+			goto out;
 	} else if (!tc_can_offload_extack(priv->netdev, f->common.extack)) {
-		return -EOPNOTSUPP;
+		goto out;
 	}
+#endif
 
 	flow_flags |= BIT(MLX5E_TC_FLOW_FLAG_NIC);
 	attr_size  = sizeof(struct mlx5_nic_flow_attr);
@@ -5512,6 +5749,9 @@ mlx5e_add_nic_flow(struct mlx5e_priv *pr
 		goto err_free;
 
 	flow_flag_set(flow, OFFLOADED);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+        free_flow_rule(rule);
+#endif
 	*__flow = flow;
 
 	return 0;
@@ -5521,6 +5761,9 @@ err_free:
 	mlx5e_mod_hdr_dealloc(&parse_attr->mod_hdr_acts);
 	mlx5e_flow_put(priv, flow);
 out:
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+        free_flow_rule(rule);
+#endif
 	return err;
 }
 
@@ -5537,8 +5780,10 @@ mlx5e_tc_add_flow(struct mlx5e_priv *pri
 
 	get_flags(flags, &flow_flags);
 
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && defined(HAVE_TC_CLS_FLOWER_OFFLOAD_COMMON)
 	if (!tc_can_offload_extack(priv->netdev, f->common.extack))
 		return -EOPNOTSUPP;
+#endif
 
 	if (esw && esw->mode == MLX5_ESWITCH_OFFLOADS)
 		err = mlx5e_add_fdb_flow(priv, f, flow_flags,
@@ -5603,12 +5848,19 @@ static void mlx5e_tc_unblock_ipsec_offlo
 int mlx5e_configure_flower(struct net_device *dev, struct mlx5e_priv *priv,
 			   struct flow_cls_offload *f, unsigned long flags)
 {
+#ifdef HAVE_TC_CLS_OFFLOAD_EXTACK
 	struct netlink_ext_ack *extack = f->common.extack;
+#endif
 	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 	struct mlx5e_rep_priv *rpriv = priv->ppriv;
 	struct mlx5e_tc_flow *flow;
 	int err = 0;
 
+#if defined(CONFIG_COMPAT_CLS_FLOWER_MOD) || \
+    (!defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD))
+	get_new_flags(priv, &flags);
+#endif
+
 	if (priv->mdev->priv.flags & MLX5_PRIV_FLAGS_DISABLE_ALL_ADEV)
 		return -ENODEV;
 
@@ -5630,6 +5882,11 @@ int mlx5e_configure_flower(struct net_de
 		if (is_flow_rule_duplicate_allowed(dev, rpriv) && flow->orig_dev != dev)
 			goto rcu_unlock;
 
+#if !defined(HAVE_TC_INDR_API)
+		if(flow->orig_dev != dev)
+			goto out;
+#endif
+
 		NL_SET_ERR_MSG_MOD(extack,
 				   "flow cookie already exists, ignoring");
 		netdev_warn_once(priv->netdev,
@@ -5643,7 +5900,9 @@ rcu_unlock:
 	if (flow)
 		goto out;
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_mlx5e_configure_flower(f);
+#endif
 	err = mlx5e_tc_add_flow(priv, f, flags, dev, &flow);
 	if (err)
 		goto out;
@@ -5671,6 +5930,10 @@ esw_release:
 	return err;
 }
 
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_configure_flower);
+#endif
+
 static bool same_flow_direction(struct mlx5e_tc_flow *flow, int flags)
 {
 	bool dir_ingress = !!(flags & MLX5_TC_FLAG(INGRESS));
@@ -5704,7 +5967,9 @@ int mlx5e_delete_flower(struct net_devic
 	rhashtable_remove_fast(tc_ht, &flow->node, tc_ht_params);
 	rcu_read_unlock();
 
+#ifndef MLX_DISABLE_TRACEPOINTS
 	trace_mlx5e_delete_flower(f);
+#endif
 	mlx5e_flow_put(priv, flow);
 
 	mlx5e_tc_unblock_ipsec_offload(dev, priv);
@@ -5715,6 +5980,9 @@ errout:
 	rcu_read_unlock();
 	return err;
 }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_delete_flower);
+#endif
 
 int mlx5e_tc_fill_action_stats(struct mlx5e_priv *priv,
 			       struct flow_offload_action *fl_act)
@@ -5729,11 +5997,21 @@ int mlx5e_stats_flower(struct net_device
 	struct rhashtable *tc_ht = get_tc_ht(priv, flags);
 	struct mlx5e_tc_flow *flow;
 	struct mlx5_fc *counter;
+#if !defined(HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD) && \
+    !defined(HAVE_TCF_EXTS_STATS_UPDATE)
+	struct tc_action *a;
+	LIST_HEAD(actions);
+#endif
 	u64 lastuse = 0;
 	u64 packets = 0;
 	u64 bytes = 0;
 	int err = 0;
 
+#if defined(CONFIG_COMPAT_CLS_FLOWER_MOD) || \
+    (!defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD))
+	get_new_flags(priv, &flags);
+#endif
+
 	rcu_read_lock();
 	flow = mlx5e_flow_get(rhashtable_lookup(tc_ht, &f->cookie,
 						tc_ht_params));
@@ -5748,7 +6026,11 @@ int mlx5e_stats_flower(struct net_device
 
 	if (mlx5e_is_offloaded_flow(flow)) {
 		if (flow_flag_test(flow, USE_ACT_STATS)) {
+#if defined(HAVE_FLOW_OFFLOAD_ACTION) && defined(HAVE_USE_ACT_STATS)
 			f->use_act_stats = true;
+#elif defined(HAVE_FLOW_ACTION_ENTRY_ACT_POINTER)
+			mlx5e_tc_act_stats_fill_stats_flow(get_act_stats_handle(flow->priv), flow);
+#endif
 		} else {
 			counter = mlx5e_tc_get_counter(flow);
 			if (!counter)
@@ -5775,7 +6057,11 @@ int mlx5e_stats_flower(struct net_device
 			if (!flow_flag_test(peer_flow, OFFLOADED))
 				continue;
 			if (flow_flag_test(flow, USE_ACT_STATS)) {
+#if defined(HAVE_FLOW_OFFLOAD_ACTION) && defined(HAVE_USE_ACT_STATS)
 				f->use_act_stats = true;
+#elif defined(HAVE_FLOW_ACTION_ENTRY_ACT_POINTER)
+				/* Waiting for support for peer flow act stats */
+#endif
 				break;
 			}
 
@@ -5794,14 +6080,81 @@ int mlx5e_stats_flower(struct net_device
 no_peer_counter:
 	mlx5_devcom_for_each_peer_end(devcom, MLX5_DEVCOM_ESW_OFFLOADS);
 out:
+#ifdef HAVE_FLOW_STATS_UPDATE_6_PARAMS
 	flow_stats_update(&f->stats, bytes, packets, 0, lastuse,
 			  FLOW_ACTION_HW_STATS_DELAYED);
-	trace_mlx5e_stats_flower(f);
-errout:
-	mlx5e_flow_put(priv, flow);
-	return err;
+#elif defined(HAVE_FLOW_STATS_UPDATE_5_PARAMS)
+	flow_stats_update(&f->stats, bytes, packets, lastuse,
+			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD)
+	flow_stats_update(&f->stats, bytes, packets, lastuse);
+#elif defined(HAVE_TCF_EXTS_STATS_UPDATE)
+	tcf_exts_stats_update(f->exts, bytes, packets, lastuse);
+#else
+	preempt_disable();
+
+#ifdef HAVE_TCF_EXTS_TO_LIST
+	tcf_exts_to_list(f->exts, &actions);
+	list_for_each_entry(a, &actions, list)
+#else
+	tc_for_each_action(a, f->exts)
+#endif
+#ifdef HAVE_TCF_ACTION_STATS_UPDATE
+		tcf_action_stats_update(a, bytes, packets, lastuse);
+#else
+	{
+		struct tcf_act_hdr *h = a->priv;
+
+		spin_lock(&h->tcf_lock);
+		h->tcf_tm.lastuse = max_t(u64, h->tcf_tm.lastuse, lastuse);
+		h->tcf_bstats.bytes += bytes;
+		h->tcf_bstats.packets += packets;
+		spin_unlock(&h->tcf_lock);
+	}
+#endif
+	preempt_enable();
+#endif /* HAVE_TC_CLS_FLOWER_OFFLOAD_HAS_STATS_FIELD */
+#ifndef MLX_DISABLE_TRACEPOINTS
+ 	trace_mlx5e_stats_flower(f);
+#endif
+ errout:
+ 	mlx5e_flow_put(priv, flow);
+ 	return err;
+ }
+#ifdef CONFIG_COMPAT_CLS_FLOWER_MOD
+EXPORT_SYMBOL(mlx5e_stats_flower);
+#endif
+
+int mlx5e_policer_validate(const struct flow_action *action,
+			   const struct flow_action_entry *act,
+			   struct netlink_ext_ack *extack)
+{
+#ifdef HAVE_FLOW_ACTION_POLICE_EXCEED
+	if (act->police.exceed.act_id != FLOW_ACTION_DROP) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when exceed action is not drop");
+		return -EOPNOTSUPP;
+	}
+
+	if (act->police.notexceed.act_id == FLOW_ACTION_ACCEPT &&
+	    !flow_action_is_last_entry(action, act)) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when conform action is ok, but action is not last");
+		return -EOPNOTSUPP;
+	}
+
+	if (act->police.peakrate_bytes_ps ||
+	    act->police.avrate || act->police.overhead) {
+		NL_SET_ERR_MSG_MOD(extack,
+				   "Offload not supported when peakrate/avrate/overhead is configured");
+		return -EOPNOTSUPP;
+	}
+#endif
+ 
+	return 0;
 }
 
+#ifdef HAVE_TC_CLSMATCHALL_STATS
 static int apply_police_params(struct mlx5e_priv *priv, u64 rate,
 			       struct netlink_ext_ack *extack)
 {
@@ -5843,6 +6196,7 @@ tc_matchall_police_validate(const struct
 			    const struct flow_action_entry *act,
 			    struct netlink_ext_ack *extack)
 {
+#if defined(HAVE_FLOW_ACTION_POLICE_EXCEED) && defined (HAVE_FLOW_ACTION_CONTINUE)
 	if (act->police.notexceed.act_id != FLOW_ACTION_CONTINUE) {
 		NL_SET_ERR_MSG_MOD(extack,
 				   "Offload not supported when conform action is not continue");
@@ -5869,6 +6223,7 @@ tc_matchall_police_validate(const struct
 		return -EOPNOTSUPP;
 	}
 
+#endif
 	return 0;
 }
 
@@ -5886,18 +6241,24 @@ static int scan_tc_matchall_fdb_actions(
 		return -EINVAL;
 	}
 
+#ifdef HAVE_FLOW_OFFLOAD_HAS_ONE_ACTION
 	if (!flow_offload_has_one_action(flow_action)) {
+#else
+	if (flow_action->num_entries != 1) {
+#endif
 		NL_SET_ERR_MSG_MOD(extack, "matchall policing support only a single action");
 		return -EOPNOTSUPP;
 	}
 
+#ifdef HAVE_FLOW_ACTION_HW_STATS_CHECK
 	if (!flow_action_basic_hw_stats_check(flow_action, extack)) {
 		NL_SET_ERR_MSG_MOD(extack, "Flow action HW stats type is not supported");
 		return -EOPNOTSUPP;
 	}
+#endif
 
 	flow_action_for_each(i, act, flow_action) {
-		switch (act->id) {
+		switch ((int) act->id) {
 		case FLOW_ACTION_POLICE:
 			err = tc_matchall_police_validate(flow_action, act, extack);
 			if (err)
@@ -5922,13 +6283,33 @@ int mlx5e_tc_configure_matchall(struct m
 				struct tc_cls_matchall_offload *ma)
 {
 	struct netlink_ext_ack *extack = ma->common.extack;
+	int prio = ma->common.prio;
+	struct flow_rule *rule;
+	int err;
 
-	if (ma->common.prio != 1) {
+#ifndef CONFIG_COMPAT_TC_PRIO_IS_MAJOR
+	prio = TC_H_MAJ(prio) >> 16;
+#endif
+ 
+	if (prio != 1) {
 		NL_SET_ERR_MSG_MOD(extack, "only priority 1 is supported");
 		return -EINVAL;
 	}
 
-	return scan_tc_matchall_fdb_actions(priv, &ma->rule->action, extack);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	rule = __alloc_flow_rule(ma->exts, NULL, 0);
+	if (IS_ERR(rule))
+		return PTR_ERR(rule);
+#else
+	rule = ma->rule;
+#endif
+
+	err = scan_tc_matchall_fdb_actions(priv, &rule->action, extack);
+#ifndef HAVE_TC_SETUP_FLOW_ACTION
+	free_flow_rule(rule);
+#endif
+
+	return err;
 }
 
 int mlx5e_tc_delete_matchall(struct mlx5e_priv *priv,
@@ -5951,9 +6332,19 @@ void mlx5e_tc_stats_matchall(struct mlx5
 	dpkts = cur_stats.rx_packets - rpriv->prev_vf_vport_stats.rx_packets;
 	dbytes = cur_stats.rx_bytes - rpriv->prev_vf_vport_stats.rx_bytes;
 	rpriv->prev_vf_vport_stats = cur_stats;
+#ifdef HAVE_FLOW_STATS_UPDATE_6_PARAMS
 	flow_stats_update(&ma->stats, dbytes, dpkts, 0, jiffies,
 			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_FLOW_STATS_UPDATE_5_PARAMS)
+	flow_stats_update(&ma->stats, dbytes, dpkts, jiffies,
+			  FLOW_ACTION_HW_STATS_DELAYED);
+#elif defined(HAVE_TC_SETUP_FLOW_ACTION)
+	flow_stats_update(&ma->stats, dbytes, dpkts, jiffies);
+#else
+	tcf_exts_stats_update(ma->exts, dbytes, dpkts, jiffies);
+#endif
 }
+#endif /* HAVE_TC_CLSMATCHALL_STATS */
 
 static void mlx5e_tc_hairpin_update_dead_peer(struct mlx5e_priv *priv,
 					      struct mlx5e_priv *peer_priv)
@@ -6202,6 +6593,10 @@ int mlx5e_tc_ht_init(struct rhashtable *
 void mlx5e_tc_ht_cleanup(struct rhashtable *tc_ht)
 {
 	rhashtable_free_and_destroy(tc_ht, _mlx5e_tc_del_flow, NULL);
+#if !defined (CONFIG_COMPAT_CLS_FLOWER_MOD) && \
+    !defined(HAVE_TC_BLOCK_OFFLOAD) && !defined(HAVE_FLOW_BLOCK_OFFLOAD)
+	tc_ht->tbl = NULL;
+#endif
 }
 
 int mlx5e_tc_esw_init(struct mlx5_rep_uplink_priv *uplink_priv)
@@ -6346,10 +6741,28 @@ void mlx5e_tc_reoffload_flows_work(struc
 	mutex_unlock(&rpriv->unready_flows_lock);
 }
 
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
+#ifdef CONFIG_MLX5_ESWITCH
+#if defined(HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_BLOCK_OFFLOAD)
 static int mlx5e_setup_tc_cls_flower(struct mlx5e_priv *priv,
+#else
+int mlx5e_setup_tc_cls_flower(struct net_device *dev,
+#endif
 				     struct flow_cls_offload *cls_flower,
 				     unsigned long flags)
 {
+#ifndef HAVE_TC_CLS_CAN_OFFLOAD_AND_CHAIN0
+#ifdef HAVE_TC_BLOCK_OFFLOAD
+	if (cls_flower->common.chain_index)
+#else
+	struct mlx5e_priv *priv = netdev_priv(dev);
+
+	if (!is_classid_clsact_ingress(cls_flower->common.classid) ||
+	    cls_flower->common.chain_index)
+#endif
+		return -EOPNOTSUPP;
+#endif
+
 	switch (cls_flower->command) {
 	case FLOW_CLS_REPLACE:
 		return mlx5e_configure_flower(priv->netdev, priv, cls_flower,
@@ -6365,6 +6778,7 @@ static int mlx5e_setup_tc_cls_flower(str
 	}
 }
 
+#if defined(HAVE_TC_BLOCK_OFFLOAD) || defined(HAVE_FLOW_BLOCK_OFFLOAD)
 int mlx5e_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
 			    void *cb_priv)
 {
@@ -6374,6 +6788,11 @@ int mlx5e_setup_tc_block_cb(enum tc_setu
 	if (!priv->netdev || !netif_device_present(priv->netdev))
 		return -EOPNOTSUPP;
 
+#if defined(HAVE_TC_CLS_OFFLOAD_EXTACK) && !defined(HAVE_PRIO_CHAIN_SUPPORT)
+	if (!tc_cls_can_offload_and_chain0(priv->netdev, type_data))
+		return -EOPNOTSUPP;
+#endif
+
 	if (mlx5e_is_uplink_rep(priv))
 		flags |= MLX5_TC_FLAG(ESW_OFFLOAD);
 	else
@@ -6387,6 +6806,37 @@ int mlx5e_setup_tc_block_cb(enum tc_setu
 	}
 }
 
+#ifndef HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE
+int mlx5e_setup_tc_block(struct net_device *dev,
+			 struct tc_block_offload *f)
+{
+	struct mlx5e_priv *priv = netdev_priv(dev);
+
+	if (f->binder_type != FLOW_BLOCK_BINDER_TYPE_CLSACT_INGRESS)
+		return -EOPNOTSUPP;
+
+	switch (f->command) {
+	case TC_BLOCK_BIND:
+		return tcf_block_cb_register(f->block, mlx5e_setup_tc_block_cb,
+					     priv, priv
+#ifdef HAVE_TC_BLOCK_OFFLOAD_EXTACK
+					     , f->extack
+#endif
+					    );
+	case TC_BLOCK_UNBIND:
+		tcf_block_cb_unregister(f->block, mlx5e_setup_tc_block_cb,
+					priv);
+		return 0;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+#endif /* HAVE_FLOW_BLOCK_CB_SETUP_SIMPLE */
+#endif /* HAVE_TC_BLOCK_OFFLOAD || HAVE_FLOW_BLOCK_OFFLOAD */
+#endif /*ESWITCH */
+#endif
+
+#if defined(HAVE_NDO_SETUP_TC_TAKES_TC_SETUP_TYPE) || defined(HAVE_NDO_SETUP_TC_RH_EXTENDED)
 static bool mlx5e_tc_restore_tunnel(struct mlx5e_priv *priv, struct sk_buff *skb,
 				    struct mlx5e_tc_update_priv *tc_priv,
 				    u32 tunnel_id)
@@ -6433,14 +6883,22 @@ static bool mlx5e_tc_restore_tunnel(stru
 	case FLOW_DISSECTOR_KEY_IPV4_ADDRS:
 		tun_dst = __ip_tun_set_dst(key.enc_ipv4.src, key.enc_ipv4.dst,
 					   key.enc_ip.tos, key.enc_ip.ttl,
+#ifdef HAVE___IP_TUN_SET_DST_7_PARAMS
+					   TUNNEL_KEY,
+#else
 					   key.enc_tp.dst, TUNNEL_KEY,
+#endif
 					   key32_to_tunnel_id(key.enc_key_id.keyid),
 					   enc_opts.key.len);
 		break;
 	case FLOW_DISSECTOR_KEY_IPV6_ADDRS:
 		tun_dst = __ipv6_tun_set_dst(&key.enc_ipv6.src, &key.enc_ipv6.dst,
 					     key.enc_ip.tos, key.enc_ip.ttl,
+#ifdef HAVE___IP_TUN_SET_DST_7_PARAMS
+					     0, TUNNEL_KEY,
+#else
 					     key.enc_tp.dst, 0, TUNNEL_KEY,
+#endif
 					     key32_to_tunnel_id(key.enc_key_id.keyid),
 					     enc_opts.key.len);
 		break;
@@ -6505,8 +6963,10 @@ static bool mlx5e_tc_restore_skb_tc_meta
 		}
 
 		if (act_miss_cookie) {
+#ifdef HAVE_TC_SKB_EXT_ACT_MISS
 			tc_skb_ext->act_miss_cookie = act_miss_cookie;
 			tc_skb_ext->act_miss = 1;
+#endif
 		} else {
 			tc_skb_ext->chain = chain;
 		}
@@ -6616,6 +7076,7 @@ bool mlx5e_tc_update_skb_nic(struct mlx5
 	return mlx5e_tc_update_skb(cqe, skb, mapping_ctx, mapped_obj_id, ct_priv, zone_restore_id,
 				   0, NULL);
 }
+#endif
 
 static struct mapping_ctx *
 mlx5e_get_priv_obj_mapping(struct mlx5e_priv *priv)
